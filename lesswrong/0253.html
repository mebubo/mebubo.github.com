<html><head><title>Neural Categories</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Neural Categories</h1><p><i>Eliezer Yudkowsky, 10 February 2008 12:33AM</i></p><div><p><strong>Followup to</strong>:  <a href="0252.html">Disguised Queries</a> [http://lesswrong.com/lw/nm/disguised_queries/]</p> <p>In <a href="0252.html">Disguised Queries</a> [http://lesswrong.com/lw/nm/disguised_queries/], I talked about a classification task of "bleggs" and "rubes".  The typical blegg is blue, egg-shaped, furred, flexible, opaque, glows in the dark, and contains vanadium.  The typical rube is red, cube-shaped, smooth, hard, translucent, unglowing, and contains palladium.  For the sake of simplicity, let us forget the characteristics of flexibility/hardness and opaqueness/translucency.  This leaves five dimensions in <a href="0251.html">thingspace</a> [http://lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/]:  Color, shape, texture, luminance, and interior.</p> <p>Suppose I want to create an Artificial Neural Network (ANN) to predict unobserved blegg characteristics from observed blegg characteristics.  And suppose I'm fairly naive about ANNs:  I've read excited popular science books about how neural networks are distributed, emergent, and parallel <em>just like the human brain!!</em> but I can't derive the differential equations for gradient descent in a non-recurrent multilayer network with sigmoid units (which is actually a lot easier than it sounds).</p> <p>Then I might design a neural network that looks something like this:</p> <p><a id="more"></a></p> <p><a href="http://lesswrong.com/static/imported/2008/02/09/blegg1_3.png"><img src="ac96e2c9.png" title="Blegg1_3" height="286" width="617" alt="Blegg1_3" border="0"></a> [http://lesswrong.com/static/imported/2008/02/09/blegg1_3.png]</p> <p>Network 1 is for classifying bleggs and rubes.  But since "blegg" is an unfamiliar and synthetic concept, I've also included a similar Network 1b for distinguishing humans from Space Monsters, with input from Aristotle ("All men are mortal") and Plato's Academy ("A featherless biped with broad nails").</p> <p>A neural network needs a learning rule.  The obvious idea is that when two nodes are often active at the same time, we should strengthen the connection between them&#8212;this is one of the first rules ever proposed for training a neural network, known as Hebb's Rule.</p> <p>Thus, if you often saw things that were both blue and furred&#8212;thus simultaneously activating the "color" node in the + state and the "texture" node in the + state&#8212;the connection would strengthen between color and texture, so that + colors activated + textures, and vice versa.  If you saw things that were blue and egg-shaped and vanadium-containing, that would strengthen positive mutual connections between color and shape and interior.</p> <p>Let's say you've already seen plenty of bleggs and rubes come off the conveyor belt.  But now you see something that's furred, egg-shaped, and&#8212;gasp!&#8212;reddish purple (which we'll model as a "color" activation level of -2/3).  You haven't yet tested the luminance, or the interior.  What to predict, what to predict?</p> <p>What happens then is that the activation levels in Network 1 bounce around a bit.  Positive activation flows luminance from shape, negative activation flows to interior from color, negative activation flows from interior to luminance...  Of course all these messages are passed in<em> parallel!!</em> and <em>asynchronously!!</em> just like the human brain...</p> <p>Finally Network 1 settles into a stable state, which has high positive activation for "luminance" and "interior".  The network may be said to "expect" (though it has not yet seen) that the object will glow in the dark, and that it contains vanadium.</p> <p>And lo, Network 1 exhibits this behavior even though there's no explicit node that says whether the object is a blegg or not.  The judgment is <em>implicit in the whole network!!</em>  Bleggness is an <em>attractor!!</em> which arises as the result of <em>emergent behavior!!</em> from the <em>distributed!!</em> learning rule.</p> <p>Now in real life, this kind of network design&#8212;however <a href="http://thedailywtf.com/Articles/No,_We_Need_a_Neural_Network.aspx">faddish</a> [http://thedailywtf.com/Articles/No,_We_Need_a_Neural_Network.aspx] it may sound&#8212;runs into <em>all sorts</em> of problems.  Recurrent networks don't always settle right away:  They can oscillate, or exhibit chaotic behavior, or just take a very long time to settle down.  This is a Bad Thing when you see something big and yellow and striped, and you have to wait five minutes for your distributed neural network to settle into the "tiger" attractor.  Asynchronous and parallel it may be, but it's not real-time.</p> <p>And there are other problems, like <a href="0078.html">double-counting the evidence</a> [http://lesswrong.com/lw/is/fake_causality/] when messages bounce back and forth:  If you suspect that an object glows in the dark, your suspicion will activate belief that the object contains vanadium, which in turn will activate belief that the object glows in the dark.</p> <p>Plus if you try to scale up the Network 1 design, it requires O(N<sup>2</sup>) connections, where N is the total number of observables.</p> <p>So what might be a more realistic neural network design?</p> <p><a href="http://lesswrong.com/static/imported/2008/02/09/blegg2.png"><img src="f3a902da.png" title="Blegg2" height="286" width="302" alt="Blegg2" border="0"></a> [http://lesswrong.com/static/imported/2008/02/09/blegg2.png]<br>In this network, a wave of activation converges on the central node from any clamped (observed) nodes, and then surges back out again to any unclamped (unobserved) nodes.  Which means we can compute the answer in one step, rather than waiting for the network to settle&#8212;an important requirement in biology when the neurons only run at 20Hz.  And the network architecture scales as O(N), rather than O(N<sup>2</sup>).</p> <p>Admittedly, there are some things you can notice more easily with the first network architecture than the second.  Network 1 has a direct connection between every two nodes.  So if red objects <em>never</em> glow in the dark, but red furred objects usually have the other blegg characteristics like egg-shape and vanadium, Network 1 can easily represent this: it just takes a very strong direct negative connection from color to luminance, but more powerful positive connections from texture to all other nodes except luminance.</p> <p>Nor is this a "special exception" to the general rule that bleggs glow&#8212;remember, in Network 1, there is no unit that represents blegg-ness; blegg-ness emerges as an attractor in the distributed network.</p> <p>So yes, those N<sup>2</sup> connections were buying us something.  But not very much.  Network 1 is not <em>more</em> useful on most real-world problems, where you rarely find an animal stuck halfway between being a cat and a dog.</p> <p>(There are also facts that you can't easily represent in Network 1 <em>or</em> Network 2.  Let's say sea-blue color and spheroid shape, when found together, always indicate the presence of palladium; but when found individually, without the other, they are each very strong evidence for vanadium.  This is hard to represent, in either architecture, without extra nodes.  Both Network 1 and Network 2 embody implicit assumptions about what kind of environmental structure is likely to exist; the ability to read this off is what separates the adults from the babes, in machine learning.)</p> <p>Make no mistake:  Neither Network 1, nor Network 2, are biologically realistic.  <em>But</em> it still seems like a fair guess that however the brain really works, it is in some sense closer to Network 2 than Network 1.  Fast, cheap, scalable, works well to distinguish dogs and cats: natural selection goes for that sort of thing like water running down a fitness landscape.</p> <p>It seems like an ordinary enough task to classify objects as either bleggs or rubes, tossing them into the appropriate bin.  But would you notice if sea-blue objects never glowed in the dark?</p> <p>Maybe, if someone presented you with twenty objects that were alike only in being sea-blue, and then switched off the light, and none of the objects glowed.  If you got hit over the head with it, in other words.  Perhaps by presenting you with all these sea-blue objects in a group, your brain forms a new subcategory, and can detect the "doesn't glow" characteristic within that subcategory.  But you probably wouldn't notice if the sea-blue objects were scattered among a hundred other bleggs and rubes.  It wouldn't be <em>easy</em> or <em>intuitive</em> to notice, the way that distinguishing cats and dogs is easy and intuitive.</p> <p>Or:  "Socrates is human, all humans are mortal, therefore Socrates is mortal."  How did Aristotle know that Socrates was human?  Well, Socrates had no feathers, and broad nails, and walked upright, and spoke Greek, and, well, was generally shaped like a human and acted like one.  So the brain decides, once and for all, that Socrates is human; and from there, infers that Socrates is mortal like all other humans thus yet observed.  It doesn't seem easy or intuitive to ask how much wearing clothes, as opposed to using language, is associated with mortality.  Just, "things that wear clothes and use language are human" and "humans are mortal".</p> <p>Are there biases associated with trying to classify things into categories once and for all?  Of course there are.  See e.g. <a href="0207.html">Cultish Countercultishness</a> [http://lesswrong.com/lw/md/cultish_countercultishness/].</p> <p>To be continued...</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words"><em>A Human's Guide to Words</em></a> [http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words]</p> <p style="text-align:right">Next post: "<a href="0254.html">How An Algorithm Feels From Inside</a> [http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside]"</p> <p style="text-align:right">Previous post: "<a href="0252.html">Disguised Queries</a> [http://lesswrong.com/lw/nm/disguised_queries/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq11.html">Sequence 11: A Human's Guide to Words</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0252.html">Disguised Queries</a></p></td><td><p><i>Next: </i><a href="0254.html">How An Algorithm Feels From Inside</a></p></td></tr></table><p><i>Referenced by: </i><a href="0252.html">Disguised Queries</a> &#8226; <a href="0254.html">How An Algorithm Feels From Inside</a> &#8226; <a href="0256.html">Feel the Meaning</a> &#8226; <a href="0263.html">Categorizing Has Consequences</a> &#8226; <a href="0273.html">Searching for Bayes-Structure</a> &#8226; <a href="0279.html">37 Ways That Words Can Be Wrong</a> &#8226; <a href="0281.html">Dissolving the Question</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/nn/neural_categories/">Neural Categories</a></p></body></html>