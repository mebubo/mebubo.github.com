<html><head><title>New report: Intelligence Explosion Microeconomics</title></head><body><h1>New report: Intelligence Explosion Microeconomics</h1><p><i>Eliezer Yudkowsky, 29 April 2013 11:14PM</i></p><div><p><strong>Summary</strong>: <a href="http://intelligence.org/files/IEM.pdf">Intelligence Explosion Microeconomics</a> [http://intelligence.org/files/IEM.pdf] (pdf) is 40,000 words taking some initial steps toward tackling the key quantitative issue in the intelligence explosion, "reinvestable returns on cognitive investments": what kind of returns can you get from an investment in cognition, can you reinvest it to make yourself even smarter, and does this process die out or blow up? This can be thought of as the compact and hopefully more coherent successor to the <a href="http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate">AI Foom Debate</a> [http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate] of a few years back.</p> <p>(Sample idea you haven't heard before:  The increase in hominid brain size over evolutionary time should be interpreted as evidence about increasing marginal fitness returns on brain size, presumably due to improved brain wiring algorithms; not as direct evidence about an intelligence scaling factor from brain size.)</p> <p>I hope that the open problems posed therein inspire further work by economists or economically literate modelers, interested specifically in the intelligence explosion <em>qua</em> cognitive intelligence rather than non-cognitive 'technological acceleration'.  MIRI has an intended-to-be-small-and-technical mailing list for such discussion.  In case it's not clear from context, I (Yudkowsky) am the author of the paper.</p> <p><strong>Abstract:</strong></p> <blockquote> <p style="padding-left: 30px;">I. J. Good's thesis of the 'intelligence explosion' is that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version of itself, and that this process could continue enough to vastly exceed human intelligence.  As Sandberg (2010) correctly notes, there are several attempts to lay down return-on-investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with I. J. Good's intelligence explosion thesis as such.</p> <p style="padding-left: 30px;">I identify the key issue as <em>returns on cognitive reinvestment</em> - the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs.  There are many phenomena in the world which have been argued as evidentially relevant to this question, from the observed course of hominid evolution, to Moore's Law, to the competence over time of machine chess-playing systems, and many more.  I go into some depth on the sort of debates which then arise on how to interpret such evidence.  I propose that the next step forward in analyzing positions on the intelligence explosion would be to formalize return-on-investment curves, so that each stance can say formally which possible microfoundations they hold to be falsified by historical observations already made.  More generally, I pose multiple open questions of 'returns on cognitive reinvestment' or 'intelligence explosion microeconomics'.  Although such questions have received little attention thus far, they seem highly relevant to policy choices affecting the outcomes for Earth-originating intelligent life.</p> </blockquote> <p>The <a href="https://docs.google.com/forms/d/1KElE2Zt_XQRqj8vWrc_rG89nrO4JtHWxIFldJ3IY_FQ/viewform"><strong>dedicated mailing list</strong></a> [https://docs.google.com/forms/d/1KElE2Zt_XQRqj8vWrc_rG89nrO4JtHWxIFldJ3IY_FQ/viewform] will be small and restricted to technical discussants.<a id="more"></a></p> <p>This topic was originally intended to be a sequence in <em>Open Problems in Friendly AI,</em> but further work produced something compacted beyond where it could be easily broken up into subposts.</p> <p><strong>Outline of contents:</strong></p> <p><strong>1</strong>:  Introduces the basic questions and the key quantitative issue of sustained reinvestable returns on cognitive investments.</p> <p><strong>2</strong>:  Discusses the basic language for talking about the intelligence explosion, and argues that we should pursue this project by looking for underlying microfoundations, not by pursuing analogies to allegedly similar historical events.</p> <p><strong>3</strong>:  Goes into detail on what I see as the main arguments for a fast intelligence explosion, constituting the bulk of the paper with the following subsections:</p> <ul> <li><strong>3.1</strong>:<span style="white-space: pre;"> </span>What the fossil record actually tells us about returns on brain size, given that most of the difference between Homo sapiens and Australopithecus was probably improved software.</li> <li><strong>3.2</strong>:<span style="white-space: pre;"> </span>How to divide credit for the human-chimpanzee performance gap between "humans are individually smarter than chimpanzees" and "the hominid transition involved a one-time qualitative gain from being able to accumulate knowledge".</li> <li><strong>3.3</strong>:<span style="white-space: pre;"> </span>How returns on speed (serial causal depth) contrast with returns from parallelism; how faster thought seems to contrast with more thought.  Whether sensing and manipulating technologies are likely to present a bottleneck for faster thinkers, or how large of a bottleneck.</li> <li><strong>3.4</strong>: <span style="white-space: pre;"> </span>How human populations seem to scale in problem-solving power; some reasons to believe that we scale inefficiently enough for it to be puzzling.  Garry Kasparov's chess match vs. The World, which Kasparov won.</li> <li><strong>3.5</strong>: <span style="white-space: pre;"> </span>Some inefficiencies that might cumulate in an estimate of humanity's net computational efficiency on a cognitive problem.</li> <li><strong>3.6</strong>: <span style="white-space: pre;"> </span>What the anthropological record actually tells us about cognitive returns on cumulative selection pressure, given that selection pressures were probably increasing over the course of hominid history.  How the observed history would be expected to look different, if there were in fact diminishing returns on cognition.</li> <li><strong>3.7</strong>: <span style="white-space: pre;"> </span>How to relate the curves for evolutionary difficulty, human-engineering difficulty, and AI-engineering difficulty, considering that they are almost certainly different.</li> <li><strong>3.8</strong>: <span style="white-space: pre;"> </span>Correcting for anthropic bias in trying to estimate the intrinsic 'difficulty 'of hominid-level intelligence just from observing that intelligence evolved here on Earth.</li> <li><strong>3.9</strong>: <span style="white-space: pre;"> </span>The question of whether to expect a 'local' (one-project) FOOM or 'global' (whole economy) FOOM and how returns on cognitive reinvestment interact with that.</li> <li><strong>3.10</strong>: <span style="white-space: pre;"> </span>The great open uncertainty about the minimal conditions for starting a FOOM; why I. J. Good's postulate of starting from 'ultraintelligence' is probably much too strong (sufficient, but very far above what is necessary).</li> <li><strong>3.11</strong>: <span style="white-space: pre;"> </span>The enhanced probability of unknown unknowns in the scenario, since a smarter-than-human intelligence will selectively seek out and exploit flaws or gaps in our current knowledge.</li> </ul> <p><strong>4</strong>:  A tentative methodology for formalizing theories of the intelligence explosion - a project of formalizing possible microfoundations and explicitly stating their alleged relation to historical experience, such that some possibilities can allegedly be falsified.</p> <p><strong>5</strong>:  Which open sub-questions seem both high-value and possibly answerable.</p> <p><strong>6</strong>:  Formally poses the Open Problem and mentions what it would take for MIRI itself to directly fund further work in this field.</p></div> <hr><p><i>Original with comments: </i><a href="http://lesswrong.com/r/lesswrong/lw/hbd/new_report_intelligence_explosion_microeconomics/">New report: Intelligence Explosion Microeconomics</a></p></body></html>