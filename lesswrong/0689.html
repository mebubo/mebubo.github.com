<html><head><title>Your Price for Joining</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Your Price for Joining</h1><p><i>Eliezer Yudkowsky, 26 March 2009 07:16AM</i></p><div><p><strong>Previously in series</strong>:  <a href="0682.html">Why Our Kind Can't Cooperate</a> [http://lesswrong.com/lw/3h/why_our_kind_cant_cooperate]</p> <p>In the <a href="http://en.wikipedia.org/wiki/Ultimatum_game">Ultimatum Game</a> [http://en.wikipedia.org/wiki/Ultimatum_game], the first player chooses how to split $10 between themselves and the second player, and the second player decides whether to accept the split or reject it&#8212;in the latter case, both parties get nothing.  So far as conventional causal decision theory goes (two-box on <a href="0242.html">Newcomb's Problem</a> [http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/], defect in <a href="0469.html">Prisoner's Dilemma</a> [http://lesswrong.com/lw/tn/the_true_prisoners_dilemma/]), the second player should prefer any non-zero amount to nothing.  But if the first player <em>expects </em>this behavior&#8212;accept any non-zero offer&#8212;then they have no motive to offer more than a penny.  As I assume you all know by now, I am <a href="0242.html">no fan of conventional causal decision theory</a> [http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/].  Those of us who remain interested in cooperating on the Prisoner's Dilemma, either because it's iterated, or because we have a term in our utility function for fairness, or because we use an unconventional decision theory, may also not accept an offer of one penny.</p> <p>And in fact, most Ultimatum "deciders" offer an even split; and most Ultimatum "accepters" reject any offer less than 20%.  A 100 USD game played in Indonesia (average per capita income at the time: 670 USD) showed offers of 30 USD being turned down, although this equates to two week's wages.  We can probably also assume that the players in Indonesia were not thinking about the academic debate over Newcomblike problems&#8212;this is just the way people feel about Ultimatum Games, even ones played for real money.</p> <p>There's an analogue of the Ultimatum Game in group coordination.  (Has it been studied?  I'd hope so...)  Let's say there's a common project&#8212;in fact, let's say that it's an altruistic common project, aimed at helping mugging victims in Canada, <em>or something</em><em>.</em>  If you join this group project, you'll get more done than you could on your own, relative to your utility function.  So, obviously, you should join.</p> <p>But wait!  The anti-mugging project keeps their funds invested in a money market fund!  That's ridiculous; it won't earn even as much interest as US Treasuries, let alone a dividend-paying index fund.</p> <p>Clearly, this project is run by morons, and you shouldn't join until they change their malinvesting ways.</p> <p>Now you might realize&#8212;if you stopped to think about it&#8212;that all things considered, you would <em>still </em>do better by working with the common anti-mugging project, than striking out on your own to fight crime.  But then&#8212;you might perhaps also realize&#8212;if you <em>too easily assent</em> to joining the group, why, what <em>motive</em> would they have to change their malinvesting ways?</p> <p>Well...  Okay, look.  Possibly because we're out of the ancestral environment where everyone knows everyone else... and possibly because the <a href="0682.html">nonconformist crowd</a> [http://lesswrong.com/lw/3h/why_our_kind_cant_cooperate] tries to repudiate <em>normal </em>group-cohering forces like conformity and leader-worship...</p> <p>...It seems to me that people in the atheist/libertarian/technophile/sf-fan/etcetera cluster often set their joining prices <em>way way way </em>too high.  Like a 50-way split Ultimatum game, where every one of 50 players demands at least 20% of the money.<a id="more"></a></p> <p>If you think how often situations like this would have arisen in the ancestral environment, then it's almost certainly a matter of <a href="0159.html">evolutionary psychology.</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/]  System 1 emotions, not System 2 calculation.  Our intuitions for when to join groups, versus when to hold out for more concessions to our own preferred way of doing things, would have been honed for hunter-gatherer environments of, e.g., 40 people all of whom you knew personally.</p> <p>And if the group is made up of 1000 people?  Then your hunter-gatherer instincts will underestimate the inertia of a group so large, and demand an unrealistically high price (in strategic shifts) for you to join.  There's a limited amount of organizational effort, and a limited number of degrees of freedom, that can go into doing things any one's person way.</p> <p>And if the strategy is large and complex, the sort of thing that takes e.g. ten people doing paperwork for a week, rather than being hammered out over a half-hour of negotiation around a campfire?  Then your hunter-gatherer instincts will underestimate the inertia of the group, relative to your own demands.</p> <p>And if you live in a wider world than a single hunter-gatherer tribe, so that you only see the one group representative who negotiates with you, and not the hundred other negotiations that have taken place already?  Then your instincts will tell you that it is just one person, a stranger at that, and the two of you are equals; whatever ideas they bring to the table are equal with whatever ideas you bring to the table, and the meeting point ought to be about even.</p> <p>And if you suffer from any weakness of will or akrasia, or if you are influenced by motives other than those you would admit to yourself that you are influenced by, then any group-altruistic project which does not offer you the rewards of status and control, may perhaps find itself underserved by your attentions.</p> <p>Now I do admit that I speak here primarily from the perspective of someone who goes around trying to herd cats; and not from the other side as someone who spends most of their time withholding their energies in order to blackmail those damned morons already on the project.  Perhaps I am a little prejudiced.</p> <p>But it seems to me that a reasonable rule of thumb might be as follows:</p> <p>If, on the whole, joining your efforts to a group project <em>would still have a net positive effect</em> according to your utility function&#8212;</p> <p>(or a larger positive effect than any other marginal use to which you could otherwise put those resources, although this latter mode of thinking seems little-used and humanly-unrealistic, for reasons I may post about some other time)</p> <p>&#8212;and the awful horrible annoying issue is not so important that <em>you personally</em> will get involved deeply enough to put in however many hours, weeks, or years may be required to get it fixed up&#8212;</p> <p>&#8212;then the issue is not worth you withholding your energies from the project; either instinctively until you see that people are paying attention to you and respecting you, or by conscious intent to blackmail the group into getting it done.</p> <p>And if the issue <em>is</em> worth that much to you... then by all means, join the group and do whatever it takes to get things fixed up.</p> <p>Now, if the existing contributors refuse to let you do this, <em>and </em>a reasonable third party would be expected to conclude that you were competent enough to do it, <em>and </em>there is no one else whose ox is being gored thereby, <em>then,</em> perhaps, we have a problem on our hands.  And it may be time for a little blackmail, if the resources you can conditionally commit are large enough to get their attention.</p> <p>Is this rule a little extreme?  Oh, maybe.  There <em>should</em> be a motive for the decision-making mechanism of a project to be responsible to its supporters; unconditional support would create its own problems.</p> <p>But <em>usually</em>... I observe that people underestimate the costs of what they ask for, or perhaps just act on instinct, and set their prices <em>way way way</em> too high.  If the nonconformist crowd ever wants to get anything done together, we need to move in the direction of joining groups and staying there at least a <em>little </em>more easily.  Even in the face of annoyances and imperfections!  Even in the face of unresponsiveness to our own better ideas!</p> <p>In the age of the Internet and in the company of nonconformists, it does get a little tiring reading the 451st public email from someone saying that the Common Project isn't worth their resources until the website has a sans-serif font.</p> <p>Of course this often isn't really about fonts.  It may be about laziness, akrasia, or <a href="0573.html">hidden rejections</a> [http://lesswrong.com/lw/wj/is_that_your_true_rejection/].  But in terms of group norms... in terms of what sort of public statements we respect, and which excuses we publicly scorn... we probably <em>do </em>want to encourage a group norm of:</p> <p><em>If the issue isn't worth your personally fixing by however much effort it takes, and it doesn't arise from outright bad faith, it's not worth refusing to contribute your efforts to a cause you deem worthwhile.</em></p> <p> </p> <p style="text-align:right">Part of the sequence <a href="0726.html"><em>The Craft and the Community</em></a> [http://lesswrong.com/lw/cz/the_craft_and_the_community/]</p> <p style="text-align:right">Next post: "<a href="0690.html">Can Humanism Match Religion's Output?</a> [http://lesswrong.com/lw/5t/can_humanism_match_religions_output/]"</p> <p style="text-align:right">Previous post: "<a href="0686.html">On Things that are Awesome</a> [http://lesswrong.com/lw/4y/on_things_that_are_awesome/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq16.html">Sequence 16: The Craft and the Community</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0686.html">On Things that are Awesome</a></p></td><td><p><i>Next: </i><a href="0690.html">Can Humanism Match Religion's Output?</a></p></td></tr></table><p><i>Referenced by: </i><a href="0686.html">On Things that are Awesome</a> &#8226; <a href="0690.html">Can Humanism Match Religion's Output?</a> &#8226; <a href="0693.html">Helpless Individuals</a> &#8226; <a href="0709.html">The Unfinished Mystery of the Shangri-La Diet</a> &#8226; <a href="0714.html">Bayesians vs. Barbarians</a> &#8226; <a href="0726.html">The Craft and the Community</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/5j/your_price_for_joining/">Your Price for Joining</a></p></body></html>