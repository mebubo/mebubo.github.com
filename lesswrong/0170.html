<html><head><title>Leaky Generalizations</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Leaky Generalizations</h1><p><i>Eliezer Yudkowsky, 22 November 2007 09:16PM</i></p><div><p><strong>Followup to</strong>:  <a href="0162.html">Terminal Values and Instrumental Values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/]</p> <p>Are apples good to eat?  Usually, but some apples are rotten.</p> <p>Do humans have ten fingers?  Most of us do, but plenty of people have lost a finger and nonetheless qualify as "human".</p> <p>Unless you descend to a level of description far below any macroscopic object - below societies, below people, below fingers, below tendon and bone, below cells, all the way down to particles and fields where the laws are <a href="0041.html">truly universal</a> [http://lesswrong.com/lw/hr/universal_law/] - then practically every generalization you use in the real world will be leaky.</p> <p>(Though there may, of course, be some exceptions to the above rule...) </p> <p>Mostly, the way you deal with leaky generalizations is that, well, you just have to deal.  If the cookie market almost always closes at 10pm, except on Thanksgiving it closes at 6pm, and today happens to be National Native American Genocide Day, you'd better show up before 6pm or you won't get a cookie.</p> <p>Our ability to manipulate leaky generalizations is opposed by <em>need for closure</em>, the degree to which we want to say once and for all that humans have fingers, and get frustrated when we have to tolerate continued ambiguity.  Raising the value of the stakes can increase need for closure - which shuts down complexity tolerance when complexity tolerance is most needed.</p><a id="more"></a><p>Life would be complicated even if the things we wanted were simple (<a href="0161.html">they</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/] <a href="0169.html">aren't</a> [http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/]).  The leakyness of leaky generalizations about what-to-do-next would leak in from the leaky structure of the real world.  Or to put it another way:<br></p> <p><a href="0162.html">Instrumental values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/] often have no specification which is both compact and local.</p> <p>Suppose there's a box containing a million dollars.  The box is locked, not with an ordinary combination lock, but with a dozen keys controlling a machine that can open the box.  If you know how the machine works, you can deduce which sequences of key-presses will open the box.  There's more than one key sequence that can trigger the button.  But if you press a sufficiently wrong sequence, the machine incinerates the money.  And if you <em>don't know</em> about the machine, there's no simple rules like "Pressing any key three times opens the box" or "Pressing five different keys with no repetitions incinerates the money."</p> <p>There's a <em>compact nonlocal</em> specification of which keys you want to press:  You want to press keys such that they open the box. You can write a compact computer program that computes which key sequences are good, bad or neutral, but the computer program will need to describe the machine, not just the keys themselves.</p> <p>There's likewise a <em>local noncompact</em> specification of which keys to press: a giant lookup table of the results for each possible key sequence.  It's a very large computer program, but it makes no mention of anything except the keys.</p> <p>But there's no way to describe which key sequences are good, bad, or neutral, which is both <em>simple</em> and phrased <em>only in terms of the keys themselves.</em></p> <p>It may be even worse if there are tempting local generalizations which turn out to be <em>leaky</em>.  Pressing <em>most</em> keys three times in a row will open the box, but there's a particular key that incinerates the money if you press it just once.  You might think you had found a perfect generalization - a locally describable class of sequences that <em>always</em> opened the box - when you had merely failed to visualize all the possible paths of the machine, or failed to value all the side effects.</p> <p>The machine represents the complexity of the real world.  The openness of the box (which is good) and the incinerator (which is bad) represent the <a href="0161.html">thousand shards of desire</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/] that make up our <a href="0162.html">terminal values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/].  The keys represent the actions and policies and strategies available to us.</p> <p>When you consider how many different ways we value outcomes, and how complicated are the paths we take to get there, it's a wonder that there exists any such thing as helpful ethical <em>advice</em>.  (Of which the strangest of all advices, <em>and yet still helpful,</em> is that "The end does not justify the means.")</p> <p>But conversely, the complicatedness of action need not say anything about the complexity of goals.  You often find people who smile wisely, and say, "Well, morality is complicated, you know, female circumcision is right in one culture and wrong in another, it's not always a bad thing to torture people.  How naive you are, how full of need for closure, that you think there are any simple rules."</p> <p>You can say, unconditionally and flatly, that killing <em>anyone</em> is a huge dose of negative terminal utility.  Yes, even Hitler.  That doesn't mean you shouldn't shoot Hitler.  It means that the net instrumental utility of shooting Hitler carries a giant dose of negative utility from Hitler's death, and an hugely larger dose of positive utility from all the other lives that would be saved as a consequence.</p> <p>Many commit the <a href="0162.html">type error</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/] that I warned against in <a href="0162.html">Terminal Values and Instrumental Values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/], and think that if the net consequential expected utility of Hitler's death is conceded to be positive, then the immediate local terminal utility must also be positive, meaning that the moral principle "Death is always a bad thing" is itself a leaky generalization.  But this is <a href="0078.html">double counting</a> [http://lesswrong.com/lw/is/fake_causality/], with utilities instead of probabilities; you're setting up a resonance between the expected utility and the utility, instead of a one-way flow from utility to expected utility.</p> <p>Or maybe it's just the urge toward a <a href="0013.html">one-sided policy debate</a> [http://lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/]: the best policy must have <em>no</em> drawbacks.</p> <p>In my moral philosophy, the <em>local</em> negative utility of Hitler's death is stable, no matter what happens to the external consequences and hence to the <em>expected</em> utility.</p> <p>Of course, you can set up a moral argument that it's an <em>inherently</em> a good thing to punish evil people, even with capital punishment for sufficiently evil people.  But you can't carry this moral argument by pointing out that the <em>consequence</em> of shooting a man with a leveled gun may be to save other lives.  This is <a href="0156.html">appealing</a> [http://lesswrong.com/lw/ky/fake_morality/] to the value of life, not appealing to the value of death.  If expected utilities are leaky and complicated, it doesn't mean that utilities must be leaky and complicated as well.  They might be!  But it would be a separate argument.</p></div> <hr><p><i>Referenced by: </i><a href="0171.html">The Hidden Complexity of Wishes</a> &#8226; <a href="0172.html">Lost Purposes</a> &#8226; <a href="0183.html">Fake Fake Utility Functions</a> &#8226; <a href="0261.html">Replace the Symbol with the Substance</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/lc/leaky_generalizations/">Leaky Generalizations</a></p></body></html>