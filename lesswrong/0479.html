<html><head><title>Optimization</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Optimization</h1><p><i>Eliezer Yudkowsky, 13 September 2008 04:00PM</i></p><div><blockquote><p>"However many ways there may be of being alive, it is certain that there are vastly more ways of being dead."<br>  &#160;&#160; &#160;&#160; -- <a href="http://www.secularhumanism.org/library/fi/dawkins_18_3.html">Richard Dawkins</a> [http://www.secularhumanism.org/library/fi/dawkins_18_3.html]</p></blockquote><p>In the coming days, I expect to be asked:  "Ah, but what do you mean by 'intelligence'?"  By way of untangling some of my dependency network for future posts, I here summarize some of my notions of "optimization".</p> <p>Consider a car; say, a Toyota Corolla.  The Corolla is made up of some number of atoms; say, on the rough order of 10<sup>29</sup>.  If you consider all possible ways to arrange 10<sup>29</sup> atoms, only an infinitesimally tiny fraction of possible configurations would qualify as a car; if you picked one random configuration per Planck interval, many ages of the universe would pass before you hit on a wheeled wagon, let alone an internal combustion engine.</p> <p>Even restricting our attention to running vehicles, there is an astronomically huge design space of possible vehicles that could be composed of the same atoms as the Corolla, and most of them, from the perspective of a human user, won't work quite as well.  We could take the parts in the Corolla's air conditioner, and mix them up in thousands of possible configurations; nearly all these configurations would result in a vehicle lower in our preference ordering, still recognizable as a car but lacking a working air conditioner.</p> <p>So there are many more configurations corresponding to nonvehicles, or vehicles <em>lower in our preference ranking</em>, than vehicles ranked <em>greater than or equal to</em> the Corolla.</p> <p>Similarly with the problem of planning, which also involves hitting tiny targets in a huge search space.  Consider the number of possible legal chess moves versus the number of winning moves.</p> <p>Which suggests one theoretical way to measure optimization - to quantify the power of a mind or mindlike process:</p><a id="more"></a><p>Put a measure on the state space - if it's discrete, you can just count.  Then collect all the states which are equal to or greater than the observed outcome, in that optimization process's implicit or explicit preference ordering.  Sum or integrate over the total size of all such states.  Divide by the total volume of the state space.  This gives you the power of the optimization process measured in terms of the improbabilities that it can produce - that is, improbability of a random selection producing an equally good result, relative to a measure and a preference ordering.</p> <p>If you prefer, you can take the reciprocal of this improbability (1/1000 becomes 1000) and then take the logarithm base 2.  This gives you the power of the optimization process in bits.  An optimizer that exerts 20 bits of power can hit a target that's one in a million.</p> <p>When I think you're a powerful intelligence, and I think I know something about your preferences, then I'll predict that you'll steer reality into regions that are higher in your preference ordering.   The more intelligent I believe you are, the more probability I'll concentrate into outcomes that I believe are higher in your preference ordering. </p> <p>There's a number of subtleties here, some less obvious than others.  I'll return to this whole topic in a later sequence.  Meanwhile: </p> <p>* A tiny fraction of the design space does describe vehicles that we would recognize as faster, more fuel-efficient, safer than the Corolla, so the Corolla is not <em>optimal.</em>  The Corolla is, however, <em>optimized,</em> because the human designer had to hit an infinitesimal target in design space just to create a working car, let alone a car of Corolla-equivalent quality.  This is not to be taken as praise of the Corolla, as such; you could say the same of the <a href="http://www.mctdirect.com/columnist/barrysample.php">Hillman Minx</a> [http://www.mctdirect.com/columnist/barrysample.php]. You can't build so much as a wooden wagon by sawing boards into random shapes and nailing them together according to coinflips.</p> <p>* When I talk to a popular audience on this topic, someone usually says:  "But isn't this what the creationists argue?  That if you took a bunch of atoms and put them in a box and shook them up, it would be astonishingly improbable for a fully functioning rabbit to fall out?"  But the logical flaw in the creationists' argument is <em>not</em> that randomly reconfiguring molecules <em>would</em> by pure chance assemble a rabbit.  The logical flaw is that there is a process, natural selection, which, through the <em>non-chance</em> retention of chance mutations, <em>selectively</em> accumulates complexity, until a few billion years later it produces a rabbit.</p> <p>* I once heard a senior mainstream AI type suggest that we might try to quantify the intelligence of an AI system in terms of its RAM, processing power, and sensory input bandwidth.  This at once reminded me of a <a href="http://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html">quote</a> [http://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html] from Dijkstra:  "If we wish to count lines of code, we should not regard them as 'lines produced' but as 'lines spent': the current conventional wisdom is so foolish as to book that count on the wrong side of the ledger."  If you want to measure the <em>intelligence</em> of a system, I would suggest measuring its optimization power as before, but then dividing by the resources used.  Or you might measure the degree of prior cognitive optimization required to achieve the same result using equal or fewer resources.  Intelligence, in other words, is <em>efficient</em> optimization.  This is why I say that <a href="0151.html">evolution is stupid by human standards</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/], even though we can't <em>yet</em> build a butterfly:  Human engineers use vastly less time/material resources than a global ecosystem of millions of species proceeding through biological evolution, and so we're catching up fast.</p> <p>* The notion of a "powerful optimization process" is necessary and sufficient to a discussion about an Artificial Intelligence that could harm or benefit humanity on a global scale.  If you say that an AI is mechanical and therefore "not really intelligent", and it outputs an action sequence that <a href="0358.html">hacks into the Internet, constructs molecular nanotechnology</a> [http://lesswrong.com/lw/qk/that_alien_message/] and wipes the solar system clean of human(e) intelligence, you are still dead.  Conversely, an AI that only has a very weak ability steer the future into regions high in its preference ordering, will not be able to much benefit or much harm humanity.</p> <p>* How do you know a mind's preference ordering?  If this can't be taken for granted, then you use some of your evidence to infer the mind's preference ordering, and then use the inferred preferences to infer the mind's power, then use those two beliefs to testably predict future outcomes.  Or you can use the Minimum Message Length formulation of Occam's Razor: if you send me a message telling me what a mind wants and how powerful it is, then this should enable you to compress your description of future events and observations, so that the total message is shorter.  Otherwise there is no predictive benefit to viewing a system as an optimization process.</p> <p>* In general, it is useful to think of a process as "optimizing" when it is easier to predict by thinking about its goals, than by trying to predict its exact internal state and exact actions.  If you're playing chess against Deep Blue, you will find it much easier to predict that Deep Blue will win (that is, the <em>final</em> board position will occupy the class of states previously labeled "wins for Deep Blue") than to predict the <em>exact</em> final board position or Deep Blue's <em>exact</em> sequence of moves.  Normally, it is not possible to predict, say, the final state of a billiards table after a shot, without extrapolating all the events along the way.</p> <p>* Although the human cognitive architecture uses the same label "<a href="0432.html">good</a> [http://lesswrong.com/lw/sm/the_meaning_of_right/]" to reflect judgments about <a href="0162.html">terminal values and instrumental values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/], this doesn't mean that all sufficiently powerful optimization processes share the same preference ordering.  Some possible minds will be steering the future into regions that are not <a href="0432.html">good</a> [http://lesswrong.com/lw/sm/the_meaning_of_right/].</p> <p>* If you came across alien machinery in space, then you might be able to infer the presence of optimization (and hence presumably powerful optimization processes standing behind it as a cause) without inferring the aliens' final goals, by way of noticing the fulfillment of convergent instrumental values.  You can look at cables through which large electrical currents are running, and be astonished to realize that the cables are flexible high-temperature high-amperage superconductors; an amazingly good solution to the <em>subproblem</em> of transporting electricity that is generated in a central location and used distantly.  You can assess this, even if you have no idea what the electricity is being used <em>for.</em></p> <p>* If you want to take probabilistic outcomes into account in judging a mind's wisdom, then you have to know or infer a <em>utility function</em> for the mind, not just a preference ranking for the optimization process.  Then you can ask how many possible plans would have equal or greater expected utility.  This assumes that you have some probability distribution, which you believe to be true; but if the other mind is smarter than you, it may have a better probability distribution, in which case you will underestimate its optimization power.  The chief sign of this would be if the mind consistently achieves higher average utility than the average expected utility you assign to its plans.</p> <p>* When an optimization process seems to have an inconsistent preference ranking - for example, it's quite possible in evolutionary biology for allele A to beat out allele B, which beats allele C, which beats allele A - then you can't interpret the system as performing optimization as it churns through its cycles.  Intelligence is efficient optimization; churning through <a href="0233.html">preference cycles</a> [http://lesswrong.com/lw/n3/circular_altruism/] is stupid, unless the interim states of churning have high terminal utility.</p> <p>* For domains outside the small and formal, it is not possible to exactly measure optimization, just as it is not possible to do exact Bayesian updates or to perfectly maximize expected utility.  Nonetheless, optimization can be a useful concept, just like the concept of Bayesian probability or expected utility - it describes the ideal you're trying to approximate with other measures.</p></div> <hr><p><i>Referenced by: </i><a href="0491.html">My Naturalistic Awakening</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/tx/optimization/">Optimization</a></p></body></html>