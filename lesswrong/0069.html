<html><head><title>Update Yourself Incrementally</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Update Yourself Incrementally</h1><p><i>Eliezer Yudkowsky, 14 August 2007 02:56PM</i></p><div><p><a href="0010.html">Politics is the mind-killer</a> [http://lesswrong.com/lw/gw/politics_is_the_mindkiller/].  Debate is war, <a href="0013.html">arguments are soldiers</a> [http://lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/].  There is the temptation to search for ways to <a href="0054.html">interpret every possible experimental result</a> [http://lesswrong.com/lw/i4/belief_in_belief/] to confirm your theory, like securing a citadel against every possible line of attack.  This you cannot do.  It is mathematically impossible. <a href="0068.html">For every expectation of evidence, there is an equal and opposite expectation of counterevidence.</a> [http://lesswrong.com/lw/ii/conservation_of_expected_evidence/]</p> <p>But it's okay if your cherished belief isn't <em>perfectly</em> defended.  If the hypothesis is that the coin comes up heads 95% of the time, then one time in twenty you will see what looks like contrary evidence.  This is okay.  It's normal.  It's even expected, so long as you've got nineteen supporting observations for every contrary one.  A probabilistic model can <a href="0066.html">take a hit or two</a> [http://lesswrong.com/lw/ig/i_defy_the_data/], and still survive, so long as the hits don't <em>keep on</em> coming in.</p> <p>Yet it is widely believed, especially in the court of public opinion, that a true theory can have <em>no </em>failures and a false theory <em>no</em> successes.</p> <p><a id="more"></a></p> <p>You find people holding up a single piece of what they conceive to be evidence, and claiming that their theory can 'explain' it, as though this were all the support that any theory needed.  Apparently a false theory can have <em>no</em> supporting evidence; it is impossible for a false theory to fit even a single event.  Thus, a single piece of confirming evidence is all that any theory needs.</p> <p>It is only slightly less foolish to hold up a single piece of <em>probabilistic</em> counterevidence as disproof, as though it were impossible for a correct theory to have even a <em>slight</em> argument against it.  But this is how humans have argued for ages and ages, trying to defeat all enemy arguments, while denying the enemy even a single shred of support.  People want their debates to be one-sided; they are accustomed to a world in which their preferred theories have not one iota of antisupport.  Thus, allowing a single item of probabilistic counterevidence would be the end of the world.</p> <p>I just know someone in the audience out there is going to say, "But you <em>can't</em> concede even a single point if you want to win debates in the real world!  If you concede that any counterarguments exist, the Enemy will harp on them over and over&#8212;you can't let the Enemy do that!  You'll <em>lose!</em>  What could be more viscerally terrifying than <em>that?</em>"</p> <p>Whatever.  Rationality is not for winning debates, it is for deciding which side to join.  If you've already decided which side to argue for, the work of rationality is <em>done</em> within you, whether well or poorly.  But how can you, yourself, decide which side to argue?  If <em>choosing the wrong side</em> is viscerally terrifying, even just a little viscerally terrifying, you'd best integrate <em>all</em> the evidence.</p> <p>Rationality is not a walk, but a dance.  On each step in that dance your foot should come down in exactly the correct spot, neither to the left nor to the right.  Shifting belief upward with each iota of confirming evidence. Shifting belief downward with each iota of contrary evidence.  Yes, <em>down.</em>  Even with a correct model, if it is not an exact model, you will sometimes need to revise your belief <em>down.</em></p> <p>If an iota or two of evidence happens to countersupport your belief, that's okay.  It happens, sometimes, with probabilistic evidence for non-exact theories.  (If an exact theory fails, you <em>are</em> in trouble!)  Just shift your belief downward a little&#8212;the probability, the odds ratio, or even a nonverbal weight of credence in your mind. Just shift downward a little, and <a href="0068.html">wait for more evidence</a> [http://lesswrong.com/lw/ii/conservation_of_expected_evidence/]. If the theory is true, supporting evidence will come in shortly, and the probability will climb again.  If the theory is false, you don't really want it anyway.</p> <p>The problem with using black-and-white, binary, qualitative reasoning is that any single observation either destroys the theory or it does not.  When not even a single contrary observation is allowed, <a href="0066.html">it creates cognitive dissonance and has to be argued away</a> [http://lesswrong.com/lw/ig/i_defy_the_data/]. And this rules out incremental progress; it rules out correct integration of all the evidence.  Reasoning probabilistically, we realize that on average, a correct theory will generate a greater weight of support than countersupport.  And so you can, <em>without fear,</em> say to yourself:  "This is gently contrary evidence, I will shift my belief downward".  Yes, <em>down.</em>  It does not destroy your cherished theory.  That is qualitative reasoning; think quantitatively.</p> <p><a href="0068.html">For every expectation of evidence, there is an equal and opposite expectation of counterevidence.</a> [http://lesswrong.com/lw/ii/conservation_of_expected_evidence/] On every occasion, you must, on average, anticipate revising your beliefs downward as much as you anticipate revising them upward.  If you think you already know what evidence will come in, then you must already be fairly sure of your theory&#8212;probability close to 1&#8212;which doesn't leave much room for the probability to go further upward.  And however unlikely it seems that you will encounter disconfirming evidence, the resulting downward shift must be large enough to precisely balance the anticipated gain on the other side.  The weighted mean of your expected posterior probability must equal your prior probability.</p> <p>How silly is it, then, to be <a href="0058.html">terrified</a> [http://lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/] of revising your probability downward, if you're bothering to investigate a matter at all?  On average, you must anticipate as much downward shift as upward shift from every individual observation.</p> <p>It may perhaps happen that an iota of antisupport comes in again, and again and again, while new support is slow to trickle in.  You may find your belief drifting downward and further downward.  Until, finally, you realize from which quarter the winds of evidence are blowing against you.  In that moment of realization, there is no point in constructing excuses.  In that moment of realization, you have <em>already relinquished</em> your cherished belief.  Yay!  Time to celebrate!  Pop a champagne bottle or send out for pizza!  You can't <a href="0022.html">become stronger</a> [http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/] by keeping the beliefs you started with, after all.</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization"><em>Against Rationalization</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0070.html">One Argument Against An Army</a> [http://lesswrong.com/lw/ik/one_argument_against_an_army/]"</p> <p style="text-align:right">Previous post: "<a href="0028.html">Knowing About Biases Can Hurt People</a> [http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq07.html">Sequence 07: Against Rationalization</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0028.html">Knowing About Biases Can Hurt People</a></p></td><td><p><i>Next: </i><a href="0070.html">One Argument Against An Army</a></p></td></tr></table><p><i>Referenced by: </i><a href="0028.html">Knowing About Biases Can Hurt People</a> &#8226; <a href="0070.html">One Argument Against An Army</a> &#8226; <a href="0390.html">LA-602 vs. RHIC Review</a> &#8226; <a href="0509.html">Crisis of Faith</a> &#8226; <a href="0566.html">Singletons Rule OK</a> &#8226; <a href="0708.html">That Crisis thing seems pretty useful</a> &#8226; <a href="0789.html">Demands for Particular Proof: Appendices</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ij/update_yourself_incrementally/">Update Yourself Incrementally</a></p></body></html>