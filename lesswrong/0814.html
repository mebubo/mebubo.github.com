<html><head><title>Causal Diagrams and Causal Models</title></head><body><h1>Causal Diagrams and Causal Models</h1><p><i>Eliezer Yudkowsky, 12 October 2012 09:49PM</i></p><div><p>Suppose a general-population survey shows that people who exercise less, weigh more. You don't have any known direction of <em>time</em> in the data - you don't know which came first, the increased weight or the diminished exercise. And you didn't randomly assign half the population to exercise less; you just surveyed an existing population.</p> <p>The statisticians who discovered causality were trying to find a way to distinguish, within survey data, the direction of cause and effect - whether, as common sense would have it, more obese people exercise less <em>because</em> they find physical activity less rewarding; or whether, as in the <a href="http://en.wikipedia.org/wiki/Just-world_hypothesis">virtue theory of metabolism</a> [http://en.wikipedia.org/wiki/Just-world_hypothesis], lack of exercise actually <em>causes</em> weight gain due to divine punishment for the sin of sloth.</p> <table border="0"> <tbody> <tr> <td><img src="41eaa27c.svg" alt=""></td> <td valign="center"> <h1><strong> vs. </strong></h1> </td> <td><img src="edee910e.svg" alt=""></td> </tr> </tbody> </table> <p>The usual way to resolve this sort of question is by <em>randomized</em> <em>intervention.</em> If you randomly assign half your experimental subjects to exercise more, and afterward the increased-exercise group doesn't lose any weight compared to the control group <a name="note1back"></a><a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note1">[1]</a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note1], you could rule out causality <em>from</em> exercise <em>to</em> weight, and conclude that the correlation between weight and exercise is probably due to physical activity being less fun when you're overweight <a name="note3back"></a><a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note3">[3]</a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note3]. The question is whether you can get causal data <em>without</em> interventions.</p> <p>For a long time, the conventional wisdom in philosophy was that this was impossible unless you knew the direction of time and knew which event had happened first. Among some philosophers of science, there was a belief that the "direction of causality" was a <em>meaningless</em> question, and that in the universe itself there were <em>only</em> correlations - that "cause and effect" was something unobservable and undefinable, that only unsophisticated non-statisticians believed in due to their lack of formal training:</p> <p style="padding-left: 30px;">"The law of causality, I believe, like much that passes muster among philosophers, is a relic of a bygone age, surviving, like the monarchy, only because it is erroneously supposed to do no harm." -- Bertrand Russell (he later changed his mind)</p> <p style="padding-left: 30px;">"Beyond such discarded fundamentals as 'matter' and 'force' lies still another fetish among the inscrutable arcana of modern science, namely, the category of cause and effect." -- Karl Pearson</p> <p>The famous statistician Fisher, who was also a smoker, testified before Congress that the correlation between smoking and lung cancer couldn't prove that the former caused the latter.  We have remnants of this type of reasoning in old-school "Correlation does not imply causation", without the now-standard appendix, "<a href="http://xkcd.com/552/">But it sure is a hint</a> [http://xkcd.com/552/]".</p> <p>This skepticism was overturned by a surprisingly simple mathematical observation.<a id="more"></a></p> <p>Let's say there are three variables in the survey data: Weight, how much the person exercises, and how much time they spend on the Internet.</p> <p>For simplicity, we'll have these three variables be binary, yes-or-no observations: Y or N for whether the person has a BMI over 25, Y or N for whether they exercised at least twice in the last week, and Y or N for whether they've checked Reddit in the last 72 hours.</p> <p>Now let's say our gathered data looks like this:</p> <table border="1"> <tbody> <tr> <th>Overweight</th> <th>Exercise</th> <th>Internet</th> <th>#</th> </tr> <tr> <td>Y</td> <td>Y</td> <td>Y</td> <td> 1,119</td> </tr> <tr> <td>Y</td> <td>Y</td> <td>N</td> <td> 16,104</td> </tr> <tr> <td>Y</td> <td>N</td> <td>Y</td> <td> 11,121</td> </tr> <tr> <td>Y</td> <td>N</td> <td>N</td> <td> 60,032</td> </tr> <tr> <td>N</td> <td>Y</td> <td>Y</td> <td> 18,102</td> </tr> <tr> <td>N</td> <td>Y</td> <td>N</td> <td> 132,111</td> </tr> <tr> <td>N</td> <td>N</td> <td>Y</td> <td> 29,120</td> </tr> <tr> <td>N</td> <td>N</td> <td>N</td> <td> 155,033</td> </tr> </tbody> </table> <p>And lo, merely by eyeballing this data -</p> <p>(which is <em>totally made up</em>, so don't go actually <em>believing </em>the conclusion I'm about to draw)</p> <p>- we now realize that <em>being overweight and spending time on the Internet both cause you to exercise less,</em> presumably because exercise is less fun and you have more alternative things to do,<em> but exercising has no causal influence on body weight or Internet use.</em></p> <p>"What!" you cry. "How can you tell <em>that</em> just by inspecting those numbers? You can't say that exercise isn't <em>correlated</em> to body weight - if you just look at all the members of the population who exercise, they clearly have lower weights. 10% of exercisers are overweight, vs. 28% of non-exercisers.  How could you rule out the obvious causal explanation for that correlation, just by looking at this data?"</p> <hr> <p>There's a wee bit of math involved.  It's <em>simple</em> math - the part we'll use doesn't involve solving equations or complicated proofs -but we do have to introduce a wee bit of novel math to explain how the heck we got there from here.</p> <p>Let me start with a question that turned out - to the surprise of many investigators involved - to be highly related to the issue we've just addressed.</p> <p>Suppose that earthquakes and burglars can both set off burglar alarms.  If the burglar alarm in your house goes off, it might be because of an actual burglar, but it might <em>also </em>be because a minor earthquake rocked your house and triggered a few sensors. Early investigators in Artificial Intelligence, who were trying to represent all high-level events using primitive tokens in a first-order logic (for reasons of historical stupidity we won't go into) were stymied by the following apparent paradox:</p> <ul> <li> <p>If you tell me that my burglar alarm went off, I infer a burglar, which I will represent in my first-order-logical database using a theorem <span style="font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; ">&#8866;</span> ALARM&#160;&#8594;&#160;BURGLAR. (The symbol "<span style="font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; ">&#8866;</span>" is called "<a href="http://en.wikipedia.org/wiki/Turnstile_(symbol)">turnstile</a> [http://en.wikipedia.org/wiki/Turnstile_(symbol)]" and means "the logical system asserts that".)</p> </li> <li> <p>If an earthquake occurs, it will set off burglar alarms. I shall represent this using the theorem <span style="font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; ">&#8866;</span> EARTHQUAKE&#160;&#8594;&#160;ALARM, or "earthquake implies alarm".</p> </li> <li> <p>If you tell me that my alarm went off, and then further tell me that an earthquake occurred, it <em>explains away</em> my burglar alarm going off. I don't need to explain the alarm by a burglar, because the alarm has already been explained by the earthquake. I conclude there was no burglar. I shall represent this by adding a theorem which says <span style="font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; ">&#8866;</span> (EARTHQUAKE &amp; ALARM)&#160;&#8594;&#160;NOT BURGLAR.</p> </li> </ul> <p>Which represents a logical contradiction, and for a while there were attempts to develop "non-monotonic logics" so that you could retract conclusions given additional data. <a href="0547.html">This didn't work very well, since the underlying structure of reasoning was a terrible fit for the structure of classical logic, even when mutated.</a> [http://lesswrong.com/lw/vt/the_nature_of_logic/]</p> <p>Just changing certainties to quantitative probabilities can fix many problems with classical logic, and one might think that this case was likewise easily fixed.</p> <p>Namely, just write a probability table of all possible combinations of earthquake or &#172;earthquake, burglar or &#172;burglar, and alarm or &#172;alarm (where&#160;&#172; is the logical negation symbol), with the following entries:</p> <table border="1"> <tbody> <tr> <th>Burglar</th> <th>Earthquake</th> <th>Alarm</th> <th>%</th> </tr> <tr> <td>b</td> <td>e</td> <td>a</td> <td>.000162</td> </tr> <tr> <td>b</td> <td>e</td> <td>&#172;a</td> <td>.0000085</td> </tr> <tr> <td>b</td> <td>&#172;e</td> <td>a</td> <td>.0151</td> </tr> <tr> <td>b</td> <td>&#172;e</td> <td>&#172;a</td> <td>.00168</td> </tr> <tr> <td>&#172;b</td> <td>e</td> <td>a</td> <td>.0078</td> </tr> <tr> <td>&#172;b</td> <td>e</td> <td>&#172;a</td> <td>.002</td> </tr> <tr> <td>&#172;b</td> <td>&#172;e</td> <td>a</td> <td>.00097</td> </tr> <tr> <td>&#172;b</td> <td>&#172;e</td> <td>&#172;a</td> <td>.972</td> </tr> </tbody> </table> <p>Using the operations of <em>marginalization</em> and <em>conditionalization,</em> we get the desired reasoning back out:</p> <p>Let's start with the <em>probability of a burglar given an alarm,</em> p(burglar|alarm). By the law of conditional probability,</p> <p><img src="77db6ba5" alt="" height="44" width="112"></p> <p>i.e. the relative fraction of cases where there's an alarm <em>and</em> a burglar, within the set of all cases where there's an alarm.</p> <p>The table doesn't directly tell us p(alarm &amp; burglar)/p(alarm), but by the law of marginal probability,</p> <p><img src="e176c1f4" alt="" height="19" width="405"></p> <p>Similarly, to get the probability of an alarm going off, p(alarm), we add up all the different sets of events that involve an alarm going off - entries 1, 3, 5, and 7 in the table.</p> <p>So the entire set of calculations looks like this:</p> <ul> <li> <p>If I hear a burglar alarm, I conclude there was probably (63%) a burglar. </p> <p><img src="fc233030" alt="" height="44" width="443"></p> </li> <li> <p>If I learn about an earthquake, I conclude there was probably (80%) an alarm. </p> <p><img src="55b41027" alt="" height="44" width="445"></p> </li> <li> <p>I hear about an alarm and then hear about an earthquake; I conclude there was probably (98%) no burglar. </p> <p><img src="fe3152de" alt="" height="44" width="417"></p> </li> </ul> <p>Thus, a joint probability distribution is indeed capable of <em>representing</em> the reasoning-behaviors we want.</p> <p>So is our problem solved? Our work done?</p> <p>Not in real life or real Artificial Intelligence work. The problem is that this solution doesn't scale. <em>Boy howdy</em>, does it not scale! If you have a model containing <em>forty</em> binary variables - alert readers may notice that the observed physical universe contains at least forty things - and you try to write out the <em>joint probability distribution</em> over all combinations of those variables, it looks like this:</p> <table border="1"> <tbody> <tr> <td>.0000000000112</td> <td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY</td> </tr> <tr> <td>.000000000000034</td> <td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYN</td> </tr> <tr> <td>.00000000000991</td> <td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNY</td> </tr> <tr> <td>.00000000000532</td> <td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNN</td> </tr> <tr> <td>.000000000145</td> <td>YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYNYY</td> </tr> <tr> <td> ...</td> <td>...</td> </tr> </tbody> </table> <p>(1,099,511,627,776 entries)</p> <p>This isn't merely a storage problem. In terms of storage, a trillion entries is just a terabyte or three. The real problem is <em>learning</em> a table like that. You have to deduce 1,099,511,627,776 floating-point probabilities from observed data, and the only constraint on this giant table is that all the probabilities must sum to exactly 1.0, a problem with 1,099,511,627,775 degrees of freedom. (If you know the first 1,099,511,627,775 numbers, you can deduce the 1,099,511,627,776th number using the constraint that they all sum to exactly 1.0.) It's not the storage cost that kills you in a problem with forty variables, it's the difficulty of gathering enough observational data to constrain a trillion different parameters. And in a universe containing <em>seventy</em> things, things are even worse.</p> <p><img src="02a185b8.png" alt="" height="325" width="551"></p> <p>So instead, suppose we approached the earthquake-burglar problem by trying to specify probabilities in a format where... never mind, it's easier to just give an example before stating abstract rules.</p> <p>First let's add, for purposes of further illustration, a new variable, "Recession", whether or not there's a depressed economy at the time. Now suppose that:</p> <ul> <li> <p>The probability of an earthquake is 0.01.</p> </li> <li> <p>The probability of a recession at any given time is 0.33 (or 1/3).</p> </li> <li> <p>The probability of a burglary given a recession is 0.04; or, given no recession, 0.01.</p> </li> <li> <p>An earthquake is 0.8 likely to set off your burglar alarm; a burglar is 0.9 likely to set off your burglar alarm.  <em>And </em>- we can't compute this model fully without this info - the combination of a burglar <em>and</em> an earthquake is 0.95 likely to set off the alarm; and in the absence of either burglars or earthquakes, your alarm has a 0.001 chance of going off anyway.</p> </li> </ul> <p> </p><table cellpadding="3" border="0"> <tbody> <tr valign="top"> <td rowspan="3"><img src="80d106c3.svg" alt="" height="259" width="358"></td> <td> <table border="1"> <tbody> <tr> <td>p(r)</td> <td>.33</td> </tr> <tr> <td>p(&#172;r)</td> <td>.67</td> </tr> </tbody> </table> </td> <td rowspan="3"> <table border="1"> <tbody> <tr> <td>p(a|be)</td> <td>.95</td> </tr> <tr> <td>p(a|b&#172;e)</td> <td>.9</td> </tr> <tr> <td>p(a|&#172;be)</td> <td>.797</td> </tr> <tr> <td>p(a|&#172;b&#172;e)</td> <td>.001</td> </tr> <tr> <td>p(&#172;a|be)</td> <td>.05</td> </tr> <tr> <td>p(&#172;a|b&#172;e)</td> <td>.1</td> </tr> <tr> <td>p(&#172;a|&#172;be)</td> <td>.203</td> </tr> <tr> <td>p(&#172;a|&#172;b&#172;e)</td> <td>.999</td> </tr> </tbody> </table> </td> </tr> <tr valign="top"> <td> <table border="1"> <tbody> <tr> <td>p(e)</td> <td>.01</td> </tr> <tr> <td>p(&#172;e)</td> <td>.99</td> </tr> </tbody> </table> </td> </tr> <tr valign="top"> <td> <table border="1"> <tbody> <tr> <td>p(b|r)</td> <td>.04</td> </tr> <tr> <td>p(b|&#172;r)</td> <td>.01</td> </tr> <tr> <td>p(&#172;b|r)</td> <td>.96</td> </tr> <tr> <td>p(&#172;b|&#172;r)</td> <td>.99</td> </tr> </tbody> </table> </td> </tr> </tbody> </table>

<p>According to this model, if you want to know "The probability that an earthquake occurs" - just the probability of that one variable, without talking about any others - you can directly look up p(e) = .01. On the other hand, if you want to know the probability of a burglar striking, you have to first look up the probability of a recession (.33), and then p(b|r) and p(b|&#172;r), and sum up p(b|r)*p(r) + p(b|&#172;r)*p(&#172;r) to get a net probability of .01*.66 + .04*.33 = .02 = p(b), a 2% probability that a burglar is around at some random time.</p> <p>If we want to compute the joint probability of four values for all four variables - for example, the probability that there is no earthquake <em>and</em> no recession<em> and</em> a burglar <em>and</em> the alarm goes off - this causal model computes this joint probability as the product:</p> <p><img src="d77c9008" alt="" height="19" width="470"></p> <p>In general, to go from a <em>causal model</em> to a <em>probability distribution,</em> we compute, for each setting of all the variables, the product</p> <p><img src="923e7af9" alt=""></p> <p>multiplying together the conditional probability of each variable <em>given the values of its immediate parents.</em>  (If a node has no parents, the probability table for it has just an unconditional probability, like "the chance of an earthquake is .01".)</p> <p>This is a <em>causal</em> model because it corresponds to a world in which each event is <em>directly</em> caused by only a small set of other events, its parent nodes in the graph. In this model, a recession can <em>indirectly</em> cause an alarm to go off - the recession increases the probability of a burglar, who in turn sets off an alarm - but the recession <em>only</em> acts on the alarm through the <em>intermediate cause</em> of the burglar.  (Contrast to a model where recessions set off burglar alarms directly.)</p> <table cellpadding="3" border="0"> <tbody> <tr valign="center"> <td><img src="570b67a1.svg" alt="" height="259" width="166"></td> <td> <h1>vs.</h1> </td> <td><img src="6bf88839.svg" alt="" height="259" width="171"></td> </tr> </tbody> </table> <p>The first diagram implies that once we <em>already know</em> whether or not there's a burglar, we don't learn <em>anything more</em> about the probability of a burglar alarm, if we find out that there's a recession:</p> <p><img src="23c0ee80" alt="" height="19" width="122"></p> <p>This is a fundamental illustration of <em>the locality of causality -</em> once I know there's a burglar, I know <em>everything I need to know</em> to calculate the probability that there's an alarm.  Knowing the state of Burglar <em>screens off</em> anything that Recession could tell me about Alarm - even though, if I <em>didn't </em>know the value of the Burglar variable, Recessions would appear to be statistically correlated with Alarms.  The present screens off the past from the future; in a causal system, if you know the <em>exact, complete</em> state of the present, the state of the past has no further physical relevance to computing the future.  It's how, in a system containing many correlations (like the recession-alarm correlation), it's still possible to compute each variable just by looking at a small number of immediate neighbors.</p> <p>Constraints like this are also how we can store a causal model - and much more importantly, <em>learn</em> a causal model - with many fewer parameters than the naked, raw, joint probability distribution.</p> <p>Let's illustrate this using a simplified version of this graph, which only talks about earthquakes and recessions. We could consider three hypothetical causal diagrams over only these two variables:</p> <table cellpadding="3" border="0"> <tbody> <tr> <td rowspan="2"><img src="c58a7051.svg" alt="" height="62" width="371"></td> <td> <table border="1"> <tbody> <tr> <td>p(r)</td> <td>0.03</td> </tr> <tr> <td>p(&#172;r)</td> <td>0.97</td> </tr> </tbody> </table> </td> </tr> <tr> <td> <table border="1"> <tbody> <tr> <td>p(e)</td> <td>0.29</td> </tr> <tr> <td>p(&#172;e)</td> <td>0.71</td> </tr> </tbody> </table> </td> </tr> <tr> <td colspan="2" align="center"> <h2>p(E&amp;R)=p(E)p(R)</h2> </td> </tr> </tbody> </table> <p> </p> <table cellpadding="3" border="0"> <tbody> <tr> <td rowspan="2"><img src="42392ee0.svg" alt="" height="160" width="192"></td> <td> <table border="1"> <tbody> <tr> <td>p(e)</td> <td>0.29</td> </tr> <tr> <td>p(&#172;e)</td> <td>0.71</td> </tr> </tbody> </table> </td> </tr> <tr> <td> <table border="1"> <tbody> <tr> <td>p(r|e)</td> <td>0.15</td> </tr> <tr> <td>p(&#172;r|e)</td> <td>0.85</td> </tr> <tr> <td>p(r|&#172;e)</td> <td>0.03</td> </tr> <tr> <td>p(&#172;r|&#172;e)</td> <td>0.97</td> </tr> </tbody> </table> </td> </tr> <tr align="center"> <td colspan="2"> <h2>p(E&amp;R) = p(E)p(R|E)</h2> </td> </tr> </tbody> </table> <p> </p> <table cellpadding="3" border="0"> <tbody> <tr> <td rowspan="2"><img src="810ebf57.svg" alt="" height="160" width="192"></td> <td> <table border="1"> <tbody> <tr> <td>p(r)</td> <td>0.03</td> </tr> <tr> <td>p(&#172;r)</td> <td>0.97</td> </tr> </tbody> </table> </td> </tr> <tr> <td> <table border="1"> <tbody> <tr> <td>p(e|r)</td> <td>0.24</td> </tr> <tr> <td>p(&#172;e|r)</td> <td>0.76</td> </tr> <tr> <td>p(e|&#172;r)</td> <td>0.09</td> </tr> <tr> <td>p(&#172;e|&#172;r)</td> <td>0.91</td> </tr> </tbody> </table> </td> </tr> <tr> <td colspan="2" align="center"> <h2>p(E&amp;R) = p(R)p(E|R)</h2> </td> </tr> </tbody> </table> <p>Let's consider the first hypothesis - that there's no causal arrows connecting earthquakes and recessions. If we build a <em>causal model</em> around this diagram, it has 2 real degrees of freedom - a degree of freedom for saying that the probability of an earthquake is, say, 29% (and hence that the probability of not-earthquake is necessarily 71%), and another degree of freedom for saying that the probability of a recession is 3% (and hence the probability of not-recession is constrained to be 97%).</p> <p>On the other hand, the full joint probability distribution would have 3 degrees of freedom - a free choice of (earthquake&amp;recession), a choice of p(earthquake&amp;&#172;recession), a choice of p(&#172;earthquake&amp;recession), and then a constrained p(&#172;earthquake&amp;&#172;recession) which must be equal to 1 minus the sum of the other three, so that all four probabilities sum to 1.0.</p> <p>By the pigeonhole principle (you can't fit 3 pigeons into 2 pigeonholes) there must be some joint probability distributions which <em>cannot be represented</em> in the first causal structure. This means the first causal structure is <em>falsifiable;</em> there's survey data we can get which would lead us to reject it as a hypothesis. In particular, the first causal model requires:</p> <p><img src="9fb2e70a" alt="" height="19" width="126"></p> <p>or equivalently</p> <p><img src="ae574092" alt=""></p> <p>or equivalently</p> <p><img src="7a1d8a38" alt="" height="19" width="100"></p> <p>which is a <em>conditional independence</em> constraint - it says that learning about recessions doesn't tell us anything about the probability of an earthquake or vice versa. If we find that earthquakes and recessions are highly correlated in the observed data - if earthquakes and recessions go together, or earthquakes and the <em>absence</em> of recessions go together - it falsifies the first causal model.</p> <p>For example, let's say that in your state, an earthquake is 0.1 probable per year and a recession is 0.2 probable. If we suppose that earthquakes don't cause recessions, earthquakes are not an effect of recessions, and that there aren't hidden aliens which produce both earthquakes and recessions, then we should find that years in which there are <em>earthquakes and recessions</em> happen around 0.02 of the time. If instead earthquakes and recessions happen 0.08 of the time, then the probability of a recession <em>given</em> an earthquake is 0.8 instead of 0.2, and we should much more strongly expect a recession any time we are told that an earthquake has occurred. Given enough samples, this falsifies the theory that these factors are unconnected; or rather, the more samples we have, the more we disbelieve that the two events are unconnected.</p> <p>On the other hand, we can't tell apart the second two possibilities from survey data, because both causal models have 3 degrees of freedom, which is the size of the full joint probability distribution. (In general, <em>fully connected</em> causal graphs in which there's a line between every pair of nodes, have the same number of degrees of freedom as a raw joint distribution - and 2 nodes connected by 1 line are "fully connected".) We can't tell if earthquakes are 0.1 likely and cause recessions with 0.8 probability, or recessions are 0.2 likely and cause earthquakes with 0.4 probability (or if there are hidden aliens which on 6% of years show up and cause earthquakes and recessions with probability 1).</p> <p>With larger universes, the difference between <em>causal models</em> and <em>joint probability distributions</em> becomes a lot more striking. If we're trying to reason about a million binary variables connected in a huge causal model, and each variable could have up to four direct 'parents' - four other variables that <em>directly</em> exert a causal effect on it - then the total number of free parameters would be at most... 16 million!</p> <p>The number of free parameters in a raw joint probability distribution over a million binary variables would be 2<sup>1,000,000</sup>. Minus one.</p> <p>So causal models which are <em>less</em> than fully connected - in which most objects in the universe are not the direct cause or direct effect of everything else in the universe - are very strongly <em>falsifiable;</em> they only allow probability distributions (hence, observed frequencies) in an infinitesimally tiny range of all possible joint probability tables. Causal models very strongly <a href="0053.html">constrain anticipation</a> [http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/] - disallow almost all possible patterns of observed frequencies - and gain mighty <a href="http://yudkowsky.net/rational/technical">Bayesian advantages</a> [http://yudkowsky.net/rational/technical] when these predictions come true.</p> <p>To see this effect at work, let's consider the <em>three</em> variables Recession, Burglar, and Alarm. </p><table border="1"> <tbody> <tr> <th>Alarm</th> <th>Burglar</th> <th>Recession</th> <th>%</th> </tr> <tr> <td>Y</td> <td>Y</td> <td>Y</td> <td>.012</td> </tr> <tr> <td>N</td> <td>Y</td> <td>Y</td> <td>.0013</td> </tr> <tr> <td>Y</td> <td>N</td> <td>Y</td> <td>.00287</td> </tr> <tr> <td>N</td> <td>N</td> <td>Y</td> <td>.317</td> </tr> <tr> <td>Y</td> <td>Y</td> <td>N</td> <td>.003</td> </tr> <tr> <td>N</td> <td>Y</td> <td>N</td> <td>.000333</td> </tr> <tr> <td>Y</td> <td>N</td> <td>N</td> <td>.00591</td> </tr> <tr> <td>N</td> <td>N</td> <td>N</td> <td>.654</td> </tr> </tbody> </table>

<p>All three variables seem correlated to each other when considered two at a time. For example, if we consider Recessions and Alarms, they should seem correlated because recessions cause burglars which cause alarms. If we learn there was an alarm, for example, we conclude it's more probable that there was a recession. So since all three variables are correlated, can we distinguish between, say, these three causal models?</p> <table cellpadding="5" border="0"> <tbody> <tr valign="center"> <td><img src="570b67a1.svg" alt="" height="259" width="166"></td> <td><img src="fdf72a08.svg" alt="" height="160" width="296"></td> <td><img src="d68cf928.svg" alt="" height="259" width="166"></td> </tr> <tr valign="center"> <td align="center"><img src="b0e59239" alt="" height="19" width="192">&#160;&#160;</td> <td align="center"><img src="950d0318" alt="" height="19" width="189">&#160;&#160;</td> <td align="center"><img src="392fb671" alt="" height="19" width="194">&#160;&#160;</td> </tr> </tbody> </table> <p>Yes we can! Among these causal models, the prediction which only the first model makes, which is not shared by either of the other two, is that <em>once we know whether a burglar is there, </em>we learn nothing <em>more</em> about whether there was an alarm by finding out that there was a recession, since recessions only affect alarms through the intermediary of burglars:</p> <p><img src="6c28ff3e" alt="" height="19" width="122"></p> <p>But the third model, in which recessions directly cause alarms, which only then cause burglars, does <em>not</em> have this property. If I know that a burglar has appeared, it's likely that an alarm caused the burglar - but it's even <em>more</em> likely that there was an alarm, if there was a recession around to cause the alarm! So the third model predicts:</p> <p><img src="807f405a" alt="" height="19" width="122"></p> <p>And in the second model, where alarms and recessions both cause burglars, we again don't have the conditional independence. If we know that there's a burglar, then we think that either an alarm or a recession caused it; and if we're told that there's an alarm, we'd conclude it was less likely that there was a recession, since the recession had been explained away.</p> <p>(This may seem a bit clearer by considering the scenario B-&gt;A&lt;-E, where burglars and earthquakes both cause alarms. If we're told the value of the bottom node, that there was an alarm, the probability of there being a burglar is <em>not</em> independent of whether we're told there was an earthquake - the two top nodes are <em>not</em> conditionally independent <em>once we condition on the bottom node.</em>)</p> <p>On the other hand, we can't tell the difference between:</p> <table cellpadding="3" border="0"> <tbody> <tr valign="center"> <td><img src="570b67a1.svg" alt="" height="259" width="166"></td> <td> <h1>vs.</h1> </td> <td><img src="d9b82fa4.svg" alt="" height="259" width="166"></td> <td> <h1>vs.</h1> </td> <td><img src="395f482b.svg" alt="" height="160" width="296"></td> </tr> </tbody> </table> <p>using <em>only</em> this data and no other variables, because all three causal structures predict the same pattern of conditional dependence and independence - three variables which all appear mutually correlated, but Alarm and Recession become independent once you condition on Burglar.</p> <p>Being able to read off patterns of conditional dependence and independence is an art known as "D-separation", and if you're good at it you can glance at a diagram like this...</p> <p><img src="3327529f.svg" alt="" height="358" width="272"></p> <p>...and see that, once we already know the Season, whether the Sprinkler is on and whether it is Raining are conditionally independent of each other - if we're told that it's Raining we conclude nothing about whether or not the Sprinkler is on. But if we then further observe that the sidewalk is Slippery, then Sprinkler and Rain become conditionally dependent once more, because if the Sidewalk is Slippery then it is probably Wet and this can be explained by either the Sprinkler or the Rain but probably not both, i.e. if we're told that it's Raining we conclude that it's less likely that the Sprinkler was on.</p> <hr> <p>Okay, back to the obesity-exercise-Internet example. You may recall that we had the following observed frequencies:</p> <table border="0"> </table> <table border="1"> <tbody> <tr> <th>Overweight</th> <th>Exercise</th> <th>Internet</th> <th>#</th> </tr> <tr> <td>Y</td> <td>Y</td> <td>Y</td> <td> 1,119</td> </tr> <tr> <td>Y</td> <td>Y</td> <td>N</td> <td> 16,104</td> </tr> <tr> <td>Y</td> <td>N</td> <td>Y</td> <td> 11,121</td> </tr> <tr> <td>Y</td> <td>N</td> <td>N</td> <td> 60,032</td> </tr> <tr> <td>N</td> <td>Y</td> <td>Y</td> <td> 18,102</td> </tr> <tr> <td>N</td> <td>Y</td> <td>N</td> <td> 132,111</td> </tr> <tr> <td>N</td> <td>N</td> <td>Y</td> <td> 29,120</td> </tr> <tr> <td>N</td> <td>N</td> <td>N</td> <td> 155,033</td> </tr> </tbody> </table> <p>Do you see where this is going?</p> <p>"Er," you reply, "Maybe if I had a calculator and ten minutes... you want to just go ahead and spell it out?"</p> <p>Sure! First, we <em>marginalize</em> over the 'exercise' variable to get the table for just weight and Internet use. We do this by taking the 1,119 people who are YYY, overweight and Reddit users and exercising, and the 11,121 people who are overweight and non-exercising and Reddit users, YNY, and adding them together to get 12,240 total people who are overweight Reddit users:</p> <table border="1"> <tbody> <tr> <th>Overweight</th> <th>Internet</th> <th>#</th> </tr> <tr> <td>Y</td> <td>Y</td> <td>12,240</td> </tr> <tr> <td>Y</td> <td>N</td> <td>76,136</td> </tr> <tr> <td>N</td> <td>Y</td> <td>47,222</td> </tr> <tr> <td>N</td> <td>N</td> <td>287,144</td> </tr> </tbody> </table> <p>"And then?"</p> <p>Well, that suggests that the <em>probability</em> of using Reddit, given that your weight is normal, is the <em>same</em> as the probability that you use Reddit, given that you're overweight. 47,222 out of 334,366 normal-weight people use Reddit, and 12,240 out of 88,376 overweight people use Reddit. That's about 14% either way.</p> <p>"And so we conclude?"</p> <p>Well, first we conclude it's not particularly likely that using Reddit causes weight gain, or that being overweight causes people to use Reddit:</p> <p><img src="fbe37c44.svg" alt=""> <img src="c218c933.svg" alt="" height="160" width="195"></p> <p>If either of those causal links existed, those two variables should be <em>correlated.</em> We shouldn't find the <em>lack of correlation</em> or <em>conditional independence</em> that we just discovered.</p> <p>Next, imagine that the real causal graph looked like this:</p> <p><img src="50d2d7a2.svg" alt="" height="160" width="358"></p> <p>In this graph, exercising <em>causes</em> you to be less likely to be overweight (due to the virtue theory of metabolism), and exercising <em>causes</em> you to spend less time on the Internet (because you have less time for it).</p> <p>But in this case we should <em>not</em> see that the groups who are/aren't overweight have the same probability of spending time on Reddit. There should be an outsized group of people who are both normal-weight and non-Redditors (because they exercise), and an outsized group of non-exercisers who are overweight and Reddit-using.</p> <p>So that causal graph is also <em>ruled out</em> by the data, as are others like:</p> <p><img src="9cea3b0e.svg" alt="" height="259" width="195"> <img src="c90d55a0.svg" alt="" height="259" width="195"> <img src="a86fdc7f.svg" alt="" height="259" width="195"></p> <p> </p> <p> <img src="3ba64234.svg" alt=""></p> <p>Leaving <em>only</em> this causal graph:</p> <p><img src="21fc85b0.svg" alt="" height="160" width="358"></p> <p>Which says that weight and Internet use exert causal effects on exercise, but exercise doesn't causally affect either.</p> <p>All this discipline was invented and systematized by Judea Pearl, Peter Spirtes, Thomas Verma, and a number of other people in the 1980s and you should be quite impressed by their accomplishment, because before then, inferring causality from correlation was thought to be a fundamentally unsolvable problem. The standard volume on causal structure is <em>Causality</em> by Judea Pearl.</p> <p>Causal <em>models</em> (with specific probabilities attached) are sometimes known as "Bayesian networks" or "Bayes nets", since they were invented by Bayesians and make use of Bayes's Theorem. They have all sorts of neat computational advantages which are far beyond the scope of this introduction - e.g. in many cases you can split up a Bayesian network into parts, put each of the parts on its own computer processor, and then update on three different pieces of evidence at once using a neatly local message-passing algorithm in which each node talks only to its immediate neighbors and when all the updates are finished propagating the whole network has settled into the correct state. For more on this see Judea Pearl's <em>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</em> which is the original book on Bayes nets and still the best introduction I've personally happened to read.</p> <hr> <p><a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note1back"></a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note1back]<a name="note1"></a> <a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note1back">[1]</a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note1back] Somewhat to my own shame, I must admit to ignoring my own observations in this department - even after I saw no discernible effect on my weight or my musculature from aerobic exercise and strength training 2 hours a day 3 times a week, I didn't really start believing that the virtue theory of metabolism was <em>wrong <a name="note2back"></a></em><a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note2">[2]</a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note2] until after <a href="http://en.wikipedia.org/wiki/Gary_Taubes">other</a> [http://en.wikipedia.org/wiki/Gary_Taubes] <a href="http://sethroberts.net/">people</a> [http://sethroberts.net/] had <a href="0205.html">started</a> [http://lesswrong.com/lw/mb/lonely_dissent/] the skeptical dogpile.</p> <p><a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note2back"></a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note2back]<a name="note2"></a> <a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note2back">[2]</a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note2back] I should mention, though, that I have confirmed a personal effect where eating <em>enough </em>cookies (at a convention where no protein is available) will cause weight gain afterward. There's no other discernible correlation between my carbs/protein/fat allocations and weight gain, <em>just</em> that eating sweets in large quantities can cause weight gain afterward. This admittedly does bear with the straight-out virtue theory of metabolism, i.e., eating pleasurable foods is sinful weakness and hence punished with fat.</p> <p><a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note3back"></a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note3back]<a name="note3"></a> <a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note3back">[3]</a> [http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/#note3back] Or there might be some hidden third factor, a gene which causes both fat and non-exercise. By Occam's Razor this is more complicated and its probability is penalized accordingly, but we can't actually rule it out. It is obviously impossible to do the converse experiment where half the subjects are randomly assigned lower weights, since there's no known intervention which can cause weight loss.</p> <hr> <p><strong>Mainstream status</strong>:  This is meant to be an introduction to completely bog-standard Bayesian networks, causal models, and causal diagrams.  Any departures from mainstream academic views are errors and should be flagged accordingly.</p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners"><em>Highly Advanced Epistemology 101 for Beginners</em></a> [http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners]</p> <p style="text-align:right">Next post: "<a href="0815.html">Stuff That Makes Stuff Happen</a> [http://lesswrong.com/lw/ezu/stuff_that_makes_stuff_happen/]"</p> <p style="text-align:right">Previous post: "<a href="0813.html">The Fabric of Real Things</a> [http://lesswrong.com/lw/eva/the_fabric_of_real_things/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq17.html">Sequence 17: Highly Advanced Epistemology 101 for Beginners</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0813.html">The Fabric of Real Things</a></p></td><td><p><i>Next: </i><a href="0815.html">Stuff That Makes Stuff Happen</a></p></td></tr></table><p><i>Referenced by: </i><a href="0813.html">The Fabric of Real Things</a> &#8226; <a href="0815.html">Stuff That Makes Stuff Happen</a> &#8226; <a href="0817.html">Proofs, Implications, and Models</a> &#8226; <a href="0819.html">Causal Universes</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/">Causal Diagrams and Causal Models</a></p></body></html>