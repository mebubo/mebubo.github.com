<html><head><title>Fighting a Rearguard Action Against the Truth</title></head><body><h1>Fighting a Rearguard Action Against the Truth</h1><p><i>Eliezer Yudkowsky, 24 September 2008 01:23AM</i></p><div><p><strong>Followup to</strong>:  <a href="0489.html">That Tiny Note of Discord</a> [http://lesswrong.com/lw/u7/that_tiny_note_of_discord/], <a href="0059.html">The Importance of Saying "Oops"</a> [http://lesswrong.com/lw/i9/the_importance_of_saying_oops/]</p> <p>When we last left Eliezer<sub>2000</sub>, he was just beginning to investigate the question of how to inscribe a morality into an AI.  His reasons for doing this don't matter at all, except insofar as they happen to historically demonstrate the importance of perfectionism.  If you practice something, you may get better at it; if you investigate something, you may find out about it; the only thing that matters is that Eliezer<sub>2000</sub> is, in fact, focusing his full-time energies on thinking technically about AI morality; rather than, as previously, finding an justification for not spending his time this way.  In the end, this is all that turns out to matter.</p> <p>But as our story begins&#8212;as the sky lightens to gray and the tip of the sun peeks over the horizon&#8212;Eliezer<sub>2001</sub> hasn't yet admitted that Eliezer<sub>1997</sub> was <em>mistaken</em> in any important sense.  He's just making Eliezer<sub>1997</sub>'s strategy <em>even better</em> by including a <em>contingency</em> plan for "the unlikely event that life turns out to be meaningless"...</p> <p>...which means that Eliezer<sub>2001</sub> now has a <a href="0270.html">line of retreat</a> [http://lesswrong.com/lw/o4/leave_a_line_of_retreat/] away from his mistake.</p> <p>I don't just mean that Eliezer<sub>2001</sub> can say "Friendly AI is a contingency plan", rather than <a href="0059.html">screaming "OOPS!"</a> [http://lesswrong.com/lw/i9/the_importance_of_saying_oops/]</p> <p>I mean that Eliezer<sub>2001</sub> now actually <em>has</em> a contingency plan.  If Eliezer<sub>2001</sub> starts to doubt his 1997 metaethics, the Singularity has a fallback strategy, namely Friendly AI.  Eliezer<sub>2001</sub> can <a href="0430.html">question his metaethics without it signaling the end of the world</a> [http://lesswrong.com/lw/sk/changing_your_metaethics/].</p> <p>And his gradient has been smoothed; he can admit a 10% chance of having previously been wrong, then a 20% chance.  He doesn't have to cough out his whole mistake in one huge lump.</p> <p>If you think this sounds like Eliezer<sub>2001</sub> is <a href="0367.html">too slow</a> [http://lesswrong.com/lw/qt/class_project/], I <a href="0059.html">quite agree</a> [http://lesswrong.com/lw/i9/the_importance_of_saying_oops/].</p> <p><a id="more"></a></p> <p>Eliezer<sub>1996-2000</sub>'s strategies had been formed in the total absence of "Friendly AI" as a consideration.  The whole idea was to get a superintelligence, <em>any</em> superintelligence, as fast as possible&#8212;codelet soup, ad-hoc heuristics, evolutionary programming, open-source, anything that looked like it might work&#8212;preferably all approaches simultaneously in a Manhattan Project.  ("All parents did the things they tell their children not to do.  That's how they know to tell them not to do it."  John Moore, <em>Slay and Rescue</em>.)  It's not as if adding one more approach could <em>hurt.</em></p> <p>His attitudes toward technological progress have been formed&#8212;or more accurately, preserved from <a href="0482.html">childhood-absorbed technophilia</a> [http://lesswrong.com/lw/u0/raised_in_technophilia/]&#8212;around the assumption that any/all movement toward superintelligence is a <a href="0480.html">pure good</a> [http://lesswrong.com/lw/ty/my_childhood_death_spiral/] <a href="0013.html">without a hint of danger</a> [http://lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/].</p> <p>Looking back, what Eliezer<sub>2001</sub>  <em>needed</em> to do at this point was declare an HMC event&#8212;Halt, Melt, and Catch Fire.  One of the foundational assumptions on which everything else has been built, has been revealed as flawed.  This calls for a mental brake to a full stop: <a href="0413.html">take your weight off all beliefs built on the wrong assumption</a> [http://lesswrong.com/lw/s3/the_genetic_fallacy/], do your best to rethink everything from scratch.  This is an art I need to write more about&#8212;it's akin to the convulsive effort required to seriously clean house, after an adult religionist notices for the first time that God doesn't exist.</p> <p>But what Eliezer<sub>2001</sub> actually did was <a href="0070.html">rehearse</a> [http://lesswrong.com/lw/ik/one_argument_against_an_army/] his previous technophilic arguments for why it's difficult to ban or governmentally control new technologies&#8212;the standard arguments against "relinquishment".</p> <p>It does seem even to my modern self, that all those awful consequences which technophiles argue to follow from various kinds of government regulation, are more or less correct&#8212;it's much easier to say what someone is doing wrong, than to say the way that is right.  My modern viewpoint hasn't shifted to think that technophiles are wrong about the downsides of technophobia; but I do tend to be a lot more sympathetic to what technophobes say about the downsides of technophilia.  What previous Eliezers said about the difficulties of, e.g., the government doing anything sensible about Friendly AI, still seems pretty true.  It's just that a lot of his hopes for science, or private industry, etc., now seem equally wrongheaded.</p> <p>Still, let's not get into the details of the <a href="http://www.acceleratingfuture.com/steven/?p=62">technovolatile</a> [http://www.acceleratingfuture.com/steven/?p=62] viewpoint.  Eliezer<sub>2001</sub> has just tossed a major foundational assumption&#8212;that AI can't be dangerous, unlike other technologies&#8212;out the window.  You would intuitively suspect that this should have some kind of large effect on his strategy.</p> <p>Well, Eliezer<sub>2001</sub> did at least give up on his 1999 idea of an open-source AI Manhattan Project using self-modifying heuristic soup, but overall...</p> <p>Overall, he'd previously wanted to charge in, guns blazing, immediately using his best idea at the time; and afterward he still wanted to charge in, guns blazing.  He didn't say, "I don't know how to do this."  He didn't say, "I need better knowledge."  He didn't say, "This project is not yet ready to start coding."  It was still all, "The clock is ticking, gotta move now!  The Singularity Institute will start coding as soon as it's got enough money!"</p> <p>Before, he'd wanted to focus as much scientific effort as possible with full information-sharing, and afterward he still thought in those terms.  Scientific secrecy = bad guy, openness = good guy.  (Eliezer<sub>2001</sub> hadn't read up on the Manhattan Project and wasn't familiar with the similar argument that Leo Szilard had with Enrico Fermi.)</p> <p>That's the problem with converting one big "Oops!" into a gradient of shifting probability.  It means there isn't a single watershed moment&#8212;a visible huge impact&#8212;to hint that equally huge changes might be in order.</p> <p>Instead, there are all these little opinion shifts... that give you a chance to repair the <em>arguments</em> for your strategies; to shift the justification a little, but keep the "basic idea" in place.  Small shocks that the system can absorb without cracking, because each time, it gets a chance to go back and repair itself.  It's just that in the domain of rationality, cracking = good, repair = bad.  In the art of rationality it's far <a href="0059.html">more efficient</a> [http://lesswrong.com/lw/i9/the_importance_of_saying_oops/] to admit one huge mistake, than to admit lots of little mistakes.</p> <p>There's some kind of instinct humans have, I think, to preserve their former strategies and plans, so that they aren't constantly thrashing around and wasting resources; and of course an instinct to preserve any position that we have publicly argued for, so that we don't suffer the humiliation of being wrong.  And though the younger Eliezer has striven for rationality for many years, he is not immune to these impulses; they waft gentle influences on his thoughts, and this, unfortunately, is more than enough damage.</p> <p>Even in 2002, the earlier Eliezer isn't yet <em>sure</em> that Eliezer<sub>1997</sub>'s plan <em>couldn't possibly</em> have worked.  It <em>might</em> have gone right.  You never know, right?</p> <p>But there came a time when it all fell crashing down.  To be continued.</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age"><em>Yudkowsky's Coming of Age</em></a> [http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age]</p> <p style="text-align:right">Next post: "<a href="0491.html">My Naturalistic Awakening</a> [http://lesswrong.com/lw/u9/my_naturalistic_awakening/]"</p> <p style="text-align:right">Previous post: "<a href="0489.html">That Tiny Note of Discord</a> [http://lesswrong.com/lw/u7/that_tiny_note_of_discord/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq22.html">Sequence 22: Yudkowsky's Coming of Age</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0489.html">That Tiny Note of Discord</a></p></td><td><p><i>Next: </i><a href="0491.html">My Naturalistic Awakening</a></p></td></tr></table><p><i>Referenced by: </i><a href="0489.html">That Tiny Note of Discord</a> &#8226; <a href="0491.html">My Naturalistic Awakening</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/u8/fighting_a_rearguard_action_against_the_truth/">Fighting a Rearguard Action Against the Truth</a></p></body></html>