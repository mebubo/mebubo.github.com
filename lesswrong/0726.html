<html><head><title>The Craft and the Community</title></head><body><h1>The Craft and the Community</h1><p><i>Eliezer Yudkowsky, 26 April 2009 05:52PM</i></p><div><p>This sequence ran from <a href="0674.html">March</a> [http://lesswrong.com/lw/1e/raising_the_sanity_waterline/] to <a href="0722.html">April</a> [http://lesswrong.com/lw/c4/go_forth_and_create_the_art/] of 2009 and dealt with the topic of building rationalist communities that could systematically improve on the art, craft, and science of human rationality.  This is a highly forward-looking sequence - not so much an immediately complete recipe, as a list of action items and warnings for anyone setting out in the future to build a craft and a community.<a id="more"></a></p> <ul> <li><strong><a href="0674.html">Raising the Sanity Waterline</a> [http://lesswrong.com/lw/1e/raising_the_sanity_waterline/]</strong>:  Behind every particular failure of social rationality is a larger and more general failure of social rationality; even if all religious content were deleted tomorrow from all human minds, the larger failures that permit religion would still be present.  Religion may serve the function of an asphyxiated canary in a coal mine - getting rid of the canary doesn't get rid of the gas.  Even a complete social victory for atheism would only be the beginning of the real work of rationalists.  What could you teach people without ever <em>explicitly</em> mentioning religion, that would raise their <em>general epistemic waterline</em> to the point that religion went underwater?</li> <li><strong><a href="0675.html">A Sense That More Is Possible</a> [http://lesswrong.com/lw/2c/a_sense_that_more_is_possible/]</strong>: The art of human rationality may have not been much developed because its practitioners lack a sense that <em>vastly more is possible.</em>  The level of expertise that most rationalists strive to develop is not on a par with the skills of a professional mathematician - more like that of a strong casual amateur.  Self-proclaimed "rationalists" don't seem to get huge amounts of personal mileage out of their craft, and no one sees a problem with this.  Yet rationalists get less systematic training in a less systematic context than a first-dan black belt gets in hitting people.</li> <li><a href="0676.html"><strong>Epistemic Viciousness</strong></a> [http://lesswrong.com/lw/2i/epistemic_viciousness/]:  An essay by Gillian Russell on "<a href="http://www.artsci.wustl.edu/%7Egrussell/epistemicviciousness.pdf">Epistemic Viciousness in the Martial Arts</a> [http://www.artsci.wustl.edu/%7Egrussell/epistemicviciousness.pdf]" generalizes amazingly to possible and actual problems with building a community around rationality.  Most notably the extreme dangers associated with "data poverty" - the difficulty of testing the skills in the real world.  But also such factors as the sacredness of the dojo, the investment in teachings long-practiced, the difficulty of book learning that leads into the need to trust a teacher, deference to historical masters, and above all, living in data poverty while continuing to act as if the luxury of trust is possible.</li> <li><a href="0677.html"><strong>Schools Proliferating Without Evidence</strong></a> [http://lesswrong.com/lw/2j/schools_proliferating_without_evidence/]:  The branching schools of "psychotherapy", another domain in which experimental verification was weak (nonexistent, actually), show that an aspiring craft lives or dies by the degree to which it can be tested in the real world.  In the absence of that testing, one becomes prestigious by inventing yet another school and having students, rather than excelling at any visible performance criterion.  The field of hedonic psychology (happiness studies) began, to some extent, with the realization that you could <em>measure</em> happiness - that there was a family of measures that by golly did validate well against each other.  The act of creating a new measurement creates new science; if it's a <em>good </em>measurement, you get good science.  </li> <li><a href="0678.html"><strong>3 Levels of Rationality Verification</strong></a> [http://lesswrong.com/lw/2s/3_levels_of_rationality_verification/]:  How far the craft of rationality can be taken, depends largely on what methods can be invented for verifying it.  Tests seem usefully stratifiable into <em>reputational, experimental,</em> and <em>organizational</em>.  A "reputational" test is some real-world problem that tests the ability of a teacher or a school (like running a hedge fund, say) - "keeping it real", but without being able to break down exactly what was responsible for success.  An "experimental" test is one that can be run on each of a hundred students (such as a well-validated survey).  An "organizational" test is one that can be used to preserve the integrity of organizations by validating individuals or small groups, even in the face of strong incentives to game the test.  The strength of solution invented at each level will determine how far the craft of rationality can go in the real world.</li> <li><strong><a href="0682.html">Why Our Kind Can't Cooperate</a> [http://lesswrong.com/lw/3h/why_our_kind_cant_cooperate/]</strong>:  The atheist/libertarian/technophile/sf-fan/early-adopter/programmer/etc crowd, aka "the nonconformist cluster", seems to be stunningly bad at coordinating group projects.  There are a number of reasons for this, but one of them is that people are as <em>reluctant</em> to speak <em>agreement</em> out loud, as they are eager to voice disagreements - the exact opposite of the situation that obtains in more cohesive and powerful communities.  This is not rational either!  It is dangerous to be half a rationalist (in general), and this also applies to teaching only disagreement but not agreement, or only lonely defiance but not coordination.  The pseudo-rationalist taboo against expressing strong feelings probably doesn't help either.</li> <li><a href="0683.html"><strong>Tolerate Tolerance</strong></a> [http://lesswrong.com/lw/42/tolerate_tolerance/]:  One of the likely characteristics of someone who sets out to be a "rationalist" is a lower-than-usual tolerance for flawed thinking.  This makes it very important to <em>tolerate other people's tolerance</em> - to avoid rejecting them <em>because they tolerate people you wouldn't</em> - since otherwise we must all have exactly the same standards of tolerance in order to work together, which is unlikely.  Even if someone has a nice word to say about complete lunatics and crackpots - so long as they don't literally believe the same ideas themselves - try to be nice to them?  Intolerance of tolerance corresponds to <em>punishment of non-punishers,</em> a very dangerous game-theoretic idiom that can lock completely arbitrary systems in place even when they benefit no one at all.</li> <li><strong><a href="0684.html">You're Calling Who A Cult Leader?</a> [http://lesswrong.com/lw/4d/youre_calling_who_a_cult_leader/]</strong>:  Paul Graham gets exactly the same accusations about "cults" and "echo chambers" and "coteries" that I do, in exactly the same tone - e.g. comparing the long hours worked by Y Combinator startup founders to the sleep-deprivation tactic used in cults, or claiming that founders were asked to move to the Bay Area startup hub as a cult tactic of separation from friends and family.  This is bizarre, considering our relative surface risk factors.  It just seems to be a failure mode of the nonconformist community in general.  By far the most cultish-looking behavior on Hacker News is people trying to <em>show off how willing they are to disagree with Paul Graham,</em> which, I can personally testify, feels really bizarre when you're the target.  Admiring someone shouldn't be so scary - I don't hold back so much when praising e.g. Douglas Hofstadter; in this world there are people who have pulled off awesome feats and it is okay to admire them highly.</li> <li><strong><a href="0686.html">On Things That Are Awesome</a> [http://lesswrong.com/lw/4y/on_things_that_are_awesome/]</strong>:  Seven followup thoughts:  I can list more than <em>one</em> thing that is awesome; when I think of "Douglas Hofstadter" I am really thinking of his all-time greatest work; the greatest work is not the person; when we imagine other people we are imagining their output, so the real Douglas Hofstadter is the source of "Douglas Hofstadter"; I most strongly get the sensation of awesomeness when I see someone outdoing me overwhelmingly, at some task I've actually tried; we tend to admire unique detailed awesome things and overlook common nondetailed awesome things; religion and its bastard child "spirituality" tends to make us overlook <em>human</em> awesomeness.</li> <li><strong><a href="0689.html">Your Price For Joining</a> [http://lesswrong.com/lw/5j/your_price_for_joining/]</strong>:  The game-theoretical puzzle of the Ultimatum game has its reflection in a real-world dilemma:  How much do you demand that an existing group adjust toward you, before you will adjust toward it?  Our hunter-gatherer instincts will be tuned to groups of 40 with very minimal administrative demands and equal participation, meaning that we underestimate the inertia of larger and more specialized groups and demand too much before joining them.  In other groups this resistance can be overcome by affective death spirals and conformity, but rationalists think themselves too good for this - with the result that people in the nonconformist cluster often set their joining prices <em>way way way</em> too high, like an 50-way split with each player demanding 20% of the money.  Nonconformists need to move in the direction of joining groups more easily, even in the face of annoyances and apparent unresponsiveness.  If an issue isn't worth <em>personally fixing by however much effort it takes,</em> it's not worth a refusal to contribute.</li> <li><a href="0690.html"><strong>Can Humanism Match Religion's Output?</strong></a> [http://lesswrong.com/lw/5t/can_humanism_match_religions_output/]:  Anyone with a <em>simple </em>and <em>obvious</em> charitable project - responding with food and shelter to a tidal wave in Thailand, say - would be better off by <em>far</em> pleading with the Pope to mobilize the Catholics, rather than with Richard Dawkins to mobilize the atheists.  <em>For so long as this is true,</em> any increase in atheism at the expense of Catholicism will be something of a hollow victory, regardless of all other benefits.  Can no rationalist match the motivation that comes from the irrational fear of Hell?  Or does the real story have more to do with the motivating power of <em>physically meeting</em> others who share your cause, and group norms of participating?</li> <li><a href="0691.html"><strong>Church vs. Taskforce</strong></a> [http://lesswrong.com/lw/5v/church_vs_taskforce/]:  Churches serve a role of providing community - but they aren't explicitly optimized for this, because their nominal role is different.  If we desire community without church, can we go one better in the course of deleting religion?  There's a great deal of work to be done in the world; rationalist communities might potentially organize themselves around good causes, while explicitly optimizing for community.</li> <li><a href="0692.html"><strong>Rationality: Common Interest of Many Causes</strong></a> [http://lesswrong.com/lw/66/rationality_common_interest_of_many_causes/]:  Many causes benefit particularly from the spread of rationality - because it takes a little more rationality than usual to see their case, as a supporter, or even just a supportive bystander.  Not just the obvious causes like atheism, but things like marijuana legalization.  In the case of my own work this effect was strong enough that after years of bogging down I threw up my hands and explicitly recursed on creating rationalists.  If such causes can come to terms with <em>not individually capturing all the rationalists they create,</em> then they can mutually benefit from mutual effort on creating rationalists.  This cooperation may require learning to shut up about disagreements between such causes, and not fight over priorities, <em>except</em> in specialized venues clearly marked.</li> <li><a href="0693.html"><strong>Helpless Individuals</strong></a> [http://lesswrong.com/lw/64/helpless_individuals/]:  When you consider that our grouping instincts are optimized for 50-person hunter-gatherer bands where everyone knows everyone else, it begins to seem miraculous that modern-day large institutions survive at all.  And in fact, the vast majority of large modern-day institutions simply <em>fail to exist in the first place.</em>  This is why funding of Science is largely through money thrown at Science rather than donations from individuals - research isn't a good emotional fit for the rare problems that individuals can manage to coordinate on.  In fact very few things are, which is why e.g. 200 million adult Americans have such tremendous trouble supervising the 535 members of Congress.  Modern humanity manages to put forth very little in the way of coordinated individual effort to serve our collective individual interests.</li> <li><a href="0694.html"><strong>Money: The Unit of Caring</strong></a> [http://lesswrong.com/lw/65/money_the_unit_of_caring/]:  Omohundro's resource balance principle implies that the inside of any approximately rational system has a common currency of expected utilons.  In our world, this common currency is called "money" and it is the unit of how much society cares about something - a brutal yet obvious point.  Many people, seeing a good cause, would prefer to help it by donating a few volunteer hours.  But this avoids the tremendous gains of comparative advantage, professional specialization, and economies of scale - the reason we're not still in caves, the only way anything ever gets done in this world, the tools <em>grownups</em> use when anyone really <em>cares</em>.  Donating hours worked within a professional specialty and paying-customer priority, whether directly, or by donating the money earned to hire other professional specialists, is far more effective than volunteering unskilled hours.</li> <li><a href="0695.html"><strong>Purchase Fuzzies and Utilons Separately</strong></a> [http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/]:  Wealthy philanthropists typically make the mistake of trying to purchase warm fuzzy feelings, status among friends, and actual utilitarian gains, simultaneously; this results in vague pushes along all three dimensions and a mediocre final result.  It should be far more effective to spend some money/effort on buying altruistic fuzzies at maximum optimized efficiency (e.g. by helping people in person and seeing the results in person), buying status at maximum efficiency (e.g. by donating to something sexy that you can brag about, regardless of effectiveness), and spending <em>most </em>of your money on expected utilons (chosen through sheer cold-blooded shut-up-and-multiply calculation, without worrying about status or fuzzies).</li> <li><a href="0697.html"><strong>Selecting Rationalist Groups</strong></a> [http://lesswrong.com/lw/77/selecting_rationalist_groups/]:  Trying to breed e.g. egg-laying chickens by individual selection can produce odd side effects on the farm level, since a more dominant hen can produce more egg mass at the <em>expense of other hens.</em>  Group selection is nearly impossible in Nature, but easy to impose in the laboratory, and group-selecting hens produced substantial increases in efficiency.  Though most of my essays are about individual rationality - and indeed, Traditional Rationality also praises the lone heretic more than evil Authority - the real effectiveness of "rationalists" may end up determined by their performance in groups.</li> <li><a href="0699.html"><strong>Incremental Progress and the Valley</strong></a> [http://lesswrong.com/lw/7k/incremental_progress_and_the_valley/]:  The optimality theorems for probability theory and decision theory, are for <em>perfect</em> probability theory and decision theory.  There is no theorem that incremental changes toward the ideal, starting from a flawed initial form, must yield incremental progress at each step along the way.  Since perfection is unattainable, why dare to try for improvement?  But my <em>limited</em> experience with <em>specialized</em> applications suggests that given <em>enough</em> progress, one can achieve <em>huge </em>improvements over baseline - it just takes a lot of progress to get there.</li> <li><a href="0704.html"><strong>Whining-Based Communities</strong></a> [http://lesswrong.com/lw/8t/whiningbased_communities/]:  Many communities feed emotional needs by offering their members someone or something to blame for failure - say, those looters who don't approve of your excellence.  You can easily imagine some group of "rationalists" congratulating themselves on how <em>reasonable</em> they were, while blaming the surrounding <em>unreasonable</em> society for keeping them down.  But this is not how real rationality works - there's no assumption that other agents are rational.  We all face unfair tests (and yes, they are unfair to different degrees for different people); and how well you do with <em>your</em> unfair tests, is the test of <em>your</em> existence.  Rationality is there to <a href="0698.html">help you </a> [http://lesswrong.com/lw/7i/rationality_is_systematized_winning/]<em><a href="0698.html">win anyway</a> [http://lesswrong.com/lw/7i/rationality_is_systematized_winning/],</em> not to provide a self-handicapping excuse for losing.  There are <a href="0702.html">no first-person extenuating circumstances</a> [http://lesswrong.com/lw/92/extenuating_circumstances/].  There is <em>absolutely no point</em> in going down the road of mutual bitterness and consolation, about <em>anything, ever.</em> </li> <li><a href="0706.html"><strong>Mandatory Secret Identities</strong></a> [http://lesswrong.com/lw/9c/mandatory_secret_identities/]:  This post was not well-received, but the point was to suggest that a student must at some point leave the dojo and test their skills in the real world.  <em>The aspiration of an excellent student should not consist primarily of founding their own dojo and having their own students</em>.</li> <li><a href="0707.html"><strong>Beware of Other-Optimizing</strong></a> [http://lesswrong.com/lw/9v/beware_of_otheroptimizing/]:  Aspiring rationalists often vastly overestimate their own ability to optimize other people's lives.  They read nineteen webpages offering productivity advice that doesn't work for them... and then encounter the twentieth page, or invent a new method themselves, and <em>wow, it really works</em> - they've discovered the <em>true method.</em>  Actually, they've just discovered the one method in twenty that works for <em>them,</em> and their confident advice is no better than randomly selecting one of the twenty blog posts.  Other-Optimizing is exceptionally dangerous when you have <em>power over</em> the other person - for then you'll just believe that they <em>aren't trying hard enough.</em></li> <li><a href="0710.html"><strong>Akrasia and Shangri-La</strong></a> [http://lesswrong.com/lw/ab/akrasia_and_shangrila/]:  The <a href="0709.html">Shangri-La diet</a> [http://lesswrong.com/lw/a6/the_unfinished_mystery_of_the_shangrila_diet/] works amazingly well for some people, but completely fails for others, for no known reason.  Since the diet has a metabolic rationale and is not <em>supposed</em> to require willpower, its failure in my and other cases is unambigiously mysterious.  If it required a component of willpower, then I and others might be tempted to blame myself for not having willpower.  The art of combating akrasia (willpower failure) has the same sort of mysteries and is in the same primitive state; we don't know the deeper rule that explains why a trick works for one person but not another.</li> <li><a href="0713.html"><strong>Collective Apathy and the Internet</strong></a> [http://lesswrong.com/lw/9m/collective_apathy_and_the_internet/]:  The causes of <a href="0712.html">bystander apathy</a> [http://lesswrong.com/lw/9j/bystander_apathy/] are even worse on the Internet.  There may be an opportunity here for a startup to deliberately try to avert bystander apathy in online group coordination.</li> <li><a href="0714.html"><strong>Bayesians vs. Barbarians</strong></a> [http://lesswrong.com/lw/5f/bayesians_vs_barbarians/]:  Suppose that a country of rationalists is attacked by a country of <a href="0050.html">Evil</a> [http://lesswrong.com/lw/i0/are_your_enemies_innately_evil/] Barbarians who know nothing of probability theory or decision theory.  There's a certain concept of "rationality" which says that the rationalists inevitably lose, because the Barbarians believe in a heavenly afterlife if they die in battle, while the rationalists would all individually prefer to stay out of harm's way.  So the rationalist civilization is doomed; it is too elegant and civilized to fight the savage Barbarians...  And then there's the idea that rationalists should be able to (a) solve group coordination problems, (b) care a lot about other people and (c) win...</li> <li><a href="0715.html"><strong>Of Gender and Rationality</strong></a> [http://lesswrong.com/lw/ap/of_gender_and_rationality/]:  Analysis of the gender imbalance that appears in "rationalist" communities, suggesting nine possible causes of the effect, and possible corresponding solutions.</li> <li><a href="0716.html"><strong>My Way</strong></a> [http://lesswrong.com/lw/bd/my_way/]:  I sometimes think of myself as being like the protagonist in a classic SF labyrinth story, wandering further and further into some alien artifact, trying to radio back a description of what I'm seeing, so that I can be followed.  But what I'm finding is not just <em>the</em> Way, the thing that lies at the center of the labyrinth; it is also <em>my</em> Way, the path that <em>I</em> would take to come closer to the center, from whatever place I started out.  And yet there is still a common thing we are all trying to find.  We should be aware that others' shortest paths may not be the <em>same</em> as our own, but this is not the same as giving up the ability to judge <em>or</em> to share.</li> <li><a href="0719.html"><strong>The Sin of Underconfidence</strong></a> [http://lesswrong.com/lw/c3/the_sin_of_underconfidence/]:  When subjects know about a bias or are warned about a bias, <em>overcorrection</em> is not unheard of as an experimental result.  That's what makes a lot of cognitive subtasks so troublesome - you know you're biased but you're not sure how much, and if you keep tweaking you may overcorrect.  The danger of underconfidence (overcorrecting for overconfidence) is that you pass up opportunities on which you could have been successful; not challenging difficult enough problems; losing forward momentum and adopting defensive postures; refusing to put the <em>hypothesis </em>of your inability to the test; losing enough hope of triumph to try <em>hard</em> enough to win.  You should ask yourself "Does this way of thinking make me stronger, or weaker?"</li> <li><a href="0720.html"><strong>Well-Kept Gardens Die By Pacifism</strong></a> [http://lesswrong.com/lw/c1/wellkept_gardens_die_by_pacifism/]:  Good online communities die primarily by refusing to defend themselves, and so it has been since the days of Eternal September.  Anyone acculturated by academia knows that <em>censorship</em> is a very grave sin... in their walled gardens where it costs thousands and thousands of dollars to enter.  A community with internal politics will treat any attempt to impose moderation as a coup attempt (since internal politics seem of far greater import than invading barbarians).  In rationalist communities this is probably an instance of underconfidence - mildly competent moderators are probably quite trustworthy to wield the banhammer.  On Less Wrong, the community <em>is</em> the moderator (via karma) and you will need to trust <em>yourselves</em> enough to wield the power and keep the garden clear.</li> <li><a href="0724.html"><strong>Practical Advice Backed By Deep Theories</strong></a> [http://lesswrong.com/lw/d4/practical_advice_backed_by_deep_theories/]:  Practical advice is genuinely much, much more useful when it's backed up by concrete experimental results, causal models that are actually true, or valid math that is validly interpreted.  (Listed in increasing order of difficulty.)  Stripping out the theories and giving the mere advice alone wouldn't have nearly the same impact or even the same message; and oddly enough, translating experiments and math into practical advice seems to be a rare niche activity relative to academia.  If there's a distinctive LW style, this is it.</li> <li><a href="0725.html"><strong>Less Meta</strong></a> [http://lesswrong.com/lw/d3/less_meta/]:  The fact that this final series was on the craft and the community seems to have delivered a push in something of the wrong direction, (a) steering toward conversation about conversation and (b) making present accomplishment pale in the light of grander dreams.  Time to go back to practical advice and deep theories, then.</li> <li><a href="0722.html"><strong>Go Forth and Create the Art!</strong></a> [http://lesswrong.com/lw/c4/go_forth_and_create_the_art/]:  I've developed primarily the art of epistemic rationality, in particular, the arts required for advanced cognitive reductionism... arts like distinguishing fake explanations from real ones and avoiding affective death spirals.  There is much else that needs developing to create a craft of rationality - fighting akrasia; coordinating groups; teaching, training, verification, and becoming a proper experimental science; developing better introductory literature...  And yet it seems to me that there is a beginning barrier to surpass before you can <em>start</em> creating high-quality craft of rationality, having to do with virtually everyone who tries to think lofty thoughts going instantly astray, or indeed even realizing that a craft of rationality exists and that you ought to be studying cognitive science literature to create it.  It's my hope that my writings, as partial as they are, will serve to surpass this initial barrier.  The rest I leave to you.</li> </ul></div> <hr><p><i>Referenced by: </i><a href="0674.html">Raising the Sanity Waterline</a> &#8226; <a href="0675.html">A Sense That More Is Possible</a> &#8226; <a href="0676.html">Epistemic Viciousness</a> &#8226; <a href="0677.html">Schools Proliferating Without Evidence</a> &#8226; <a href="0678.html">3 Levels of Rationality Verification</a> &#8226; <a href="0682.html">Why Our Kind Can't Cooperate</a> &#8226; <a href="0683.html">Tolerate Tolerance</a> &#8226; <a href="0684.html">You're Calling *Who* A Cult Leader?</a> &#8226; <a href="0686.html">On Things that are Awesome</a> &#8226; <a href="0689.html">Your Price for Joining</a> &#8226; <a href="0690.html">Can Humanism Match Religion's Output?</a> &#8226; <a href="0691.html">Church vs. Taskforce</a> &#8226; <a href="0692.html">Rationality: Common Interest of Many Causes</a> &#8226; <a href="0693.html">Helpless Individuals</a> &#8226; <a href="0694.html">Money: The Unit of Caring</a> &#8226; <a href="0695.html">Purchase Fuzzies and Utilons Separately</a> &#8226; <a href="0697.html">Selecting Rationalist Groups</a> &#8226; <a href="0699.html">Incremental Progress and the Valley</a> &#8226; <a href="0704.html">Whining-Based Communities</a> &#8226; <a href="0706.html">Mandatory Secret Identities</a> &#8226; <a href="0707.html">Beware of Other-Optimizing</a> &#8226; <a href="0710.html">Akrasia and Shangri-La</a> &#8226; <a href="0713.html">Collective Apathy and the Internet</a> &#8226; <a href="0714.html">Bayesians vs. Barbarians</a> &#8226; <a href="0715.html">Of Gender and Rationality</a> &#8226; <a href="0716.html">My Way</a> &#8226; <a href="0719.html">The Sin of Underconfidence</a> &#8226; <a href="0720.html">Well-Kept Gardens Die By Pacifism</a> &#8226; <a href="0722.html">Go Forth and Create the Art!</a> &#8226; <a href="0724.html">Practical Advice Backed By Deep Theories</a> &#8226; <a href="0725.html">Less Meta</a> &#8226; <a href="0727.html">The End (of Sequences)</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/cz/the_craft_and_the_community/">The Craft and the Community</a></p></body></html>