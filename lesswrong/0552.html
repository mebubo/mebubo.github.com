<html><head><title>Failure By Affective Analogy</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Failure By Affective Analogy</h1><p><i>Eliezer Yudkowsky, 18 November 2008 07:14AM</i></p><div><p><strong>Previously in series</strong>:  <a href="0551.html">Failure By Analogy</a> [http://lesswrong.com/lw/vx/failure_by_analogy/]</p> <p>Alchemy is a way of thinking that humans do not <em>instinctively</em> spot as stupid.  Otherwise alchemy would never have been popular, even in medieval days.  Turning lead into gold by mixing it with things that seemed similar to gold, sounded <em>every bit as reasonable</em>, back in the day, as trying to build a flying machine with flapping wings.  (And yes, it was worth trying <em>once,</em> but you should <em>notice</em> if Reality keeps saying "So what?")</p> <p>And the final and most dangerous form of <a href="0551.html">failure by analogy</a> [http://lesswrong.com/lw/vx/failure_by_analogy/] is to say <em>a lot of nice things</em> about X, which is similar to Y, so we should expect nice things of Y. You may also say horrible things about Z, which is the polar opposite of Y, so if Z is bad, Y should be good.</p> <p>Call this "failure by affective analogy".</p> <p><a id="more"></a></p> <p>Failure by affective analogy is when you don't just say, "This lemon glazing is yellow, gold is yellow, QED."  But rather say:</p> <p>"And now we shall add delicious lemon glazing to the formula for the Philosopher's Stone the root of all wisdom, since lemon glazing is beautifully yellow, like gold is beautifully yellow, and also lemon glazing is delightful on the tongue, indicating that it is possessed of a superior potency that delights the senses, just as the beauty of gold delights the senses..."</p> <p>That's why you find people saying things like, "Neural networks are decentralized, <em>just like democracies</em>" or "Neural networks are emergent, <em>just like capitalism</em>".</p> <p>A summary of the Standard Prepackaged Revolutionary New AI Paradigm might look like the following - and when reading, ask yourself how many of these ideas are <em>affectively laden:</em></p> <ul> <li>The Dark Side is Top-Down.  The Light Side is Bottom-Up. </li> <li>The Dark Side is Centralized.  The Light Side is Distributed. </li> <li>The Dark Side is Logical.  The Light Side is Fuzzy. </li> <li>The Dark Side is Serial.  The Light Side is Parallel. </li> <li>The Dark Side is Rational.  The Light Side is Intuitive. </li> <li>The Dark Side is Deterministic.  The Light Side is Stochastic. </li> <li>The Dark Side tries to Prove things.  The Light Side tries to Test them. </li> <li>The Dark Side is Hierarchical.  The Light Side is Heterarchical. </li> <li>The Dark Side is Clean.  The Light Side is Noisy. </li> <li>The Dark Side operates in Closed Worlds.  The Light Side operates in Open Worlds. </li> <li>The Dark Side is Rigid.  The Light Side is Flexible. </li> <li>The Dark Side is Sequential.  The Light Side is Realtime. </li> <li>The Dark Side demands Control and Uniformity.  The Light Side champions Freedom and Individuality. </li> <li>The Dark Side is Lawful.  The Light Side, on the other hand, is <a href="0540.html"><em>Creative</em></a> [http://lesswrong.com/lw/vm/lawful_creativity/].</li> </ul> <p>By means of this tremendous package deal fallacy, lots of good feelings are generated about the New Idea (even if it's thirty years old).  Enough nice words may even manage to start an <a href="0180.html">affective death spiral</a> [http://lesswrong.com/lw/lm/affective_death_spirals/].  Until finally, via the standard channels of <a href="0174.html">affect heuristic</a> [http://lesswrong.com/lw/lg/the_affect_heuristic/] and <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/], it seems that the New Idea will <em>surely</em> be able to accomplish some extremely difficult end -</p> <p>- like, say, <span style="font-style: italic;">true</span><em> general intelligence</em> -</p> <p>- even if you can't quite give a walkthrough of the internal mechanisms which are going to produce that output.</p> <p>(Why yes, I <em>have</em> seen <a href="0494.html">AGIfolk</a> [http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/] trying to pull this on Friendly AI - as they explain how all we need to do is stamp the AI with the properties of Democracy and Love and Joy and Apple Pie and paint an American Flag on the case, and <em>surely</em> it will be Friendly as well - though they can't quite walk through internal cognitive mechanisms.)</p> <p>From such reasoning as this (and <a href="0461.html">this</a> [http://lesswrong.com/lw/tf/dreams_of_ai_design/]), came the string of false promises that were published in the newspapers (and led futurists who grew up during that era to <a href="0368.html">be very disappointed in AI, leading them to feel negative affect</a> [http://lesswrong.com/lw/qu/a_premature_word_on_ai/] that now causes them to put AI a hundred years in the future).</p> <p>Let's say it again:  <a href="0190.html">Reversed stupidity is not intelligence</a> [http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/] - if people are making bad predictions you should just discard them, not reason from their failure.</p> <p>But there is a certain lesson to be learned.  A bounded rationalist cannot do all things, but the true Way should not <em>overpromise</em> - it should not (systematically/regularly/on average) hold out the prospect of success, and then deliver failure.  Even a bounded rationalist can aspire to be <em>well calibrated,</em> to not assign 90% probability unless they really <em>do</em> have good enough information to be right nine times out of ten.  If you only have good enough information to be right 6 times out of 10, just say 60% instead.  A bounded rationalist cannot do all things, but the true Way does not overpromise.</p> <p>If you want to avoid failed promises of AI... then history suggests, I think, that you should not expect good things out of your AI system unless you have a good idea of how <em>specifically</em> it is going to happen.  I don't mean <a href="0534.html">writing out the exact internal program state in advance</a> [http://lesswrong.com/lw/vg/building_something_smarter/].  But I also don't mean saying that the refrigeration unit will <a href="0461.html">cool down the AI and make it more contemplative</a> [http://lesswrong.com/lw/tf/dreams_of_ai_design/].  For myself, I seek to know the laws governing the AI's <a href="0542.html">lawful uncertainty</a> [http://lesswrong.com/lw/vo/lawful_uncertainty/] and <a href="0540.html">lawful creativity</a> [http://lesswrong.com/lw/vm/lawful_creativity/] - though I don't expect to know the full content of its future knowledge, or the exact design of its future inventions.</p> <p>Don't want to be disappointed?  Don't hope!</p> <p>Don't ask yourself if you're <em>allowed to believe</em> that your AI design will work.</p> <p>Don't guess.  Know.</p> <p>For this much I do know - if I don't <em>know</em> that my AI design will work, it won't.</p> <p>There are various obvious caveats that need to be attached here, and various obvious stupid interpretations of this principle not to make.  You can't be sure a search will return successfully before you have run it -</p> <p>- but you should understand on a gut level:  If you are <em>hoping</em> that your AI design will work, it will fail.  If you <em>know</em> that your AI design will work, then it <em>might</em> work.</p> <p>And on the Friendliness part of that you should hold yourself to an even higher standard - ask yourself if you are <em>forced</em> to believe the AI will be Friendly - because in that aspect, above all, you must constrain Reality so tightly that even Reality is not allowed to say, "So what?"  This is a very tough test, but if you do not apply it, you will just find yourself trying to paint a flag on the case, and hoping.</p></div> <hr><p><i>Referenced by: </i><a href="0574.html">Artificial Mysterious Intelligence</a> &#8226; <a href="0616.html">Getting Nearer</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/vy/failure_by_affective_analogy/">Failure By Affective Analogy</a></p></body></html>