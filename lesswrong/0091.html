<html><head><title>Availability</title></head><body><h1>Availability</h1><p><i>Eliezer Yudkowsky, 06 September 2007 06:55AM</i></p><div><p>The <em>availability heuristic</em> is judging the frequency or probability of an event, by the ease with which examples of the event come to mind.</p> <p>A famous 1978 study by Lichtenstein, Slovic, Fischhoff, Layman, and Combs, "Judged Frequency of Lethal Events", studied errors in quantifying the severity of risks, or judging which of two dangers occurred more frequently.  Subjects thought that accidents caused about as many deaths as disease; thought that homicide was a more frequent cause of death than suicide.  Actually, diseases cause about 16 times as many deaths as accidents, and suicide is twice as frequent as homicide.</p> <p>An obvious hypothesis to account for these skewed beliefs is that murders are more likely to be talked about than suicides - thus, someone is more likely to recall hearing about a murder than hearing about a suicide.  Accidents are more dramatic than diseases - perhaps this makes people more likely to remember, or more likely to recall, an accident.  In 1979, a followup study by Combs and Slovic showed that the skewed probability judgments correlated strongly (.85 and .89) with skewed reporting frequencies in two newspapers.  This doesn't disentangle whether murders are more available to memory because they are more reported-on, or whether newspapers report more on murders because murders are more vivid (hence also more remembered).  But either way, an availability bias is at work.</p><a id="more"></a><p>Selective reporting is one major source of availability biases.  In the ancestral environment, much of what you knew, you experienced yourself; or you heard it directly from a fellow tribe-member who had seen it.  There was usually at most one layer of selective reporting between you, and the event itself.  With today's Internet, you may see reports that have passed through the hands of six bloggers on the way to you - six successive filters.  Compared to our ancestors, we live in a larger world, in which far more happens, and far less of it reaches us - a much stronger selection effect, which can create much larger availability biases.</p> <p>In real life, you're unlikely to ever meet Bill Gates.  But thanks to selective reporting by the media, you may be tempted to compare your life success to his - and suffer hedonic penalties accordingly.  The objective frequency of Bill Gates is 0.00000000015, but you hear about him much more often.  Conversely, 19% of the planet lives on less than $1/day, and I doubt that one fifth of the blog posts you read are written by them.</p> <p>Using availability seems to give rise to an <a href="0090.html">absurdity bias</a> [http://lesswrong.com/lw/j4/absurdity_heuristic_absurdity_bias/]; events that have never happened, are not recalled, and hence deemed to have probability zero.  When no flooding has recently occurred (and yet the probabilities are still fairly calculable), people refuse to buy flood insurance even when it is heavily subsidized and priced far below an actuarially fair value.  Kunreuther et. al. (1993) suggests underreaction to threats of flooding may arise from "the inability of individuals to conceptualize floods that have never occurred... Men on flood plains appear to be very much prisoners of their experience... Recently experienced floods appear to set an upward bound to the size of loss with which managers believe they ought to be concerned."</p> <p>Burton et. al. (1978) report that when dams and levees are built, they reduce the frequency of floods, and thus apparently create a false sense of security, leading to reduced precautions. While building dams decreases the <em>frequency</em> of floods, damage <em>per flood</em> is afterward so much greater that average yearly damage <em>increases.<br><br></em>The wise would extrapolate from a memory of small hazards to the possibility of large hazards.  Instead, past experience of small hazards seems to set a perceived upper bound on risk.  A society well-protected against minor hazards takes no action against major risks, building on flood plains once the regular minor floods are eliminated.  A society subject to regular minor hazards treats those minor hazards as an upper bound on the size of the risks, guarding against regular minor floods but not occasional major floods.</p> <p>Memory is not always a good guide to probabilities in the past, let alone the future.</p> <hr> <p>Burton, I., Kates, R. and White, G. 1978. <em>Environment as Hazard.</em> New York: Oxford University Press. </p> <p>Combs, B. and Slovic, P. 1979. Causes of death: Biased newspaper coverage and biased judgments. <em>Journalism Quarterly,</em> <strong>56</strong>: 837-843. </p> <p>Kunreuther, H., Hogarth, R. and Meszaros, J. 1993. Insurer ambiguity and market failure. <em>Journal of Risk and Uncertainty,</em> <strong>7</strong>: 71-87. </p> <p>Lichtenstein, S., Slovic, P., Fischhoff, B., Layman, M. and Combs, B. 1978. Judged Frequency of Lethal Events. <em>Journal of Experimental Psychology: Human Learning and Memory</em>, <strong>4</strong>(6), November: 551-78.</p></div> <hr><p><i>Referenced by: </i><a href="0092.html">Why is the Future So Absurd?</a> &#8226; <a href="0100.html">Doublethink (Choosing to be Biased)</a> &#8226; <a href="0102.html">Planning Fallacy</a> &#8226; <a href="0119.html">We Change Our Minds Less Often Than We Think</a> &#8226; <a href="0131.html">The Logical Fallacy of Generalization from Fictional Evidence</a> &#8226; <a href="0208.html">My Strange Beliefs</a> &#8226; <a href="0368.html">A Premature Word on AI</a> &#8226; <a href="0446.html">Abstracted Idealized Dynamics</a> &#8226; <a href="0599.html">Dunbar's Function</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/j5/availability/">Availability</a></p></body></html>