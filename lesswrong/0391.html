<html><head><title>Heading Toward Morality</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Heading Toward Morality</h1><p><i>Eliezer Yudkowsky, 20 June 2008 08:08AM</i></p><div><p><strong>Followup to</strong>:  <a href="0389.html">Ghosts in the Machine</a> [http://lesswrong.com/lw/rf/ghosts_in_the_machine/], <a href="0183.html">Fake Fake Utility Functions</a> [http://lesswrong.com/lw/lp/fake_fake_utility_functions/], <a href="0184.html">Fake Utility Functions</a> [http://lesswrong.com/lw/lq/fake_utility_functions/]</p> <p>As people were complaining before about not seeing where the <a href="0379.html">quantum physics sequence</a> [http://lesswrong.com/lw/r5/the_quantum_physics_sequence/] was going, I shall go ahead and tell you where I'm heading now.<br> <br> Having <a href="0281.html">dissolved</a> [http://lesswrong.com/lw/of/dissolving_the_question/] the confusion surrounding the word "<a href="0385.html">could</a> [http://lesswrong.com/lw/rb/possibility_and_couldness/]", the trajectory is now heading toward <em>should.</em></p> <p>In fact, I've been heading there for a while.  Remember the <a href="0183.html">whole sequence</a> [http://lesswrong.com/lw/lp/fake_fake_utility_functions/] on <a href="0184.html">fake utility functions</a> [http://lesswrong.com/lw/lq/fake_utility_functions/]?  Back in... well... November 2007?</p> <p><a id="more"></a></p> <p>I sometimes think of there being a train that goes to the Friendly AI station; but it makes several stops before it gets there; and at each stop, a large fraction of the remaining passengers get off.<br> <br> One of those stops is the one I spent a month leading up to in November 2007, the sequence chronicled in <a href="0183.html">Fake Fake Utility Functions</a> [http://lesswrong.com/lw/lp/fake_fake_utility_functions/] and concluded in <a href="0184.html">Fake Utility Functions</a> [http://lesswrong.com/lw/lq/fake_utility_functions/].</p> <p>That's the stop where someone thinks of the One Great Moral Principle That Is All We Need To Give AIs.</p> <p>To deliver that one warning, I had to go through all sorts of topics&#8212;which topics one might find useful even if not working on Friendly AI.  I warned against <a href="0180.html">Affective</a> [http://lesswrong.com/lw/lm/affective_death_spirals/] <a href="0181.html">Death</a> [http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/] <a href="0182.html">Spirals</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/], which required recursing on the <a href="0174.html">affect heuristic</a> [http://lesswrong.com/lw/lg/the_affect_heuristic/] and <a href="0177.html">halo</a> [http://lesswrong.com/lw/lj/the_halo_effect/] <a href="0178.html">effect</a> [http://lesswrong.com/lw/lk/superhero_bias/], so that your good feeling about one particular moral principle wouldn't spiral out of control.  I did <a href="0149.html">that</a> [http://lesswrong.com/lw/kr/an_alien_god/] <a href="0150.html">whole</a> [http://lesswrong.com/lw/ks/the_wonder_of_evolution/] <a href="0151.html">sequence</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/] on <a href="0154.html">evolution</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]; and discursed on the human ability to make <a href="0157.html">almost any goal appear to support almost any policy</a> [http://lesswrong.com/lw/kz/fake_optimization_criteria/]; I went into <a href="0159.html">evolutionary psychology</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/] to argue for why we shouldn't expect human <a href="0162.html">terminal values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/] to reduce to <a href="0161.html">any simple principle</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/], even <a href="0169.html">happiness</a> [http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/], explaining the concept of "<a href="0162.html">expected utility</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/]" along the way...</p> <p>...and talked about <a href="0171.html">genies</a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/] and more; but you can read <a href="0183.html">the Fake Utility sequence</a> [http://lesswrong.com/lw/lp/fake_fake_utility_functions/] for that.</p> <p>So that's <em>just</em> the warning against trying to <a href="0184.html">oversimplify human morality</a> [http://lesswrong.com/lw/lq/fake_utility_functions/] into One Great Moral Principle.</p> <p>If you want to actually <a href="0281.html">dissolve the confusion</a> [http://lesswrong.com/lw/of/dissolving_the_question/] that surrounds the word "should"&#8212;which is the next stop on the train&#8212;then that takes a much longer introduction.  Not just one November.</p> <p>I went through the <a href="0279.html">sequence on words and definitions</a> [http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/] so that I would be able to later say things like "The next project is to <a href="0260.html">Taboo</a> [http://lesswrong.com/lw/nu/taboo_your_words/] the word 'should' and <a href="0261.html">replace it with its substance</a> [http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/]", or "Sorry, saying that morality is self-interest '<a href="0265.html">by definition</a> [http://lesswrong.com/lw/nz/arguing_by_definition/]' isn't going to cut it here".</p> <p>And also the words-and-definitions sequence was the simplest example I knew to introduce the notion of <a href="0254.html">How An Algorithm Feels From Inside</a> [http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/], which is one of the great master keys to <a href="0281.html">dissolving</a> [http://lesswrong.com/lw/of/dissolving_the_question/] <a href="0282.html">wrong questions</a> [http://lesswrong.com/lw/og/wrong_questions/].  Though it seems to us that <a href="0284.html">our cognitive representations are the very substance of the world</a> [http://lesswrong.com/lw/oi/mind_projection_fallacy/], they have a character that comes from cognition and often cuts crosswise to a universe made of quarks.  E.g. <a href="0285.html">probability</a> [http://lesswrong.com/lw/oj/probability_is_in_the_mind/]; if we are uncertain of a phenomenon, that is a fact about our state of mind, not an intrinsic character of the phenomenon.</p> <p>Then the reductionism sequence: that a <a href="0289.html">universe made only of quarks</a> [http://lesswrong.com/lw/on/reductionism/], does not mean that things of value are <a href="0290.html">lost</a> [http://lesswrong.com/lw/oo/explaining_vs_explaining_away/] or even <a href="0293.html">degraded to mundanity</a> [http://lesswrong.com/lw/or/joy_in_the_merely_real/].  And the notion of how <a href="0306.html">the sum can seem unlike the parts</a> [http://lesswrong.com/lw/p4/heat_vs_motion/], and yet <a href="0304.html">be as much the parts as our hands are fingers</a> [http://lesswrong.com/lw/p2/hand_vs_fingers/].</p> <p>Followed by a new example, one step up in difficulty from words and their <a href="0256.html">seemingly intrinsic meanings</a> [http://lesswrong.com/lw/nq/feel_the_meaning/]:  "Free will" and <a href="0385.html">seemingly intrinsic could-ness</a> [http://lesswrong.com/lw/rb/possibility_and_couldness/].</p> <p>But before that point, it was useful to introduce <a href="0379.html">quantum physics</a> [http://lesswrong.com/lw/r5/the_quantum_physics_sequence/].  Not just to get to <a href="0363.html">timeless physics</a> [http://lesswrong.com/lw/qp/timeless_physics/] and dissolve the "<a href="0375.html">determinism</a> [http://lesswrong.com/lw/r1/timeless_control/]" part of the "free will" confusion.  But also, more fundamentally, to <a href="0372.html">break belief in an intuitive universe</a> [http://lesswrong.com/lw/qy/why_quantum/] that looks just like our brain's cognitive representations.  And present examples of the dissolution of even such fundamental intuitions as those concerning <a href="0371.html">personal identity</a> [http://lesswrong.com/lw/qx/timeless_identity/].  And to illustrate the idea that you are <a href="0374.html">within physics</a> [http://lesswrong.com/lw/r0/thou_art_physics/], <a href="0384.html">within causality</a> [http://lesswrong.com/lw/ra/causality_and_moral_responsibility/], and that strange things will go wrong in your mind if ever you forget it.</p> <p>Lately we have begun to approach the final precautions, with warnings against such notions as <a href="0386.html">Author* control</a> [http://lesswrong.com/lw/rc/the_ultimate_source/]: every mind which computes a morality must do so within a chain of lawful causality, it cannot arise from <a href="0389.html">the free will of a ghost in the machine</a> [http://lesswrong.com/lw/rf/ghosts_in_the_machine/].</p> <p>And the warning against <a href="0387.html">Passing the Recursive Buck</a> [http://lesswrong.com/lw/rd/passing_the_recursive_buck/] to some meta-morality that is not itself computably specified, or some meta-morality that is chosen by a ghost without it being programmed in, or to a notion of "moral truth" just as confusing as "should" itself...</p> <p>And the warning on the difficulty of <a href="0388.html">grasping slippery things</a> [http://lesswrong.com/lw/re/grasping_slippery_things/] like "should"&#8212;demonstrating how very easy it will be to just invent another black box equivalent to should-ness, to sweep should-ness under a slightly different rug&#8212;or to bounce off into mere modal logics of primitive should-ness...</p> <p>We aren't yet at the point where I can explain morality.</p> <p>But I think&#8212;though I could be mistaken&#8212;that we are finally getting close to the final sequence.</p> <p>And if you don't care about my goal of explanatorily transforming Friendly AI from a Confusing Problem into a merely Extremely Difficult Problem, then stick around anyway.  I tend to go through interesting intermediates along my way.</p> <p>It might seem like confronting "the nature of morality" from the perspective of Friendly AI is only asking for additional trouble.</p> <p>Artificial Intelligence melts people's brains.  Metamorality melts people's brains.  Trying to think about AI and metamorality at the same time can cause people's brains to spontaneously combust and burn for years, emitting toxic smoke&#8212;don't laugh, I've seen it happen multiple times.</p> <p>But the discipline imposed by Artificial Intelligence is this: you cannot escape into things that are "self-evident" or "obvious".  That doesn't stop people from trying, but the programs don't work.  Every thought has to be computed somehow, by transistors made of mere quarks, and not by moral self-evidence to some ghost in the machine.</p> <p>If what you care about is rescuing children from burning orphanages, I don't think you will find many moral surprises here; my metamorality adds up to moral normality, <a href="http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/">as it should</a> [http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/].  You do not need to worry about metamorality when you are <em>personally</em> trying to rescue children from a burning orphanage.  The point at which metamoral issues <em>per se</em> have high stakes in the real world, is when you try to compute morality in an AI standing in front of a burning orphanage.</p> <p>Yet there is also a good deal of needless despair and misguided fear of science, stemming from notions such as, "Science tells us the universe is empty of morality".  This is damage done by a confused metamorality that fails to add up to moral normality.  For that I hope to write down a counterspell of understanding.  Existential depression has always annoyed me; it is one of the world's most pointless forms of suffering.</p> <p>Don't expect the final post on this topic to come tomorrow, but at least you know where we're heading.</p> <p> </p> <p style="text-align:right">Part of <a href="http://wiki.lesswrong.com/wiki/Metaethics_sequence"><em>The Metaethics Sequence</em></a> [http://wiki.lesswrong.com/wiki/Metaethics_sequence]</p> <p style="text-align:right">Next post: "<a href="0397.html">No Universally Compelling Arguments</a> [http://lesswrong.com/lw/rn/no_universally_compelling_arguments/]"</p> <p style="text-align:right">(start of sequence)</p></div> <hr><table><tr><th colspan="2"><a href="seq14.html">Sequence 14: Metaethics</a>:</th></tr><tr><td><p><i>Previous: </i><a href="seq13.html">Sequence 13: Quantum Physics</a></p></td><td><p><i>Next: </i><a href="0397.html">No Universally Compelling Arguments</a></p></td></tr></table><p><i>Referenced by: </i><a href="0397.html">No Universally Compelling Arguments</a> &#8226; <a href="0431.html">Setting Up Metaethics</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/rh/heading_toward_morality/">Heading Toward Morality</a></p></body></html>