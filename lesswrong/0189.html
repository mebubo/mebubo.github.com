<html><head><title>Every Cause Wants To Be A Cult</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Every Cause Wants To Be A Cult</h1><p><i>Eliezer Yudkowsky, 12 December 2007 03:04AM</i></p><div><p><strong>Followup to</strong>:  <a href="0049.html">Correspondence Bias</a> [http://lesswrong.com/lw/hz/correspondence_bias/], <a href="0180.html">Affective Death Spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/], <a href="0187.html">The Robbers Cave Experiment</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/]</p> <p>Cade Metz at <em>The Register</em> <a href="http://www.theregister.co.uk/2007/12/04/wikipedia_secret_mailing/">recently</a> [http://www.theregister.co.uk/2007/12/04/wikipedia_secret_mailing/] <a href="http://www.theregister.co.uk/2007/12/06/wikipedia_and_overstock/">alleged</a> [http://www.theregister.co.uk/2007/12/06/wikipedia_and_overstock/] that a secret mailing list of Wikipedia's top administrators has become obsessed with banning all critics and possible critics of Wikipedia.  Including banning a productive user when one administrator&#8212;solely <em>because</em> of the productivity&#8212;became convinced that the user was a spy sent by <em>Wikipedia Review.</em>  And that the top people at Wikipedia closed ranks to defend their own.  (I have not investigated these allegations myself, as yet.  Hat tip to <a href="http://postbiota.org/pipermail/tt/2007-December/001947.html">Eugen Leitl</a> [http://postbiota.org/pipermail/tt/2007-December/001947.html].)</p> <p>Is there some deep moral flaw in seeking to systematize the world's knowledge, which would lead pursuers of that Cause into madness?  Perhaps only people with innately totalitarian tendencies would try to become the world's authority on everything&#8212;</p> <p><a href="0049.html">Correspondence bias</a> [http://lesswrong.com/lw/hz/correspondence_bias/] alert!  (Correspondence bias: making inferences about someone's unique disposition from behavior that can be entirely explained by the situation in which it occurs.  When we see someone else kick a vending machine, we think they are "an angry person", but when we kick the vending machine, it's because the bus was late, the train was early and the machine ate our money.)  If the allegations about Wikipedia are true, they're explained by <em>ordinary</em> human nature, not by <em>extraordinary</em> human nature.</p> <p>The <a href="0187.html">ingroup-outgroup dichotomy</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/] is part of ordinary human nature.  So are <a href="0180.html">happy death spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/] and <a href="0186.html">spirals of hate</a> [http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/].  A Noble Cause doesn't need a deep hidden flaw for its adherents to form a cultish in-group.  It is sufficient that the adherents be human.  Everything else follows naturally, decay by default, like food spoiling in a refrigerator after the electricity goes off.</p> <p><a id="more"></a></p> <p>In the same sense that every thermal differential wants to equalize itself, and every computer program wants to become a collection of ad-hoc patches, every Cause <em>wants</em> to be a cult.  It's a high-entropy state into which the system trends, an attractor in human psychology.  It may have nothing to do with whether the Cause is truly Noble.  You might think that a Good Cause would rub off its goodness on every aspect of the people associated with it&#8212;that the Cause's followers would also be less susceptible to status games, ingroup-outgroup bias, affective spirals, leader-gods.  But believing one true idea won't switch off the <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/].  A noble cause won't make its adherents something other than human.  There are plenty of bad ideas that can do plenty of damage&#8212;but that's not necessarily what's going on.</p> <p>Every group of people with an unusual goal&#8212;good, bad, or silly&#8212;will trend toward the cult attractor unless they make a constant effort to resist it.  You can keep your house cooler than the outdoors, but you have to run the air conditioner constantly, and as soon as you turn off the electricity&#8212;give up the fight against entropy&#8212;things will go back to "normal".</p> <p>On one notable occasion there was a group that went semicultish whose rallying cry was "Rationality!  Reason!  Objective reality!"  (More on this in future posts.)  Labeling the Great Idea "rationality" won't protect you any more than putting up a sign over your house that says "Cold!"  You still have to run the air conditioner&#8212;expend the required energy per unit time to reverse the natural slide into cultishness.  Worshipping rationality won't make you sane any more than worshipping gravity enables you to fly.  You can't talk to thermodynamics and you can't pray to probability theory.  You can <em>use</em> it, but not join it as an in-group.</p> <p>Cultishness is quantitative, not qualitative.  The question is not "Cultish, yes or no?" but "How much cultishness and where?"  Even in Science, which is the archetypal Genuinely Truly Noble Cause, we can readily point to the current frontiers of the war against cult-entropy, where the current battle line creeps forward and back.  Are journals more likely to accept articles with a well-known authorial byline, or from an unknown author from a well-known institution, compared to an unknown author from an unknown institution?  How much belief is due to authority and how much is from the experiment?  Which journals are using blinded reviewers, and how effective is blinded reviewing?</p> <p>I cite this example, rather than the <a href="0130.html">standard</a> [http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/] vague accusations of "Scientists aren't open to new ideas", because it shows a <em>battle line</em>&#8212;a place where human psychology is being actively driven back, where accumulated cult-entropy is being pumped out.  (Of course this requires emitting some waste heat.)</p> <p>This post is not a catalog of techniques for actively pumping against cultishness.  <a href="0181.html">Some</a> [http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/] <a href="0121.html">such</a> [http://lesswrong.com/lw/jz/the_meditation_on_curiosity/] techniques I have said before, and some I will say later.  <em>Today</em> I just want to point out that the worthiness of the Cause does not mean you can spend any <em>less</em> effort in resisting the cult attractor.  And that if you can point to current battle lines, it does not mean you confess your Noble Cause unworthy.  You might think that if the question were "Cultish, yes or no?" that you were obliged to answer "No", or else betray your beloved Cause.  But that is like thinking that you should divide engines into "perfectly efficient" and "inefficient", instead of measuring waste.</p> <p>Contrariwise, if you believe that it was the Inherent Impurity of those Foolish Other Causes that made them go wrong, if you laugh at the folly of "cult victims", if you think that cults are led and populated by mutants, then you will not expend the necessary effort to pump against entropy&#8212;to resist being human.</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor"><em>Death Spirals and the Cult Attractor</em></a> [http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0193.html">Guardians of the Truth</a> [http://lesswrong.com/lw/lz/guardians_of_the_truth/]"</p> <p style="text-align:right">Previous post: "<a href="0186.html">When None Dare Urge Restraint</a> [http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq04.html">Sequence 04: Death Spirals and the Cult Attractor</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0187.html">The Robbers Cave Experiment</a></p></td><td><p><i>Next: </i><a href="0193.html">Guardians of the Truth</a></p></td></tr></table><p><i>Referenced by: </i><a href="0186.html">When None Dare Urge Restraint</a> &#8226; <a href="0190.html">Reversed Stupidity Is Not Intelligence</a> &#8226; <a href="0193.html">Guardians of the Truth</a> &#8226; <a href="0195.html">Guardians of Ayn Rand</a> &#8226; <a href="0198.html">Two Cult Koans</a> &#8226; <a href="0207.html">Cultish Countercultishness</a> &#8226; <a href="0241.html">Something to Protect</a> &#8226; <a href="0384.html">Causality and Moral Responsibility</a> &#8226; <a href="0430.html">Changing Your Metaethics</a> &#8226; <a href="0639.html">The Thing That I Protect</a> &#8226; <a href="0640.html">...And Say No More Of It</a> &#8226; <a href="0684.html">You're Calling *Who* A Cult Leader?</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/">Every Cause Wants To Be A Cult</a></p></body></html>