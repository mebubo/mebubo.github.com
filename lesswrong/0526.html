<html><head><title>Belief in Intelligence</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Belief in Intelligence</h1><p><i>Eliezer Yudkowsky, 25 October 2008 03:00PM</i></p><div><p><strong>Previously in series</strong>:  <a href="0525.html">Expected Creative Surprises</a> [http://lesswrong.com/lw/v7/expected_creative_surprises/] </p> <p>Since I am so uncertain of Kasparov's moves, what is the empirical content of my belief that "Kasparov is a highly intelligent chess player"?  <a href="0053.html">What real-world experience does my belief tell me to anticipate?</a> [http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/]  Is it a cleverly masked form of total ignorance? </p> <p>To sharpen the dilemma, suppose Kasparov plays against some mere chess grandmaster Mr. G, who's not in the running for world champion.  My own ability is far too low to distinguish between these levels of chess skill.  When I try to guess Kasparov's move, or Mr. G's next move, all I can do is try to guess "the best chess move" using my own meager knowledge of chess.  Then I would produce exactly the same prediction for Kasparov's move or Mr. G's move in any particular chess position.  So what is the empirical content of my belief that "Kasparov is a <em>better</em> chess player than Mr. G"?</p><a id="more"></a><p>The empirical content of my belief is the testable, falsifiable prediction that the <em>final</em> chess position will occupy the class of chess positions that are wins for Kasparov, rather than drawn games or wins for Mr. G.  (Counting resignation as a legal move that leads to a chess position classified as a loss.)  The degree to which I think Kasparov is a "better player" is reflected in the amount of probability mass I concentrate into the "Kasparov wins" class of outcomes, versus the "drawn game" and "Mr. G wins" class of outcomes.  These classes are extremely vague in the sense that they refer to vast spaces of possible chess positions - but "Kasparov wins" <em>is</em> more specific than maximum entropy, because it can be <a href="0065.html">definitely falsified</a> [http://lesswrong.com/lw/if/your_strength_as_a_rationalist/] by a vast set of chess positions. </p> <p>The <em>outcome</em> of Kasparov's game is predictable because I know, and understand, Kasparov's goals.  Within the confines of the chess board, I know Kasparov's motivations - I know his success criterion, his utility function, his target as an optimization process.  I know where Kasparov is <em>ultimately</em> trying to steer the future and I anticipate he is powerful enough to get there, although I don't anticipate much about <em>how</em> Kasparov is going to do it. </p> <p>Imagine that I'm visiting a distant city, and a local friend volunteers to drive me to the airport.  I don't know the neighborhood. Each time my friend approaches a street intersection, I don't know whether my friend will turn left, turn right, or continue straight ahead.  I can't predict my friend's move even as we approach each individual intersection - let alone, predict the whole sequence of moves in advance. </p> <p>Yet I can predict the <em>result</em> of my friend's unpredictable actions: we will arrive at the airport.  Even if my friend's house were located elsewhere in the city, so that my friend made a completely different sequence of turns, I would just as confidently predict our arrival at the airport.  I can predict this long in advance, before I even get into the car.  My flight departs soon, and there's no time to waste; I wouldn't get into the car in the first place, if I couldn't confidently predict that the car would travel to the airport along an unpredictable pathway. </p> <p>Isn't this a remarkable situation to be in, from a scientific perspective?  I can predict the <em>outcome</em> of a process, without being able to predict any of the <em>intermediate steps</em> of the process.</p> <p>How is this even possible?  Ordinarily one predicts by imagining the present and then running the visualization forward in time.  If you want a <em>precise</em> model of the Solar System, one that takes into account planetary perturbations, you must start with a model of all major objects and run that model forward in time, step by step.</p> <p>Sometimes simpler problems have a closed-form solution, where calculating the future at time T takes the same amount of work regardless of T.  A coin rests on a table, and after each minute, the coin turns over.  The coin starts out showing heads.  What face will it show a hundred minutes later?  Obviously you did not answer this question by visualizing a hundred intervening steps.  You used a closed-form solution that worked to predict the outcome, and would <em>also</em> work to predict any of the intervening steps.</p> <p>But when my friend drives me to the airport, I can predict the outcome successfully using a strange model that won't work to predict <em>any</em> of the intermediate steps.  My model doesn't even require me to input the initial conditions - I don't need to know where we start out in the city!</p> <p>I do need to know something about my friend.  I must know that my friend wants me to make my flight.  I must credit that my friend is a good enough planner to successfully drive me to the airport (if he wants to).  These are properties of my <em>friend's</em> initial state - properties which let me predict the final destination, though not any intermediate turns.</p> <p>I must also credit that my friend knows enough about the city to drive successfully.  This may be regarded as a relation between my friend and the city; hence, <a href="0271.html">a property of both</a> [http://lesswrong.com/lw/o5/the_second_law_of_thermodynamics_and_engines_of/].  But an extremely <em>abstract</em> property, which does not require any <em>specific</em> knowledge about either the city, or about my friend's knowledge about the city.</p> <p>This is one way of viewing the subject matter to which I've devoted my life - these <em>remarkable situations</em> which place us in such an odd epistemic positions.  And my work, in a sense, can be viewed as unraveling the exact form of that strange abstract knowledge we can possess; whereby, not knowing the actions, we can justifiably know the consequence.</p> <p>"Intelligence" is too narrow a term to describe these remarkable situations in full generality.  I would say rather "optimization process".  A similar situation accompanies the study of biological natural selection, for example; we can't predict the exact form of the next organism observed.</p> <p>But my own specialty is the kind of optimization process called "intelligence"; and even narrower, a particular kind of intelligence called "Friendly Artificial Intelligence" - of which, I hope, I will be able to obtain especially precise abstract knowledge.</p></div> <hr><p><i>Referenced by: </i><a href="0527.html">Aiming at the Target</a> &#8226; <a href="0534.html">Building Something Smarter</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/v8/belief_in_intelligence/">Belief in Intelligence</a></p></body></html>