<html><head><title>By Which It May Be Judged</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>By Which It May Be Judged</h1><p><i>Eliezer Yudkowsky, 10 December 2012 04:26AM</i></p><div><p><strong>Followup to</strong>: <a href="0820.html">Mixed Reference: The Great Reductionist Project</a> [http://lesswrong.com/lw/frz/mixed_reference_the_great_reductionist_project/]</p> <blockquote> <p style="padding-left: 30px;"><span style="font-variant: small-caps;">Humans need fantasy to be human.</span></p> <p style="padding-left: 30px;">"Tooth fairies? Hogfathers? Little&#8212;"</p> <p style="padding-left: 30px;"><span style="font-variant: small-caps;">Yes. As practice. You have to start out learning to believe the <em>little</em> lies.</span></p> <p style="padding-left: 30px;">"So we can believe the big ones?"</p> <p style="padding-left: 30px;"><span style="font-variant: small-caps;">Yes. Justice. Mercy. Duty. That sort of thing.</span></p> <p style="padding-left: 30px;">"They're not the same at all!"</p> <p style="padding-left: 30px;"><span style="font-variant: small-caps;">You think so? Then take the universe and grind it down to the finest powder and sieve it through the finest sieve and then <em>show</em> me one atom of justice, one molecule of mercy.</span></p> <p style="padding-left: 60px;">- Susan and Death, in <em>Hogfather</em> by Terry Pratchett</p> </blockquote> <p><a href="0404.html">Suppose three people find a pie</a> [http://lesswrong.com/lw/ru/the_bedrock_of_fairness/] - that is, three people exactly simultaneously spot a pie which has been exogenously generated in unclaimed territory. Zaire wants the entire pie; Yancy thinks that 1/3 each is fair; and Xannon thinks that fair would be taking into equal account everyone's ideas about what is "fair".<a id="more"></a></p> <p>I myself would say unhesitatingly that a third of the pie each, is fair. "Fairness", as an ethical concept, can get a lot more complicated in more elaborate contexts. But in this simple context, a lot of other things that "fairness" could depend on, like work inputs, have been eliminated or made constant. Assuming no relevant conditions other than those already stated, "fairness" simplifies to the mathematical procedure of splitting the pie into equal parts; and when this logical function is run over physical reality, it outputs "1/3 for Zaire, 1/3 for Yancy, 1/3 for Xannon".</p> <p>Or to put it another way - just like we get "If Oswald hadn't shot Kennedy, nobody else would've" by <a href="0820.html">running a logical function over a true causal model</a> [http://lesswrong.com/lw/frz/mixed_reference_the_great_reductionist_project/] - similarly, we can get the hypothetical 'fair' situation, whether or not it actually happens, by running the physical starting scenario through a logical function that describes what a 'fair' outcome would look like:</p> <p><img src="0128e353.jpg" alt="" height="460" width="240"></p> <p>So am I (as Zaire would claim) just assuming-by-authority that I get to have everything my way, since I'm not defining 'fairness' the way <em>Zaire</em> wants to define it?</p> <p>No more than mathematicians are flatly ordering everyone to assume-without-proof that <a href="0818.html">two different numbers can't have the same successor</a> [http://lesswrong.com/lw/f4e/logical_pinpointing/]. For fairness to be what everyone thinks is "fair" would be <em>entirely</em> circular, structurally isomorphic to "Fzeem is what everyone thinks is fzeem"... or like trying to define the counting numbers as "whatever anyone thinks is a number". It only even <em>looks</em> coherent because everyone secretly already has a mental picture of "numbers" - because their brain already navigated to the referent.  But <em>something </em>akin to axioms is needed to talk about "numbers, as opposed to something else" in the first place. Even an inchoate mental image of "0, 1, 2, ..." implies the axioms no less than a formal statement - we can extract the axioms back out by asking <a href="0818.html">questions about this rough mental image</a> [http://lesswrong.com/lw/f4e/logical_pinpointing/].</p> <p>Similarly, the intuition that fairness has <em>something</em> to do with dividing up the pie equally, plays a role akin to secretly already having "0, 1, 2, ..." in mind as the subject of mathematical conversation. You need axioms, not as assumptions that aren't justified, but as pointers to what the heck the conversation is supposed to be <em>about.</em></p> <p>Multiple philosophers have suggested that this stance seems similar to "rigid designation", i.e., when I say 'fair' it intrinsically, rigidly refers to something-to-do-with-equal-division. I confess I don't see it that way myself - if somebody thinks of Euclidean geometry when you utter the sound "num-berz" they're not doing anything false, they're associating the sound to a different logical thingy. It's not about words with intrinsically rigid referential power, it's that the words are <em>window dressing</em> on the underlying entities. I want to <em>talk about</em> a particular <em>logical entity,</em> as it might be defined by either axioms or inchoate images, regardless of which word-sounds may be associated to it.  If you want to call that "rigid designation", that seems to me like adding a level of indirection; I don't care about the <em>word </em>'fair' in the first place, I care about the logical entity of fairness.  (Or to put it even more sharply: since my ontology does not have room for physics, logic, <em>plus</em> designation, I'm not very interested in discussing this 'rigid designation' business unless it's being reduced to something else.)</p> <p>Once issues of justice become more complicated and all the contextual variables get added back in, we might not be sure if a<em>disagreement</em> about 'fairness' reflects:</p> <ol> <li>The equivalent of a multiplication error within the same axioms - incorrectly dividing by 3.  (Or more complicatedly:  You might have a sophisticated axiomatic concept of 'equity', and <em>incorrectly </em>process those axioms to invalidly yield the assertion that, in a context where 2 of the 3 must starve and there's only enough pie for at most 1 person to survive, you should still divide the pie equally instead of flipping a 3-sided coin.  Where I'm assuming that this conclusion is 'incorrect', not because I disagree with it, but because it didn't actually follow from the axioms.)</li> <li>Mistaken models of the physical world fed into the function - mistakenly thinking there's 2 pies, or mistakenly thinking that Zaire has no subjective experiences and is not an object of ethical value.</li> <li>People associating different logical functions to the letters F-A-I-R, which isn't a <em>disagreement about</em> some common pinpointed variable, but just different people wanting different things.</li> </ol> <p>There's a lot of people who feel that this picture leaves out something fundamental, especially once we make the jump from "fair" to the broader concept of "moral", "good", or "right".  And it's this worry about leaving-out-something-fundamental that I hope to address next...</p> <p>...but please note, if we confess that 'right' lives in a world of physics and logic - because <em>everything</em> lives in a world of physics and logic - then we <em>have</em> to translate 'right' into those terms <em>somehow.</em></p> <p>And that is the answer Susan should have given - if she could talk about sufficiently advanced epistemology, sufficiently fast - to Death's entire statement:</p> <p style="padding-left: 30px;"><span style="font-variant: small-caps;">You think so? Then take the universe and grind it down to the finest powder and sieve it through the finest sieve and then <em>show</em> me one atom of justice, one molecule of mercy. And yet </span>&#8212; Death waved a hand. <span style="font-variant: small-caps;">And yet you act as if there is some ideal order in the world, as if there is some ... </span><em style="font-variant: small-caps;">rightness</em><span style="font-variant: small-caps;"> in the universe by which it may be judged.</span></p> <p>"But!" Susan should've said.  "When we judge the universe we're comparing it to a <em>logical </em>referent, a sort of thing that isn't <em>in</em> the universe!  Why, it's just like looking at a heap of 2 apples and a heap of 3 apples on a table, and comparing their invisible product to the number 6 - there isn't any 6 if you grind up the whole table, even if you grind up the whole universe, but the product is <em>still</em> 6, physico-logically speaking."</p> <hr> <p>If you require that Rightness be written on some particular great Stone Tablet somewhere - to be "a light that shines from the sky", outside people, as a different Terry Pratchett book put it - then indeed, there's no such Stone Tablet anywhere in our universe.</p> <p>But there <em>shouldn't</em> be such a Stone Tablet, <em>given</em> standard intuitions about morality.  This follows from the Euthryphro Dilemma out of ancient Greece.</p> <p>The original Euthryphro dilemma goes, "Is it pious because it is loved by the gods, or loved by the gods because it is pious?" The religious version goes, "Is it good because it is commanded by God, or does God command it because it is good?"</p> <p>The standard atheist reply is:  "Would you say that it's an intrinsically good thing - even if the event has no further causal consequences which are good - to slaughter babies or torture people, if that's what God says to do? If so, then it seems to me that you have no morality and that your religion has destroyed your humanity."</p> <p>So if we can't make it good to slaughter babies by tweaking the state of God, then morality doesn't come from God; so goes the standard atheist argument.</p> <p>But if you can't make it good to slaughter babies by tweaking the physical state of <em>anything</em> - if we can't imagine a world where some great Stone Tablet of Morality has been physically rewritten, and what is right has changed - then this is telling us that...</p> <p>(drumroll)</p> <p>...what's "right" is a logical thingy rather than a physical thingy, that's all.  The mark of a logical validity is that we can't concretely visualize a coherent possible world where the proposition is false.</p> <p>And I mention this in hopes that I can show that it is not moral anti-realism to say that moral statements take their truth-value from logical entities.  Even in Ancient Greece, philosophers implicitly knew that 'morality' ought to be such an entity - that it <em>couldn't</em> be something you found when you ground the Universe to powder, because then you could resprinkle the powder and make it wonderful to kill babies - though they didn't know how to say what they knew.</p> <hr> <p>There's a lot of people who still feel that Death <em>would</em> be right, if the universe were all physical; that the kind of dry logical entity I'm describing here, isn't sufficient to carry the bright alive feeling of goodness.</p> <p>And there are others who accept that physics and logic is everything, but who - I think <em>mistakenly</em> - go ahead and also accept Death's stance that this makes morality a lie, or, in lesser form, that the bright alive feeling can't make it.  (Sort of like people who accept an incompatibilist theory of free will, also accept physics, and conclude with sorrow that they are indeed being <a href="0374.html">controlled by physics</a> [http://lesswrong.com/lw/r0/thou_art_physics/].)</p> <p>In case anyone is bored that I'm <em>still</em> trying to fight this battle, well, here's a quote from a recent Facebook conversation with a famous early transhumanist:</p> <blockquote> <p>No doubt a "crippled" AI that didn't understand the existence or nature of first-person facts could be nonfriendly towards sentient beings... Only a zombie wouldn't value Heaven over Hell. For reasons we simply don't understand, the negative value and normative aspect of agony and despair is built into the nature of the experience itself. Non-reductionist? Yes, on a standard materialist ontology. But not IMO within a more defensible Strawsonian physicalism.</p> </blockquote> <p>It would actually be <em>quite surprisingly helpful</em> for increasing the percentage of people who will participate meaningfully in saving the planet, if there were some reliably-working standard explanation for why physics and logic together have enough room to contain morality.  People who think that reductionism means we have to lie to our children, as Pratchett's Death advocates, won't be much enthused about the Center for Applied Rationality.  And there are a fair number of people out there who still advocate proceeding in the confidence of ineffable morality to construct sloppily designed AIs.</p> <p>So far I don't know of any exposition that works reliably - for the thesis for how morality <em>including</em> our intuitions about whether things <em>really are justified</em> and so on, is preserved in the analysis to physics plus logic; that morality has been <a href="0290.html">explained rather than explained away</a> [http://lesswrong.com/lw/oo/explaining_vs_explaining_away/].  Nonetheless I shall now take another stab at it, starting with a simpler bright feeling:</p> <hr> <p>When I see an unusually neat mathematical proof, unexpectedly short or surprisingly general, my brain gets a joyous sense of <em>elegance.</em></p> <p><em></em>There's presumably some functional slice through my brain that implements this emotion - some configuration subspace of spiking neural circuitry which corresponds to my <em>feeling</em> of elegance.  Perhaps I should say that elegance is <em>merely </em>about my brain switching on its elegance-signal?  But there are concepts like <a href="http://wiki.lesswrong.com/wiki/Kolmogorov_complexity">Kolmogorov complexity</a> [http://wiki.lesswrong.com/wiki/Kolmogorov_complexity] that give more formal meanings of "simple" than "Simple is whatever makes my brain feel the emotion of simplicity."  Anything you do to fool my brain wouldn't make the proof <em>really</em> elegant, not in that sense.  The emotion is not free of semantic content; we could build a correspondence theory for it and navigate to its logical+physical referent, and say:  "Sarah feels like this proof is elegant, and her feeling is <em>true.</em>"  You could even say that certain proofs are elegant even if no conscious agent sees them.</p> <p>My description of 'elegance' admittedly did invoke agent-dependent concepts like 'unexpectedly' short or 'surprisingly' general.  It's almost certainly true that with a different mathematical background, I would have different standards of elegance and experience that feeling on <em>somewhat</em> different occasions.  Even so, that still seems like moving around in a field of <em>similar </em>referents for the emotion - much more similar to each other than to, say, the distant cluster of 'anger'.</p> <p>Rewiring my brain so that the 'elegance' sensation gets activated when I see mathematical proofs where the words have lots of vowels - that wouldn't <em>change</em> what is elegant.  Rather, it would make the feeling be <em>about </em>something else entirely; different semantics with a different truth-condition.</p> <p>Indeed, it's not clear that this thought experiment is, or should be, <em>really</em> conceivable.  If all the associated computation is about vowels instead of elegance, then <a href="0254.html">from the inside</a> [http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/] you would expect that to <em>feel vowelly</em>, not <em>feel elegant</em>...</p> <p>...which is to say that even feelings can be associated with logical entities.  Though unfortunately not in any way that will <em>feel like</em> qualia if you can't read your own source code.  I could write out an exact description of your visual cortex's spiking code for 'blue' on paper, and it wouldn't actually <em>look </em><span style="color:#0000FF">blue </span>to you.  Still, on the higher level of description, it should seem intuitively plausible that if you tried rewriting the relevant part of your brain to count vowels, the resulting sensation would no longer have the content or even the <em>feeling </em>of elegance.  It would compute vowelliness, and feel vowelly.</p> <hr> <p>My feeling of mathematical elegance is motivating; it makes me more likely to search for similar such proofs later and go on doing math.  You could construct an agent that tried to add more vowels instead, and if the agent asked itself why it was doing that, the resulting justification-thought wouldn't <em>feel like</em> because-it's-elegant, it would <em>feel like</em> because-it's-vowelly.</p> <p>In the same sense, when you try to do what's right, you're motivated by things like (to yet again quote Frankena's list of terminal values):</p> <blockquote> <p>"Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in one's own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc."</p> </blockquote> <p>If we reprogrammed you to count paperclips instead, it wouldn't feel like <em>different</em> things having the <em>same</em> kind of motivation behind it.  It wouldn't feel like doing-what's-right for a different guess about what's right.  It would feel like doing-what-leads-to-paperclips.</p> <p>And I quoted the above list because the feeling of rightness isn't <em>about</em> implementing a particular logical function; it contains no mention of logical functions at all; in the environment of evolutionary ancestry nobody has <em>heard</em> of axiomatization; these feelings are <em>about</em> life, consciousness, etcetera.  If I could write out the whole truth-condition of the feeling in a way you could compute, you would still feel Moore's Open Question:  "I can see that this event is high-rated by logical function X, but is X really <em>right?</em>" - since you can't read your own source code and the description wouldn't be commensurate with your brain's native format.</p> <p>"But!" you cry.  "But, is it really <em>better</em> to do what's right, than to maximize paperclips?"  Yes!  As soon as you start trying to cash out the logical function that gives betterness its truth-value, it will output "life, consciousness, etc. &gt;<sub>B</sub> paperclips".  And if your brain were computing a different logical function instead, like makes-more-paperclips, it wouldn't feel <em>better,</em> it would feel <em>moreclippy.</em></p> <p>But is it really <em>justified</em> to keep our own sense of betterness?  Sure, and that's a logical fact - it's the objective output of the logical function corresponding to your experiential sense of what it means for something to be 'justified' in the first place.  This doesn't mean that Clippy the Paperclip Maximizer will self-modify to do only things that are justified; Clippy doesn't judge between self-modifications by computing justifications, but rather, computing <em>clippyflurphs.</em></p> <p>But isn't it <em>arbitrary</em> for Clippy to maximize paperclips?  Indeed; once you implicitly or explicitly pinpoint the logical function that gives judgments of arbitrariness their truth-value - presumably, revolving around the presence or absence of justifications - then this logical function will objectively yield that there's no justification whatsoever for maximizing paperclips (which is why<em> I'm </em>not going to do it) and hence that Clippy's decision is arbitrary. Conversely, Clippy finds that there's no clippyflurph for preserving life, and hence that it is unclipperiffic.  But unclipperifficness isn't arbitrariness any more than the number 17 is a right triangle; they're different logical entities pinned down by different axioms, and the corresponding judgments will have different semantic content and <em>feel different.</em>  If Clippy is architected to experience that-which-you-call-qualia, Clippy's feeling of clippyflurph will be <em>structurally</em> different from the way justification feels, not just red versus blue, but vision versus sound.</p> <p>But surely one <em>shouldn't</em> praise the clippyflurphers rather than the just?  I quite agree; and as soon as you navigate referentially to the coherent logical entity that is the truth-condition of <em>should</em> - a function on potential actions and future states - it will agree with you that it's better to avoid the arbitrary than the unclipperiffic.  Unfortunately, this logical fact does not correspond to the truth-condition of any meaningful proposition computed by Clippy in the course of how it efficiently transforms the universe into paperclips, in much the same way that rightness plays no role in that-which-is-maximized by the blind processes of natural selection.</p> <p>Where moral judgment is concerned, it's logic all the way down.  <em>ALL </em>the way down.  Any frame of reference where you're worried that it's <em>really </em>no better to do what's right then to maximize paperclips... well, that <em>really</em> part has a truth-condition (or what does the "really" mean?) and as soon as you write out the truth-condition you're going to end up with yet another ordering over actions or algorithms or meta-algorithms or <em>something.</em>  And since grinding up the universe won't and <em>shouldn't</em> yield any miniature '&gt;' tokens, it must be a <em>logical</em> ordering.  And so whatever logical ordering it is you're worried about, it probably <em>does</em> produce 'life &gt; paperclips' - but Clippy isn't computing that logical fact any more than your pocket calculator is computing it.</p> <p>Logical facts have no power to directly affect the universe except when some part of the universe is computing them, and morality is (and <em>should</em> be) logic, not physics.</p> <p>Which is to say:</p> <blockquote> <p>The old wizard was staring at him, a sad look in his eyes. "I suppose I <em>do</em> understand now," he said quietly.</p> <p>"Oh?" said Harry. "Understand what?"</p> <p>"Voldemort," said the old wizard. "I understand him now at last. Because to believe that the world is truly like that, you must believe there is no justice in it, that it is woven of darkness at its core. I asked you why he became a monster, and you could give no reason. And if I could ask <em>him</em>, I suppose, his answer would be: Why not?"</p> <p>They stood there gazing into each other's eyes, the old wizard in his robes, and the young boy with the lightning-bolt scar on his forehead.</p> <p>"Tell me, Harry," said the old wizard, "will <em>you</em> become a monster?"</p> <p>"No," said the boy, an iron certainty in his voice.</p> <p>"Why not?" said the old wizard.</p> <p>The young boy stood very straight, his chin raised high and proud, and said: "There is no justice in the laws of Nature, Headmaster, no term for fairness in the equations of motion. The universe is neither evil, nor good, it simply does not care. The stars don't care, or the Sun, or the sky. But they don't have to! <em>We</em> care! There <em>is</em> light in the world, and it is <em>us!</em>"</p> </blockquote> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners"><em>Highly Advanced Epistemology 101 for Beginners</em></a> [http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners]</p> <p style="text-align:right">Next post: "<a href="0822.html">Standard and Nonstandard Numbers</a> [http://lesswrong.com/lw/g0i/standard_and_nonstandard_numbers/]"</p> <p style="text-align:right">Previous post: "<a href="0820.html">Mixed Reference: The Great Reductionist Project</a> [http://lesswrong.com/lw/frz/mixed_reference_the_great_reductionist_project/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq17.html">Sequence 17: Highly Advanced Epistemology 101 for Beginners</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0820.html">Mixed Reference: The Great Reductionist Project</a></p></td><td><p><i>Next: </i><a href="seq18.html">Sequence 18: Joy in the Merely Real</a></p></td></tr></table><p><i>Referenced by: </i><a href="0820.html">Mixed Reference: The Great Reductionist Project</a> &#8226; <a href="0822.html">Standard and Nonstandard Numbers</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/r/lesswrong/lw/fv3/by_which_it_may_be_judged/">By Which It May Be Judged</a></p></body></html>