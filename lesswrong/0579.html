<html><head><title>What I Think, If Not Why</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>What I Think, If Not Why</h1><p><i>Eliezer Yudkowsky, 11 December 2008 05:41PM</i></p><div><p><strong>Reply to</strong>:  <a href="http://www.overcomingbias.com/2008/12/two-visions-of.html">Two Visions Of Heritage</a> [http://www.overcomingbias.com/2008/12/two-visions-of.html]</p><p>Though it really goes tremendously against my grain - it feels like sticking my neck out over a cliff (or something) - I guess I have no choice here but to try and make a list of <em>just </em>my positions, without justifying them.  We can only talk justification, I guess, after we get straight what my positions <em>are.</em>  I will also <a href="http://www.overcomingbias.com/2008/06/against-disclai.html">leave off many disclaimers</a> [http://www.overcomingbias.com/2008/06/against-disclai.html] to present the points <em>compactly</em> enough to be <em>remembered.</em></p><p>&#8226; A well-designed mind should be <strong><em>much more efficient</em> than a human</strong>, capable of doing more with <a href="0358.html">less sensory data</a> [http://lesswrong.com/lw/qk/that_alien_message/] and <a href="0347.html">fewer computing operations</a> [http://lesswrong.com/lw/q9/the_failures_of_eld_science/].  It is not <em>infinitely efficient</em> and <strong>does not use <em>zero</em></strong> <strong>data</strong>.  But it does use little enough that <em>local pipelines</em> such as a small pool of programmer-teachers and, later, a huge pool of e-data, are sufficient.</p><p>&#8226; An AI that reaches a certain point in its own development becomes able to (<a href="0572.html">sustainably, strongly</a> [http://lesswrong.com/lw/wi/sustained_strong_recursion/]) improve itself.  At this point, <strong><a href="0560.html">recursive</a> [http://lesswrong.com/lw/w6/recursion_magic/] <a href="0559.html">cascades</a> [http://lesswrong.com/lw/w5/cascades_cycles_insight/] slam over many internal growth curves to near the limits of their current hardware</strong>, and the AI undergoes a vast increase in capability.  This point is at, or probably considerably before, a minimally transhuman mind capable of writing its own AI-theory textbooks - an upper bound beyond which it could swallow and improve its <em>entire </em>design chain.</p><p>&#8226; It is <em>likely</em> that this capability increase or "FOOM" has an intrinsic maximum velocity that a human would regard as "fast" if it happens at all.  A human week is ~1e15 serial operations for a population of 2GHz cores, and a century is ~1e19 serial operations; this whole range is a narrow window.  However,<strong> <em>the core argument does not require one-week speed</em></strong> and a FOOM that takes two years (~1e17 serial ops) will still carry the weight of the argument.</p><p> </p><a id="more"></a> <p> </p><p>
&#8226; <strong>The <em>default </em>case of FOOM is an unFriendly AI</strong>, built by researchers with shallow insights.  This AI becomes able to improve itself in a haphazard way, makes various changes that are net improvements but may introduce value drift, and then gets smart enough to do guaranteed self-improvement, at which point its values freeze (forever).</p><p>&#8226; <strong>The <em>desired</em> case of FOOM is a Friendly AI</strong>, built using deep insight, so that the AI never makes any changes to itself that potentially change its internal values; all such changes are guaranteed using <a href="0547.html">strong techniques</a> [http://lesswrong.com/lw/vt/the_nature_of_logic/] that allow for a billion sequential self-modifications without losing the guarantee.  The guarantee is written over the AI's <em>internal search criterion</em> for actions, rather than <em>external consequences</em>.</p><p>&#8226; The <strong>good guys do <em>not </em>write</strong> an AI which values <strong>a bag of things that the programmers think are good ideas</strong>, like libertarianism or socialism or making people happy or whatever.  There were <em>multiple</em> Overcoming Bias sequences about this <em>one point</em>, like the <a href="0183.html">Fake Utility Function sequence</a> [http://lesswrong.com/lw/lp/fake_fake_utility_functions/] and the sequence on metaethics.  It is dealt with at length in the document <a href="http://intelligence.org/upload/CEV.html"><strong>Coherent</strong> <strong>*Extrapolated*</strong> Volition</a> [http://intelligence.org/upload/CEV.html].&#160;
It is the first thing, the last thing, and the middle thing that I say about Friendly AI.  I have said it over and over.  I truly do not understand how anyone can pay <em>any</em> attention to <em>anything</em> I have said on this subject, and come away with the impression that I think programmers are supposed to directly impress their non-meta personal philosophies onto a Friendly AI.</p><p>&#8226; <strong>The good guys do not directly impress their personal values onto a Friendly AI.<br></strong></p><p>&#8226; Actually setting up a Friendly AI's values is <strong>an extremely <em>meta</em> operation</strong>, less "make the AI want to make people happy" and more like "<a href="http://intelligence.org/upload/CEV.html"><strong>superpose</strong><span> the possible </span><strong><span>reflective equilibria</span></strong><span> of the <strong>whole </strong></span><strong><span>human species</span></strong><span>, and <strong>output new code</strong> that overwrites the current AI and has the <strong>most coherent</strong> support within that superposition</span></a> [http://intelligence.org/upload/CEV.html]".  This actually seems to be something of a Pons Asinorum in FAI - the ability to understand and endorse metaethical concepts that do not <em>directly </em>sound like amazing wonderful happy ideas.  <strong>Describing this as declaring total war on the rest of humanity, does not seem <a href="0404.html">fair</a> [http://lesswrong.com/lw/ru/the_bedrock_of_fairness/]</strong><span> </span>(or accurate).</p><p>&#8226; <strong>I myself am strongly individualistic</strong>:  The most painful memories in my life have been when other people thought they knew better than me, and tried to do things on my behalf.  It is also a known principle of hedonic psychology that people are happier when they're steering their own lives and doing their own interesting work.  When I try myself to visualize what a beneficial superintelligence ought to do, it consists of <strong>setting up a world that works by better rules, and then fading into the background</strong>, silent as the laws of Nature once were; and finally folding up and vanishing when it is no longer needed.  But this is only the thought of my mind that is merely human, and <strong>I am barred from programming any such consideration <em>directly </em>into a Friendly AI</strong>, for the reasons given above.</p><p>&#8226; Nonetheless, it does seem to me that this particular scenario <strong>could not be justly described as "a God to rule over us all"</strong>, unless the current fact that humans age and die is "a malevolent God to rule us all".  So either Robin has a very different idea about what human reflective equilibrium values are likely to look like; or Robin believes that the Friendly AI project is bound to <em>fail</em> in such way as to create a paternalistic God; or - and this seems more likely to me - Robin didn't read all the way through all the blog posts in which I tried to explain all the ways that this is not how Friendly AI works.</p><p>&#8226; <strong>Friendly AI is technically difficult and requires an <a href="0506.html">extra-ordinary</a> [http://lesswrong.com/lw/uo/make_an_extraordinary_effort/] effort on multiple levels.</strong>  <a href="0171.html">English sentences</a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/] like "make people happy" cannot describe the values of a Friendly AI.  <a href="0459.html">Testing is not sufficient to guarantee that values have been successfully transmitted</a> [http://lesswrong.com/lw/td/magical_categories/].</p><p>&#8226; White-hat AI researchers are distinguished by the degree to which <strong>they understand that a single misstep could be fatal, and can discriminate strong and weak assurances</strong>.  Good intentions are not only common, they're cheap.  The story isn't about good versus evil, it's about people trying to <a href="0507.html">do the impossible</a> [http://lesswrong.com/lw/up/shut_up_and_do_the_impossible/] versus <a href="0494.html">others</a> [http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/] who... aren't.<span style="font-style: italic;"></span></p><p>&#8226; Intelligence is about being able to <strong>learn lots of things, not about knowing lots of things</strong>.  Intelligence is especially not about tape-recording lots of parsed English sentences a la Cyc.  Old AI work was poorly focused due to inability to introspectively see the first and higher <em>derivatives </em>of knowledge; human beings have an easier time reciting sentences than reciting their ability to learn.</p><p>&#8226; <strong>Intelligence is mostly about architecture</strong>, or "knowledge" along the lines of knowing to look for causal structure (Bayes-net type stuff) in the environment; this kind of knowledge will usually be expressed procedurally as well as declaratively.<strong>  Architecture is mostly about deep insights.</strong>  This point has not yet been addressed (much) on Overcoming Bias, but Bayes nets can be considered as an archetypal example of "architecture" and "deep insight".  Also, ask yourself how lawful intelligence seemed to you before you started reading this blog, how lawful it seems to you now, then extrapolate outward from that.</p></div> <hr><p><i>Referenced by: </i><a href="0583.html">Not Taking Over the World</a> &#8226; <a href="0591.html">Imaginary Positions</a> &#8226; <a href="0593.html">Devil's Offers</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/wp/what_i_think_if_not_why/">What I Think, If Not Why</a></p></body></html>