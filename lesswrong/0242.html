<html><head><title>Newcomb's Problem and Regret of Rationality</title></head><body><h1>Newcomb's Problem and Regret of Rationality</h1><p><i>Eliezer Yudkowsky, 31 January 2008 07:36PM</i></p><div><p><strong>Followup to</strong>:  <a href="0241.html">Something to Protect</a> [http://lesswrong.com/lw/nb/something_to_protect/]</p> <p>The following may well be the most controversial dilemma in the history of decision theory:</p> <blockquote> <p>A superintelligence from another galaxy, whom we shall call Omega, comes to Earth and sets about playing a strange little game.  In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.</p> <p>Box A is transparent and contains a thousand dollars.<br>Box B is opaque, and contains either a million dollars, or nothing.</p> <p>You can take both boxes, or take only box B.</p> <p>And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.</p> <p>Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars.  (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)</p> <p>Before you make your choice, Omega has flown off and moved on to its next game.  Box B is already empty or already full.</p> <p>Omega drops two boxes on the ground in front of you and flies off.</p> <p>Do you take both boxes, or only box B?</p> </blockquote> <p>And the standard philosophical conversation runs thusly:</p> <blockquote> <p>One-boxer:  "I take only box B, of course.  I'd rather have a million than a thousand."</p> <p>Two-boxer:  "Omega has already left.  Either box B is already full or already empty.  If box B is already empty, then taking both boxes nets me $1000, taking only box B nets me $0.  If box B is already full, then taking both boxes nets $1,001,000, taking only box B nets $1,000,000.  In either case I do better by taking both boxes, and worse by leaving a thousand dollars on the table - so I will be rational, and take both boxes."</p> <p>One-boxer:  "If you're so rational, why ain'cha rich?"</p> <p>Two-boxer:  "It's not my fault Omega chooses to reward only people with irrational dispositions, but it's already too late for me to do anything about that."</p> </blockquote> <p><a id="more"></a></p> <p>There is a <em>large</em> literature on the topic of Newcomblike problems - especially if you consider the Prisoner's Dilemma as a special case, which it is generally held to be.  "Paradoxes of Rationality and Cooperation" is an edited volume that includes Newcomb's original essay.  For those who read only online material, <a href="http://w3.ub.uni-konstanz.de/v13/volltexte/2000/524//pdf/ledwig.pdf">this PhD thesis</a> [http://w3.ub.uni-konstanz.de/v13/volltexte/2000/524//pdf/ledwig.pdf] summarizes the major standard positions.</p> <p>I'm not going to go into the whole literature, but the dominant consensus in modern decision theory is that one should two-box, and Omega is just rewarding agents with irrational dispositions.  This dominant view goes by the name of "causal decision theory".</p> <p>As you know, the primary <a href="0101.html">reason I'm blogging</a> [http://lesswrong.com/lw/jf/why_im_blooking/] is that I am an incredibly slow writer when I try to work in any other format.  So I'm not going to try to present my own analysis here.  Way too long a story, even by my standards.</p> <p>But it is agreed even among causal decision theorists that if you have the power to precommit yourself to take one box, in Newcomb's Problem, then you should do so.  If you can precommit yourself before Omega examines you; then you are directly causing box B to be filled.</p> <p>Now in my field - which, in case you have forgotten, is self-modifying AI - this works out to saying that if you build an AI that two-boxes on Newcomb's Problem, it will self-modify to one-box on Newcomb's Problem, if the AI considers in advance that it might face such a situation.  Agents with free access to their own source code have access to a cheap method of precommitment.</p> <p>What if you expect that you might, in general, face a Newcomblike problem, without knowing the exact form of the problem?  Then you would have to modify yourself into a sort of agent whose disposition was such that it would generally receive high rewards on Newcomblike problems.</p> <p>But what does an agent with a disposition generally-well-suited to Newcomblike problems look like?  Can this be formally specified?</p> <p>Yes, but when I tried to write it up, I realized that I was starting to write a small book.  And it wasn't the most important book I had to write, so I shelved it.  My slow writing speed really is the bane of my existence.  The theory I worked out seems, to me, to have many nice properties besides being well-suited to Newcomblike problems.  It would make a nice PhD thesis, if I could get someone to accept it as my PhD thesis.  But that's pretty much what it would take to make me unshelve the project.  Otherwise I can't justify the time expenditure, not at the speed I currently write books.</p> <p>I say all this, because there's a common attitude that "Verbal arguments for one-boxing are easy to come by, what's hard is developing a good decision theory that one-boxes" - coherent math which one-boxes on Newcomb's Problem without producing absurd results elsewhere.  So I do understand that, and I did set out to develop such a theory, but my writing speed on big papers is so slow that I can't publish it.  Believe it or not, it's true.</p> <p>Nonetheless, I would like to present some of my <em>motivations</em> on Newcomb's Problem - the reasons I felt impelled to seek a new theory - because they illustrate my source-attitudes toward rationality.  Even if I can't present the theory that these motivations motivate...</p> <p>First, foremost, fundamentally, above all else:</p> <p>Rational agents should WIN.</p> <p>Don't mistake me, and think that I'm talking about the Hollywood Rationality stereotype that rationalists should be selfish or shortsighted.  If your utility function has a term in it for others, then win their happiness.  If your utility function has a term in it for a million years hence, then win the eon.</p> <p>But at any rate, <em>WIN</em>.  Don't lose reasonably, <strong><em>WIN</em></strong>.</p> <p>Now there are defenders of causal decision theory who argue that the two-boxers are doing their best to win, and cannot help it if they have been cursed by a Predictor who favors irrationalists.  I will talk about this defense in a moment.  But first, I want to draw a distinction between causal decision theorists who believe that two-boxers are genuinely doing their best to win; versus someone who thinks that two-boxing is the <em>reasonable</em> or the <em>rational</em> thing to do, but that the reasonable move just happens to predictably lose, in this case.  There are a <em>lot</em> of people out there who think that rationality predictably loses on various problems - that, too, is part of the Hollywood Rationality stereotype, that Kirk is predictably superior to Spock.</p> <p>Next, let's turn to the charge that Omega favors irrationalists.  I can conceive of a superbeing who rewards only people born with a particular gene, <em>regardless of their choices.</em>  I can conceive of a superbeing who rewards people whose brains inscribe the <em>particular algorithm</em> of "Describe your options in English and choose the last option when ordered alphabetically," but who does not reward anyone who chooses the same option for a different reason.  But Omega rewards people who choose to take only box B, <em>regardless of which algorithm they use to arrive at this decision,</em> and this is why I don't buy the charge that Omega is rewarding the irrational.  Omega doesn't care whether or not you follow some particular ritual of cognition; Omega only cares about your predicted <em>decision</em>.</p> <p>We can choose whatever reasoning algorithm we like, and will be rewarded or punished only according to that algorithm's choices, with no other dependency - Omega just cares where we go, not how we got there.</p> <p>It is precisely the notion that Nature does not care about our <em>algorithm,</em> which frees us up to pursue the winning Way - without attachment to any particular ritual of cognition, apart from our belief that it wins.  Every rule is up for grabs, <em>except</em> the rule of winning.</p> <p>As Miyamoto Musashi said - it's really worth repeating:</p> <blockquote> <p>"You can win with a long weapon, and yet you can also win with a short weapon.  In short, the Way of the Ichi school is the spirit of winning, whatever the weapon and whatever its size."</p> </blockquote> <p>(Another example:  It was <a href="0240.html">argued by McGee</a> [http://lesswrong.com/lw/na/trust_in_bayes/] that we must adopt bounded utility functions or be subject to "Dutch books" over infinite times.  But:  <em>The utility function is not up for grabs.</em>  I love life <a href="http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/">without limit or upper bound:</a> [http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/]  There is no finite amount of life lived N where I would prefer a 80.0001% probability of living N years to an 0.0001% chance of living a googolplex years and an 80% chance of living forever.  This is a sufficient condition to imply that my utility function is unbounded.  So I just have to figure out how to optimize <em>for that morality.</em>  You can't tell me, first, that above all I must conform to a particular ritual of cognition, and then that, if I conform to that ritual, I must change my morality to avoid being Dutch-booked.  Toss out the losing ritual; don't change the definition of winning.  That's like deciding to prefer $1000 to $1,000,000 so that Newcomb's Problem doesn't make your preferred ritual of cognition look bad.)</p> <p>"But," says the causal decision theorist, "to take only one box, you must somehow believe that your choice can affect whether box B is empty or full - and that's <em>unreasonable!</em>  Omega has already left!  It's physically impossible!"</p> <p>Unreasonable?  I am a rationalist: what do I care about being unreasonable?  I don't have to conform to a particular ritual of cognition.  I don't have to take only box B <em>because I believe my choice affects the box, even though Omega has already left.</em>  I can just... take only box B.</p> <p>I do have a proposed alternative ritual of cognition which computes this decision, which this margin is too small to contain; but I shouldn't need to show this to you.  The point is not to have an elegant theory of winning - the point is to win; elegance is a side effect.</p> <p>Or to look at it another way:  Rather than starting with a concept of what is the reasonable decision, and then asking whether "reasonable" agents leave with a lot of money, start by looking at the agents who leave with a lot of money, develop a theory of which agents tend to leave with the most money, and from this theory, try to figure out what is "reasonable".  "Reasonable" may just refer to decisions in conformance with our current ritual of cognition - what else would determine whether something seems "reasonable" or not?</p> <p>From James Joyce (no relation), <em>Foundations of Causal Decision Theory:</em></p> <blockquote> <p>Rachel has a perfectly good answer to the "Why ain't you rich?" question.  "I am not rich," she will say, "because I am not the kind of person the psychologist thinks will refuse the money.  I'm just not like you, Irene.  Given that I know that I am the type who takes the money, and given that the psychologist knows that I am this type, it was reasonable of me to think that the $1,000,000 was not in my account.  The $1,000 was the most I was going to get no matter what I did.  So the only reasonable thing for me to do was to take it."</p> <p>Irene may want to press the point here by asking, "But don't you wish you were like me, Rachel?  Don't you wish that you were the refusing type?"  There is a tendency to think that Rachel, a committed causal decision theorist, must answer this question in the negative, which seems obviously wrong (given that being like Irene would have made her rich).  This is not the case.  Rachel can and should admit that she does wish she were more like Irene.  "It would have been better for me," she might concede, "had I been the refusing type."  At this point Irene will exclaim, "You've admitted it!  It wasn't so smart to take the money after all."  Unfortunately for Irene, her conclusion does not follow from Rachel's premise.  Rachel will patiently explain that wishing to be a refuser in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 <em>whatever type one is.</em>  When Rachel wishes she was Irene's type she is wishing <em>for Irene's options,</em> not sanctioning her choice.</p> </blockquote> <p>It is, I would say, a general principle of rationality - indeed, part of how I <em>define</em> rationality - that you never end up envying someone else's mere <em>choices.</em>  You might envy someone their genes, if Omega rewards genes, or if the genes give you a generally happier disposition.  But Rachel, above, envies Irene her choice, and <em>only</em> her choice, irrespective of what algorithm Irene used to make it.  Rachel wishes<em> just</em> that she had a disposition to choose differently.</p> <p>You shouldn't claim to be more rational than someone and simultaneously envy them their choice - <em>only</em> their choice.  Just <em>do</em> the act you envy.</p> <p>I keep trying to say that rationality is the winning-Way, but causal decision theorists insist that taking both boxes is what <em>really</em> wins, because you <em>can't possibly do better</em> by leaving $1000 on the table... even though the single-boxers leave the experiment with more money.  Be careful of this sort of argument, any time you find yourself defining the "winner" as someone other than the agent who is currently smiling from on top of a giant heap of utility.</p> <p>Yes, there are various thought experiments in which some agents start out with an advantage - but if the task is to, say, decide whether to jump off a cliff, you want to be careful not to define cliff-refraining agents as having an unfair prior advantage over cliff-jumping agents, by virtue of their unfair refusal to jump off cliffs.  At this point you have covertly redefined "winning" as conformance to a particular ritual of cognition.  <em>Pay attention to the money!</em></p> <p>Or here's another way of looking at it:  Faced with Newcomb's Problem, would you want to look really hard for a reason to believe that it was perfectly reasonable and rational to take only box B; because, if such a line of argument existed, you would take only box B and find it full of money?  Would you spend an extra hour thinking it through, if you were confident that, at the end of the hour, you would be able to convince yourself that box B was the rational choice?  This too is a rather odd position to be in.  Ordinarily, the work of rationality goes into figuring out which choice is the best - not finding a reason to believe that a particular choice is the best.</p> <p>Maybe it's too easy to say that you "ought to" two-box on Newcomb's Problem, that this is the "reasonable" thing to do, so long as the money isn't actually in front of you.  Maybe you're just numb to philosophical dilemmas, at this point.  What if your daughter had a 90% fatal disease, and box A contained a serum with a 20% chance of curing her, and box B might contain a serum with a 95% chance of curing her?  What if there was an asteroid rushing toward Earth, and box A contained an asteroid deflector that worked 10% of the time, and box B might contain an asteroid deflector that worked 100% of the time?</p> <p>Would you, at that point, find yourself <em>tempted to make an unreasonable choice?</em></p> <p>If the stake in box B was <a href="0241.html">something you <em>could not</em> leave behind</a> [http://lesswrong.com/lw/nb/something_to_protect/]?  Something overwhelmingly more important to you than being reasonable?  If you absolutely <em>had to</em> win - <em>really</em> win, not just be defined as winning?</p> <p>Would you <em>wish with all your power</em> that the "reasonable" decision was to take only box B?</p> <p>Then maybe it's time to update your definition of reasonableness.</p> <p>Alleged rationalists should not find themselves envying the mere decisions of alleged nonrationalists, because your decision can be whatever you like.  When you find yourself in a position like this, you shouldn't chide the other person for failing to conform to your concepts of reasonableness.  You should realize you got the Way wrong.</p> <p>So, too, if you ever find yourself keeping separate track of the "reasonable" belief, versus the belief that seems likely to be actually <em>true.</em>  Either you have misunderstood reasonableness, or your second intuition is just wrong.</p> <p>Now one can't simultaneously <em>define</em> "rationality" as the winning Way, and <em>define</em> "rationality" as Bayesian probability theory and decision theory.  But it is the argument that I am putting forth, and the moral of my advice to <a href="0240.html">Trust In Bayes</a> [http://lesswrong.com/lw/na/trust_in_bayes/], that the laws governing winning have indeed proven to be <a href="0223.html">math</a> [http://lesswrong.com/lw/mt/beautiful_probability/].  If it ever turns out that Bayes fails - receives systematically lower rewards on some problem, relative to a superior alternative, in virtue of its mere decisions - then Bayes has to go <em>out the window.  </em>"Rationality" is just the label I use for my beliefs about the winning Way - the Way of the agent smiling from on top of the giant heap of utility.  <em>Currently, </em>that label refers to Bayescraft.</p> <p>I realize that this is not a knockdown criticism of causal decision theory - that would take the actual book and/or PhD thesis - but I hope it illustrates some of my underlying attitude toward this notion of "rationality".</p> <p>You shouldn't find yourself distinguishing the winning choice from the reasonable choice.  Nor should you find yourself distinguishing the reasonable belief from the belief that is most likely to be true.</p> <p>That is why I use the word "rational" to denote my beliefs about accuracy and winning - <em>not</em> to denote <a href="0002.html">verbal</a> [http://lesswrong.com/lw/go/why_truth_and/] reasoning, or strategies which yield <a href="0216.html">certain</a> [http://lesswrong.com/lw/mm/the_fallacy_of_gray/] success, or that which is <a href="0124.html">logically</a> [http://lesswrong.com/lw/k2/a_priori/] provable, or that which is <a href="0073.html">publicly demonstrable</a> [http://lesswrong.com/lw/in/scientific_evidence_legal_evidence_rational/], or that which is reasonable.</p> <p>As Miyamoto Musashi said:</p> <blockquote> <p>"The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy's cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him."</p> </blockquote></div> <hr><p><i>Referenced by: </i><a href="0261.html">Replace the Symbol with the Substance</a> &#8226; <a href="0309.html">Zombies! Zombies?</a> &#8226; <a href="0335.html">Decoherent Essences</a> &#8226; <a href="0346.html">Many Worlds, One Best Guess</a> &#8226; <a href="0351.html">Science Isn't Strict Enough</a> &#8226; <a href="0357.html">Einstein's Speed</a> &#8226; <a href="0363.html">Timeless Physics</a> &#8226; <a href="0372.html">Why Quantum?</a> &#8226; <a href="0401.html">The Moral Void</a> &#8226; <a href="0412.html">My Kind of Reflection</a> &#8226; <a href="0427.html">Can Counterfactuals Be True?</a> &#8226; <a href="0469.html">The True Prisoner's Dilemma</a> &#8226; <a href="0470.html">The Truly Iterated Prisoner's Dilemma</a> &#8226; <a href="0520.html">Prices or Bindings?</a> &#8226; <a href="0521.html">Ethics Notes</a> &#8226; <a href="0536.html">Today's Inspirational Tale</a> &#8226; <a href="0573.html">Is That Your True Rejection?</a> &#8226; <a href="0586.html">High Challenge</a> &#8226; <a href="0592.html">Harmful Options</a> &#8226; <a href="0601.html">Free to Optimize</a> &#8226; <a href="0679.html">What Do We Mean By "Rationality"?</a> &#8226; <a href="0682.html">Why Our Kind Can't Cooperate</a> &#8226; <a href="0689.html">Your Price for Joining</a> &#8226; <a href="0698.html">Rationality is Systematized Winning</a> &#8226; <a href="0701.html">Newcomb's Problem standard positions</a> &#8226; <a href="0714.html">Bayesians vs. Barbarians</a> &#8226; <a href="0719.html">The Sin of Underconfidence</a> &#8226; <a href="0746.html">Timeless Decision Theory: Problems I Can't Solve</a> &#8226; <a href="0752.html">Ingredients of Timeless Decision Theory</a> &#8226; <a href="0762.html">The Lifespan Dilemma</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/">Newcomb's Problem and Regret of Rationality</a></p></body></html>