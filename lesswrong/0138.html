<html><head><title>Expecting Short Inferential Distances</title></head><body><h1>Expecting Short Inferential Distances</h1><p><i>Eliezer Yudkowsky, 22 October 2007 11:42PM</i></p><div><p><em>Homo sapiens'</em> <a href="http://en.wikipedia.org/wiki/Evolutionary_psychology#The_environment_of_evolutionary_adaptedness">environment of evolutionary adaptedness</a> [http://en.wikipedia.org/wiki/Evolutionary_psychology#The_environment_of_evolutionary_adaptedness] (aka EEA or "ancestral environment") consisted of hunter-gatherer <a href="http://en.wikipedia.org/wiki/Band_society">bands</a> [http://en.wikipedia.org/wiki/Band_society] of at most <a href="http://en.wikipedia.org/wiki/Dunbar%27s_number">200 people</a> [http://en.wikipedia.org/wiki/Dunbar%27s_number], with no writing.  All inherited knowledge was passed down by speech and memory.</p> <p>In a world like that, all background knowledge is universal knowledge.  All information not strictly private is public, period.</p> <p>In the ancestral environment, you were unlikely to end up more than <em>one inferential step</em> away from anyone else.  When you discover a new oasis, you don't have to explain to your fellow tribe members what an oasis is, or why it's a good idea to drink water, or how to walk.  Only you know where the oasis lies; this is private knowledge.  But everyone has the background to understand your description of the oasis, the concepts needed to think about water; this is universal knowledge.  When you explain things in an ancestral environment, you almost <em>never</em> have to explain your concepts.  At most you have to explain <em>one</em> new concept, not two or more simultaneously.</p> <a id="more"></a><p>In the ancestral environment there were no abstract disciplines with vast bodies of carefully gathered evidence generalized into elegant theories transmitted by written books whose conclusions are <em>a hundred inferential steps removed</em> from universally shared background premises.</p> <p>In the ancestral environment, anyone who says something with no obvious support, is a liar or an idiot.  You're not likely to think, "Hey, maybe this guy has well-supported background knowledge that no one in my band has even heard of," because it was a reliable invariant of the ancestral environment that this didn't happen. </p> <p>Conversely, if you say something blatantly obvious and the other person doesn't see it, <em>they're</em> the idiot, or they're being deliberately obstinate to annoy you.</p> <p>And to top it off, if someone says something with no obvious support and <em>expects</em> you to believe it - acting all indignant when you don't - then they must be <em>crazy.</em></p> <p>Combined with <a href="0136.html">the illusion of transparency</a> [http://lesswrong.com/lw/ke/illusion_of_transparency_why_no_one_understands/] and <a href="0137.html">self-anchoring</a> [http://lesswrong.com/lw/kf/selfanchoring/], I think this explains a <em>lot</em> about the legendary difficulty most scientists have in communicating with a lay audience - or even communicating with scientists from other disciplines.  When I observe failures of explanation, I usually see the explainer taking <em>one</em> step back, when they need to take two or more steps back.  Or listeners, assuming that things should be visible in one step, when they take two or more steps to explain.  Both sides act as if they expect very short inferential distances from universal knowledge to any new knowledge.</p> <p>A biologist, speaking to a physicist, can justify evolution by saying it is "the simplest explanation".  But not everyone on Earth has been inculcated with that legendary history of science, from Newton to Einstein, which invests the phrase "simplest explanation" with its awesome import: a Word of Power, spoken at the birth of theories and carved on their tombstones.  To someone else, "But it's the simplest explanation!" may sound like an interesting but hardly knockdown argument; it doesn't feel like all that powerful a tool for comprehending office politics or fixing a broken car.  Obviously the biologist is infatuated with his own ideas, too arrogant to be open to alternative explanations which sound just as plausible.  (If it sounds plausible to me, it should sound plausible to any sane member of my band.)</p> <p>And from the biologist's perspective, he can understand how evolution might sound a little odd at first - but when someone rejects evolution even after the biologist explains that it's the simplest explanation, well, it's clear that nonscientists are just idiots and there's no point in talking to them.</p> <p>A clear argument has to lay out an inferential <em>pathway,</em> starting from what the audience <em>already knows or accepts.</em>  If you don't recurse far enough, you're just talking to yourself.</p> <p>If at any point you make a statement without obvious justification in arguments you've previously supported, the audience just thinks you're a cult victim.</p> <p>This also happens when you allow yourself to be seen <em>visibly</em> attaching greater weight to an argument than is justified in the eyes of the audience <em>at that time.</em>  For example, talking as if you think "simpler explanation" is a knockdown argument for evolution (which it is), rather than a sorta-interesting idea (which it sounds like to someone who hasn't been raised to revere Occam's Razor).</p> <p>Oh, and you'd better not drop any hints that <em>you</em> think you're working a dozen inferential steps away from what the audience knows, or that <em>you</em> think you have special background knowledge not available to them.  The audience doesn't know anything about an evolutionary-psychological argument for a cognitive bias to underestimate inferential distances leading to traffic jams in communication.  They'll just think you're condescending.</p> <p>And if you think you can explain the concept of "systematically underestimated inferential distances" briefly, in just a few words, I've got some sad news for you...</p></div> <hr><p><i>Referenced by: </i><a href="0139.html">Explainers Shoot High. Aim Low!</a> &#8226; <a href="0140.html">Double Illusion of Transparency</a> &#8226; <a href="0141.html">No One Knows What Science Doesn't Know</a> &#8226; <a href="0183.html">Fake Fake Utility Functions</a> &#8226; <a href="0184.html">Fake Utility Functions</a> &#8226; <a href="0207.html">Cultish Countercultishness</a> &#8226; <a href="0208.html">My Strange Beliefs</a> &#8226; <a href="0217.html">Absolute Authority</a> &#8226; <a href="0229.html">Zut Allais!</a> &#8226; <a href="0273.html">Searching for Bayes-Structure</a> &#8226; <a href="0324.html">Identity Isn't In Specific Atoms</a> &#8226; <a href="0359.html">My Childhood Role Model</a> &#8226; <a href="0368.html">A Premature Word on AI</a> &#8226; <a href="0371.html">Timeless Identity</a> &#8226; <a href="0372.html">Why Quantum?</a> &#8226; <a href="0440.html">Contaminated by Optimism</a> &#8226; <a href="0445.html">Moral Error and Moral Disagreement</a> &#8226; <a href="0446.html">Abstracted Idealized Dynamics</a> &#8226; <a href="0454.html">You Provably Can't Trust Yourself</a> &#8226; <a href="0573.html">Is That Your True Rejection?</a> &#8226; <a href="0707.html">Beware of Other-Optimizing</a> &#8226; <a href="0724.html">Practical Advice Backed By Deep Theories</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/kg/expecting_short_inferential_distances/">Expecting Short Inferential Distances</a></p></body></html>