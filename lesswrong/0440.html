<html><head><title>Contaminated by Optimism</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Contaminated by Optimism</h1><p><i>Eliezer Yudkowsky, 06 August 2008 12:26AM</i></p><div><p><strong>Followup to</strong>: <a href="0439.html">Anthropomorphic Optimism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/], <a href="0171.html">The Hidden Complexity of Wishes </a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/]</p> <p><a href="0439.html">Yesterday</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/], I reprised in further detail <a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/], in which early biologists believed that predators would voluntarily restrain their breeding to avoid exhausting the prey population; the given excuse was "group selection".  Not only does it turn out to be nearly impossible for group selection to overcome a countervailing individual advantage; but when these nigh-impossible conditions were <em>created in the laboratory</em> - group selection for low-population groups - the actual result was not restraint in breeding, but, of course, cannibalism, especially of immature females.</p> <p><a href="0084.html">I've made even sillier mistakes</a> [http://lesswrong.com/lw/iy/my_wild_and_reckless_youth/], by the way - though about AI, not evolutionary biology.  And the thing that strikes me, looking over these cases of anthropomorphism, is the extent to which you are screwed as soon as you let anthropomorphism <em>suggest ideas to examine</em>.</p> <p><a href="0110.html">In large hypothesis spaces, the vast majority of the cognitive labor goes into <em>noticing</em> the true hypothesis</a> [http://lesswrong.com/lw/jo/einsteins_arrogance/].  By the time you have enough evidence to consider the correct theory as one of just a few plausible alternatives - to represent the correct theory in your mind - you're practically <em>done</em>.  Of this I have spoken <a href="0356.html">several times before</a> [http://lesswrong.com/lw/qi/faster_than_science/].</p> <p>And by the same token, my experience suggests that as soon as you let anthropomorphism <em>promote a hypothesis to your attention,</em> so that you start wondering if <em>that particular hypothesis</em> might be true, you've already committed most of the mistake.</p> <p><a id="more"></a></p> <p>The group selectionists did not <em>deliberately</em> extend credit to the belief that evolution would do the aesthetic thing, the nice thing.  The group selectionists were doomed when they let their aesthetic sense make a <em>suggestion</em> - when they let it <a href="0110.html">promote a hypothesis to the level of deliberate consideration</a> [http://lesswrong.com/lw/jo/einsteins_arrogance/].</p> <p>It's not like I knew the original group selectionists.  But I've made analogous mistakes as a teenager, and then watched others make the mistake many times over.  So I do have some experience whereof I speak, when I speak of instant doom.</p> <p>Unfortunately, the prophylactic against this mistake, is <a href="0351.html">not a recognized technique of Traditional Rationality</a> [http://lesswrong.com/lw/qd/science_isnt_strict_enough/].</p> <p>In Traditional Rationality, you can get your ideas from anywhere.  Then you weigh up the evidence for and against them, searching for arguments on both sides.  If the question hasn't been definitely settled by experiment, you should try to do an experiment to test your opinion, and dutifully accept the result.</p> <p>"Sorry, you're not allowed to suggest ideas using that method" is not something you hear, under Traditional Rationality.</p> <p>But it is a fact of life, an experimental result of cognitive psychology, that when people have an idea <em>from any source</em>, they tend to <a href="0082.html">search for support rather than contradiction</a> [http://lesswrong.com/lw/iw/positive_bias_look_into_the_dark/] - even in the absence of emotional commitment (see link).</p> <p>It is a fact of life that <a href="0125.html">priming and contamination</a> [http://lesswrong.com/lw/k3/priming_and_contamination/] occur: just being briefly exposed to completely uninformative, known false, or totally irrelevant "information" can exert significant influence on subjects' estimates and decisions.  This happens on a level below deliberate awareness, and that's going to be pretty hard to beat on problems where anthropomorphism is bound to rush in and make suggestions - but at least you can avoid deliberately making it worse.</p> <p>It is a fact of life that <a href="0119.html">we change our minds less often than we think</a> [http://lesswrong.com/lw/jx/we_change_our_minds_less_often_than_we_think/].  Once an idea gets into our heads, it is harder to get it out than we think.  Only an extremely restrictive chain of reasoning, that <em>definitely prohibited</em> most possibilities from consideration, would be sufficient to undo this damage - to root an idea out of your head once it lodges.  The less you know for sure, the easier it is to become contaminated - weak domain knowledge increases contamination effects.</p> <p>It is a fact of life that <a href="0144.html">we are far more likely to stop searching for further alternatives</a> [http://lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/] at a point when we have a conclusion we like, than when we have a conclusion we dislike.</p> <p>It is a fact of life that <a href="0028.html">we hold ideas we would like to believe, to a lower standard of proof than ideas we would like to disbelieve</a> [http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/].  In the former case we ask "Am I allowed to believe it?" and in the latter case ask "Am I forced to believe it?"  If your domain knowledge is weak, you will not know enough for your own knowledge to grab you by the throat and tell you "You're wrong!  That can't possibly be true!"  You will find that you are allowed to believe it.  You will search for plausible-sounding scenarios where your belief is true.  If the search space of possibilities is large, you will almost certainly find some "winners" - your domain knowledge being too weak to definitely prohibit those scenarios.</p> <p>It is a fact of history that the group selectionists failed to relinquish their folly.  They found what they thought was a <em>perfectly plausible</em> way that evolution (evolution!) could end up producing foxes who voluntarily avoided reproductive opportunities(!).  And the group selectionists did in fact cling to that hypothesis.  <em>That's what happens in real life!  Be warned!</em></p> <p>To beat anthropomorphism you have to be scared of letting anthropomorphism make <em>suggestions.</em>  You have to try to avoid being <em>contaminated</em> by anthropomorphism (to the best extent you can).</p> <p>As soon as you let anthropomorphism generate the idea and ask, "Could it be true?" then your brain has already swapped out of forward-extrapolation mode and into <a href="0116.html">backward-rationalization mode</a> [http://lesswrong.com/lw/ju/rationalization/].  Traditional Rationality contains inadequate warnings against this, IMO.  See in particular the post where I argue against the Traditional interpretation of <a href="0377.html">Devil's Advocacy</a> [http://lesswrong.com/lw/r3/against_devils_advocacy/].</p> <p>Yes, there are occasions when you want to perform abductive inference, such as when you have evidence that something is true and you are asking how it could be true.  We call that "Bayesian updating", in fact.  An occasion where you <em>don't</em> have any evidence but your brain has made a cute little anthropomorphic suggestion, is <em>not</em> a time to start wondering how it could be true.  Especially if the search space of possibilities is large, and your domain knowledge is too weak to prohibit plausible-sounding scenarios.  Then your prediction ends up being determined by anthropomorphism.  If the real process is not controlled by a brain similar to yours, this is not a good thing for your predictive accuracy.</p> <p>This is a war I wage primarily on the battleground of Unfriendly AI, but it seems to me that many of the conclusions apply to optimism in general.</p> <p>How did the <em>idea first come to you,</em> that the subprime meltdown wouldn't decrease the value of your investment in Danish deuterium derivatives?  Were you just thinking neutrally about the course of financial events, trying to extrapolate some of the many different ways that one financial billiard ball could ricochet off another?  Even <a href="0102.html">this method tends to be subject to optimism</a> [http://lesswrong.com/lw/jg/planning_fallacy/]; if we know which way we want each step to go, we tend to visualize it going that way.  But better that, than starting with a pure hope - an outcome generated because it ranked high in your preference ordering - and then permitting your mind to invent plausible-sounding reasons it might happen.  This is just rushing to failure.</p> <p>And to spell out the application to Unfriendly AI:  You've got various people insisting that an arbitrary mind, including an expected paperclip maximizer, would do various nice things or obey various comforting conditions:  "Keep humans around, because diversity is important to creativity, and the humans will provide a different point of view."  Now you might want to seriously ask if, <em>even granting that premise,</em> you'd be kept in a nice house with air conditioning; or kept in a tiny cell with life support tubes and regular electric shocks if you didn't generate enough interesting ideas that day (and of course you wouldn't be allowed to die); or uploaded to a very small computer somewhere, and restarted every couple of years.  No, let me guess, you'll be more productive if you're happy.  So it's clear why <em>you</em> want that to be the argument; but unlike you, the paperclip maximizer is not frantically searching for a reason not to torture you.</p> <p>Sorry, the whole scenario is still around as unlikely as your carefully picking up ants on the sidewalk, rather than stepping on them, and keeping them in a happy ant colony <em>for the sole express purpose of suggesting blog comments.</em>  There are reasons in <em>my</em> goal system to keep sentient beings alive, even if they aren't "useful" at the moment.  But from the perspective of a Bayesian superintelligence whose only <a href="0162.html">terminal value</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/] is paperclips, it is not an optimal use of matter and energy toward the instrumental value of producing diverse and creative ideas for making paperclips, to keep around six billion highly similar human brains.  <em>Unlike you, the paperclip maximizer doesn't start out knowing it wants that to be the conclusion.</em></p> <p>Your brain starts out knowing that it wants humanity to live, and so it starts trying to come up with arguments for why that is a perfectly reasonable thing for a paperclip maximizer to do.  But <em>the paperclip maximizer itself</em> would not start from the conclusion that it wanted humanity to live, and reason backward.  It would just try to make paperclips.  It wouldn't <a href="0144.html">stop</a> [http://lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/], the way your own mind tends to stop, if it did find one argument for keeping humans alive; instead it would go on searching for an even superior alternative, some way to use the same resources to greater effect.  Maybe you just want to keep 20 humans and randomly perturb their brain states a lot.</p> <p>If you can't blind your eyes to human goals and just think about the paperclips, you can't understand what the goal of making paperclips implies.  It's like expecting kind and merciful results from natural selection, which lets old elephants starve to death when they run out of teeth.</p> <p>A priori, if you want a nice result that takes 10 bits to specify, then a priori you should expect a 1/1024 probability of finding that some <em>unrelated</em> process generates that nice result.  And a genuinely nice outcome in a large outcome space takes <a href="0171.html">a lot more information</a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/] than the English word "nice", because what we consider a good outcome has <a href="0161.html">many components of value</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/].  It's extremely suspicious if you start out with a nice result in mind, search for a plausible reason that a not-inherently-nice process would generate it, and, by golly, find an amazing clever argument.</p> <p>And the more complexity you add to your requirements - humans not only have to survive, but have to survive under what we would consider good living conditions, etc. - the less you should expect, a priori, a non-nice process to generate it.  The less you should expect to, amazingly, find a genuine valid reason why the non-nice process happens to do what you want.  And the more suspicious you should be, if you find a clever-sounding argument why this should be the case.  To expect this to happen with non-trivial probability is <a href="0272.html">pulling information from nowhere</a> [http://lesswrong.com/lw/o6/perpetual_motion_beliefs/]; a blind arrow is hitting the center of a small target.  Are you sure it's wise to even search for such possibilities?  Your chance of deceiving yourself is far greater than the <em>a priori</em> chance of a good outcome, especially if your domain knowledge is too weak to definitely rule out possibilities.</p> <p>No more than you can <a href="0272.html">guess a lottery ticket</a> [http://lesswrong.com/lw/o6/perpetual_motion_beliefs/], should you expect a process not shaped by human niceness, to produce nice results in a large outcome space.  You may not know the domain very well, but you can understand that, a priori, "nice" results require specific complexity to happen for no reason, and complex specific miracles are rare.</p> <p>I wish I could tell people:  "Stop!  Stop right there!  You defeated yourself the moment you knew what you <em>wanted!</em>  You need to throw away your thoughts and start over with a neutral forward extrapolation, not seeking any particular outcome."  But the <a href="0138.html">inferential distance</a> [http://lesswrong.com/lw/kg/expecting_short_inferential_distances/] is too great; and then begins the slog of, "I don't see why that couldn't happen" and "I don't think you've proven my idea is wrong."</p> <p>It's Unfriendly superintelligence that tends to worry me most, of course.  But I do think the point generalizes to quite a lot of optimism.  You may know what you want, but Nature doesn't care.</p></div> <hr><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/su/contaminated_by_optimism/">Contaminated by Optimism</a></p></body></html>