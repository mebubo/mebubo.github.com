<html><head><title>Evaporative Cooling of Group Beliefs</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Evaporative Cooling of Group Beliefs</h1><p><i>Eliezer Yudkowsky, 07 December 2007 11:08PM</i></p><div><p><strong>Followup to</strong>:  <a href="0182.html">Uncritical Supercriticality</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/]</p> <p>Early studiers of cults were surprised to discover than when cults receive a major shock&#8212;a prophecy fails to come true, a moral flaw of the founder is revealed&#8212;they often come back stronger than before, with increased belief and fanaticism.  The Jehovah's Witnesses placed Armageddon in 1975, based on Biblical calculations; 1975 has come and passed.  The Unarian cult, still going strong today, survived the <a href="http://findarticles.com/p/articles/mi_m0SOR/is_n2_v59/ai_20913876/pg_3">nonappearance of an intergalactic spacefleet</a> [http://findarticles.com/p/articles/mi_m0SOR/is_n2_v59/ai_20913876/pg_3] on September 27, 1975.  (The <a href="http://en.wikipedia.org/wiki/Unarius_Academy_of_Science">Wikipedia article</a> [http://en.wikipedia.org/wiki/Unarius_Academy_of_Science] on Unarianism mentions a failed prophecy in 2001, but makes no mention of the earlier failure in 1975, interestingly enough.)</p> <p>Why would a group belief become <em>stronger</em> after encountering crushing counterevidence?</p> <p><a id="more"></a></p> <p>The conventional interpretation of this phenomenon is based on cognitive dissonance.  When people have taken "irrevocable" actions in the service of a belief&#8212;given away all their property in anticipation of the saucers landing&#8212;they cannot possibly admit they were mistaken. The challenge to their belief presents an immense cognitive dissonance; they must find reinforcing thoughts to counter the shock, and so become more fanatical.  In this interpretation, the increased group fanaticism is the result of increased individual fanaticism.</p> <p>I was looking at a Java applet which demonstrates <a href="http://www.colorado.edu/physics/2000/bec/evap_cool.html">the use of evaporative cooling to form a Bose-Einstein condensate</a> [http://www.colorado.edu/physics/2000/bec/evap_cool.html], when it occurred to me that another force entirely might operate to increase fanaticism.  Evaporative cooling sets up a potential energy barrier around a collection of hot atoms.  Thermal energy is essentially statistical in nature&#8212;not all atoms are moving at the exact same speed.  The kinetic energy of any given atom varies as the atoms collide with each other.  If you set up a potential energy barrier that's just a little higher than the average thermal energy, the workings of chance will give an occasional atom a kinetic energy high enough to escape the trap.  When an unusually fast atom escapes, it takes with an unusually large amount of kinetic energy, and the average energy decreases.  The group becomes substantially cooler than the potential energy barrier around it.  <a href="http://www.colorado.edu/physics/2000/bec/evap_cool.html">Playing with the Java applet</a> [http://www.colorado.edu/physics/2000/bec/evap_cool.html] may make this clearer.</p> <p>In Festinger's classic "When Prophecy Fails", one of the cult members walked out the door immediately after the flying saucer failed to land.  Who gets fed up and leaves <em>first?</em>  An <em>average</em> cult member?  Or a relatively more skeptical member, who previously might have been acting as a voice of moderation, a brake on the more fanatic members?</p> <p>After the members with the highest kinetic energy escape, the remaining discussions will be between the extreme fanatics on one end and the slightly less extreme fanatics on the other end, with the group consensus somewhere in the "middle".</p> <p>And what would be the analogy to collapsing to form a Bose-Einstein condensate?  Well, there's no real need to stretch the analogy that far.  But you may recall that I used a fission chain reaction analogy for the affective death spiral; when a group ejects all its voices of moderation, then all the people encouraging each other, and suppressing dissents, may internally increase in average fanaticism.  (No thermodynamic analogy here, unless someone develops a nuclear weapon that explodes when it gets cold.)</p> <p>When Ayn Rand's long-running affair with Nathaniel Branden was revealed to the Objectivist membership, a substantial fraction of the Objectivist membership broke off and followed Branden into espousing an "open system" of Objectivism not bound so tightly to Ayn Rand.  Who stayed with Ayn Rand even after the scandal broke?  The ones who <em>really, really</em> believed in her&#8212;and perhaps some of the undecideds, who, after the voices of moderation left, heard arguments from only one side.  This may account for how the Ayn Rand Institute is (reportedly) more fanatic after the breakup, than the original core group of Objectivists under Branden and Rand.</p> <p>A few years back, I was on a transhumanist mailing list where a small group espousing "social democratic transhumanism" vitriolically insulted every libertarian on the list.  Most libertarians left the mailing list, most of the others gave up on posting.  As a result, the remaining group shifted substantially to the left.  Was this deliberate?  Probably not, because I don't think the perpetrators knew that much psychology.  (For that matter, I can't recall seeing the evaporative cooling analogy elsewhere, though that doesn't mean it hasn't been noted before.)  At most, they might have thought to make themselves "bigger fish in a smaller pond".</p> <p>This is one reason why it's important to be prejudiced in favor of tolerating dissent.  Wait until substantially <em>after</em> it seems to you justified in ejecting a member from the group, before actually ejecting.  If you get rid of the old outliers, the group position will shift, and someone else will become the oddball.  If you eject them too, you're well on the way to becoming a Bose-Einstein condensate and, er, exploding.</p> <p>The flip side:  Thomas Kuhn believed that a science has to become a "paradigm", with a shared technical language that excludes outsiders, before it can get any real work done. In the formative stages of a science, according to Kuhn, the adherents go to great pains to make their work comprehensible to outside academics.  But (according to Kuhn) a science can only make real progress as a technical discipline once it abandons the requirement of outside accessibility, and scientists working in the paradigm assume familiarity with large cores of technical material in their communications.  This sounds cynical, relative to what is usually <a href="0097.html">said</a> [http://lesswrong.com/lw/jb/applause_lights/] about public understanding of science, but I can definitely see a core of truth here.</p> <p>My own theory of Internet moderation is that you have to be willing to exclude trolls and spam to get a conversation going.  You must even be willing to exclude kindly but technically uninformed folks from technical mailing lists if you want to get any work done.  A genuinely open conversation on the Internet degenerates fast.  It's the <em>articulate</em> trolls that you should be wary of ejecting, on this theory&#8212;they serve the hidden function of legitimizing less extreme disagreements.  But you should not have so many articulate trolls that they begin arguing with each other, or begin to dominate conversations.  If you have one person around who is the famous Guy Who Disagrees With Everything, anyone with a more reasonable, more moderate disagreement won't look like the sole nail sticking out.  This theory of Internet moderation may not have served me too well in practice, so take it with a grain of salt.</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor"><em>Death Spirals and the Cult Attractor</em></a> [http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0186.html">When None Dare Urge Restraint</a> [http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/]"</p> <p style="text-align:right">Previous post: "<a href="0182.html">Uncritical Supercriticality</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq04.html">Sequence 04: Death Spirals and the Cult Attractor</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0182.html">Uncritical Supercriticality</a></p></td><td><p><i>Next: </i><a href="0186.html">When None Dare Urge Restraint</a></p></td></tr></table><p><i>Referenced by: </i><a href="0182.html">Uncritical Supercriticality</a> &#8226; <a href="0186.html">When None Dare Urge Restraint</a> &#8226; <a href="0195.html">Guardians of Ayn Rand</a> &#8226; <a href="0207.html">Cultish Countercultishness</a> &#8226; <a href="0682.html">Why Our Kind Can't Cooperate</a> &#8226; <a href="0758.html">Why I'm Staying On Bloggingheads.tv</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/">Evaporative Cooling of Group Beliefs</a></p></body></html>