<html><head><title>Inseparably Right; or, Joy in the Merely Good</title></head><body><h1>Inseparably Right; or, Joy in the Merely Good</h1><p><i>Eliezer Yudkowsky, 09 August 2008 01:00AM</i></p><div><p><strong>Followup to</strong>:  <a href="0432.html">The Meaning of Right</a> [http://lesswrong.com/lw/sm/the_meaning_of_right/]</p> <p>I fear that in my drive for full explanation, I may have obscured the punchline from <a href="0432.html">my theory of metaethics</a> [http://lesswrong.com/lw/sm/the_meaning_of_right/].  Here then is an attempted rephrase:</p> <p>There is no pure ghostly essence of goodness apart from things like truth, happiness and sentient life.</p> <p>What do you value?  At a guess, you value the life of your friends and your family and your Significant Other and yourself, all in different ways.  You would probably say that you value human life in general, and I would <a href="0021.html">take your word for it</a> [http://lesswrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/], though Robin Hanson might ask how you've acted on this supposed preference.  If you're reading this blog you probably attach some value to <a href="0241.html">truth for the sake of truth</a> [http://lesswrong.com/lw/nb/something_to_protect/].  If you've ever learned to play a musical instrument, or paint a picture, or if you've ever solved a math problem for the fun of it, then you probably attach real value to good art.  You value your freedom, the control that you possess over your own life; and if you've ever really helped someone you probably enjoyed it.  You might not think of playing a video game as a great sacrifice of dutiful morality, but I for one would not wish to see the joy of complex challenge perish from the universe.  You may not think of telling jokes as a matter of <a href="0433.html">interpersonal morality</a> [http://lesswrong.com/lw/sn/interpersonal_morality/], but I would consider the human sense of humor as part of <a href="0420.html">the gift we give to tomorrow</a> [http://lesswrong.com/lw/sa/the_gift_we_give_to_tomorrow/].</p> <p>And you value <a href="0161.html">many more things</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/] than these.</p> <p>Your brain assesses these things I have said, or others, or more, depending on the specific event, and finally affixes a little internal representational label that we recognize and call "good".</p> <p>There's no way you can detach the little label from what it stands for, and still make ontological or moral sense.</p> <p><a id="more"></a></p> <p>Why might the little 'good' label <em>seem</em> detachable?  <a href="0432.html">A number of reasons</a> [http://lesswrong.com/lw/sm/the_meaning_of_right/].</p> <p>Mainly, that's just how your mind is structured&#8212;the labels it attaches internally seem like <a href="0284.html">extra, floating, ontological properties</a> [http://lesswrong.com/lw/oi/mind_projection_fallacy/].</p> <p>And there's no <em>one</em> value that determines whether a complicated event is good or not&#8212;and no five values, either.  No matter what rule you try to describe, there's always something left over, some counterexample.  <a href="0386.html">Since no single value defines goodness, this can make it seem like all of them together couldn't define goodness</a> [http://lesswrong.com/lw/rc/the_ultimate_source/].  But when you add them up all together, there is nothing else left.</p> <p>If there's no detachable property of goodness, what does this mean?</p> <p>It means that the question, "Okay, but what makes happiness or self-determination, <em>good?</em>" is either very quickly answered, or else malformed.</p> <p>The concept of a "utility function" or "optimization criterion" is detachable when talking about optimization processes.  Natural selection, for example, optimizes for inclusive genetic fitness.  But there are <a href="0396.html">possible minds that implement any utility function</a> [http://lesswrong.com/lw/rm/the_design_space_of_mindsingeneral/], so you don't get any advice there about what you <em>should</em> do.  You can't ask about utility apart from any utility function.</p> <p>When you ask "But which utility function <em>should</em> I use?" the word <em>should</em> is something inseparable from the dynamic that labels a choice "should"&#8212;inseparable from the reasons like "Because I can save more lives that way."</p> <p>Every time you say <em>should,</em> it includes an implicit criterion of choice; there is no should-ness that can be abstracted away from any criterion.</p> <p>There is no separable right-ness that you could abstract from pulling a child off the train tracks, and attach to some other act.</p> <p>Your values can <a href="0419.html">change in response to arguments</a> [http://lesswrong.com/lw/s9/whither_moral_progress/]; you have metamorals as well as morals.  So it probably does make sense to think of an idealized good, or idealized right, that you would assign if you could think of all possible arguments.  Arguments may even convince you to change your criteria of what counts as a persuasive argument.  Even so, when you consider the total trajectory arising out of that <em>entire framework,</em> that <em>moral frame of reference,</em> there is no separable property of justification-ness, apart from any particular criterion of justification; no final answer apart from a starting question.</p> <p>I sometimes say that morality is "<a href="0402.html">created already in motion</a> [http://lesswrong.com/lw/rs/created_already_in_motion/]".</p> <p>There is no perfect argument that persuades the ideal philosopher of perfect emptiness to attach a perfectly abstract label of 'good'.  The notion of the perfectly abstract label is incoherent, which is why people chase it round and round in circles.  What would distinguish a perfectly empty label of 'good' from a perfectly empty label of 'bad'?  How would you tell which was which?</p> <p>But since every supposed criterion of goodness that we describe, turns out to be wrong, or incomplete, or changes the next time we hear a moral argument, it's easy to see why someone might think that 'goodness' was a thing apart from any criterion at all.</p> <p>Humans have a cognitive architecture that easily misleads us into conceiving of goodness as something that can be detached from any criterion.</p> <p>This conception turns out to be incoherent.  Very sad.  I too was hoping for a perfectly abstract argument; it appealed to my <a href="0433.html">universalizing</a> [http://lesswrong.com/lw/sn/interpersonal_morality/] instinct.  But...</p> <p>But the question then becomes: is that little fillip of human psychology, more important than everything else?  Is it more important than the happiness of your family, your friends, your mate, your extended tribe, and yourself?  If your universalizing instinct is frustrated, is that worth abandoning life?  If you represented rightness wrongly, do pictures stop being beautiful and maths stop being elegant?  Is that one tiny mistake worth forsaking <a href="0420.html">the gift we could give to tomorrow</a> [http://lesswrong.com/lw/sa/the_gift_we_give_to_tomorrow/]?  Is it even really worth all that much in the way of existential angst?</p> <p>Or will you just say "Oops" and go back to life, to truth, fun, art, freedom, challenge, humor, moral arguments, and all those other things that in their sum and in their reflective trajectory, are the entire and only meaning of the word 'right'?</p> <p>Here is the strange habit of thought I mean to convey:  Don't look to some <a href="http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/">surprising</a> [http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/] <a href="0373.html">unusual</a> [http://lesswrong.com/lw/qz/living_in_many_worlds/] twist of logic for your justification.  Look to the living child, successfully dragged off the train tracks.  There you will find your justification.  What ever should be more important than that?</p> <p>I could dress that up in <a href="0442.html">computational metaethics and FAI theory</a> [http://lesswrong.com/lw/sw/morality_as_fixed_computation/]&#8212;which indeed is whence the notion first came to me&#8212;but when I translated it all back into human-talk, that is what it turned out to say.</p> <p>If we cannot take joy in things that are merely good, our lives shall be empty indeed.</p> <p> </p> <p style="text-align:right">Part of <a href="http://wiki.lesswrong.com/wiki/Metaethics_sequence"><em>The Metaethics Sequence</em></a> [http://wiki.lesswrong.com/wiki/Metaethics_sequence]</p> <p style="text-align:right">Next post: "<a href="0444.html">Sorting Pebbles Into Correct Heaps</a> [http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/]"</p> <p style="text-align:right">Previous post: "<a href="0442.html">Morality as Fixed Computation</a> [http://lesswrong.com/lw/sw/morality_as_fixed_computation/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq14.html">Sequence 14: Metaethics</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0442.html">Morality as Fixed Computation</a></p></td><td><p><i>Next: </i><a href="0444.html">Sorting Pebbles Into Correct Heaps</a></p></td></tr></table><p><i>Referenced by: </i><a href="0442.html">Morality as Fixed Computation</a> &#8226; <a href="0444.html">Sorting Pebbles Into Correct Heaps</a> &#8226; <a href="0445.html">Moral Error and Moral Disagreement</a> &#8226; <a href="0447.html">"Arbitrary"</a> &#8226; <a href="0449.html">The Bedrock of Morality: Arbitrary?</a> &#8226; <a href="0566.html">Singletons Rule OK</a> &#8226; <a href="0585.html">Prolegomena to a Theory of Fun</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0629.html">Value is Fragile</a> &#8226; <a href="0641.html">(Moral) Truth in Fiction?</a> &#8226; <a href="0674.html">Raising the Sanity Waterline</a> &#8226; <a href="0749.html">Unspeakable Morality</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/">Inseparably Right; or, Joy in the Merely Good</a></p></body></html>