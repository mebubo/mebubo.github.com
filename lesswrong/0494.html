<html><head><title>Above-Average AI Scientists</title></head><body><h1>Above-Average AI Scientists</h1><p><i>Eliezer Yudkowsky, 28 September 2008 11:04AM</i></p><div><p><strong>Followup to</strong>:  <a href="0492.html">The Level Above Mine</a> [http://lesswrong.com/lw/ua/the_level_above_mine/], <a href="0493.html">Competent Elites</a> [http://lesswrong.com/lw/ub/competent_elites/]</p> <p>(Those who didn't like the last two posts should <em>definitely</em> skip this one.)</p> <p>I recall one fellow, who seemed like a nice person, and who was quite eager to get started on Friendly AI work, to whom I had trouble explaining that he didn't have a hope.  He said to me:</p> <blockquote> <p>"If someone with a Masters in chemistry isn't intelligent enough, then you're not going to have much luck finding someone to help you."</p> </blockquote> <p>It's hard to distinguish the grades above your own.  And even if you're literally the best in the world, there are still electron orbitals above yours&#8212;they're just unoccupied.  Someone had to be "the best physicist in the world" during the time of Ancient Greece.  Would they have been able to visualize Newton?</p> <p>At one of the first conferences organized around the tiny little subfield of Artificial General Intelligence, I met someone who was heading up a funded research project specifically declaring AGI as a goal, within a major corporation.  I believe he had people under him on his project.  He was probably paid at least three times as much as I was paid (at that time).  His academic credentials were superior to mine (what a surprise) and he had many more years of experience.  He had access to lots and lots of computing power.</p> <p>And like nearly everyone in the field of AGI, he was rushing forward to write code immediately&#8212;not holding off and searching for a sufficiently precise theory to permit stable self-improvement.</p> <p>In short, he was just the sort of fellow that...  Well, many people, when they hear about Friendly AI, say:  "Oh, it doesn't matter what you do, because [someone like this guy] will create AI first."  He's the sort of person about whom journalists ask me, "You say that this isn't the time to be talking about regulation, but don't we need laws to stop people like this from creating AI?"</p> <p><a id="more"></a></p> <p>"I suppose," you say, your voice heavy with irony, "that you're about to tell us, that this person doesn't really have so much of an advantage over you as it might seem.  Because your theory&#8212;whenever you actually come up with a theory&#8212;is going to be so much better than his.  Or," your voice becoming even more ironic, "that he's too mired in boring <em>mainstream</em> methodology&#8212;"</p> <p>No.  I'm about to tell you that I happened to be seated at the same table as this guy at lunch, and I made some kind of comment about evolutionary psychology, and he turned out to be...</p> <p>...a creationist.</p> <p>This was the point at which I really got, on a gut level, that there was no test you needed to pass in order to start your own AGI project.</p> <p>One of the failure modes I've come to better understand in myself since observing it in others, is what I call, "living in the should-universe".  The universe where everything works the way it common-sensically ought to, as opposed to the actual is-universe we live in.  There's more than one way to live in the should-universe, and outright delusional optimism is only the least subtle.  Treating the should-universe as your <a href="0475.html">point of departure</a> [http://lesswrong.com/lw/tt/points_of_departure/]&#8212;describing the real universe as the should-universe plus a diff&#8212;can also be dangerous.</p> <p>Up until the moment when yonder AGI researcher explained to me that he didn't believe in evolution because that's not what the Bible said, I'd been living in the should-universe.  In the sense that I was organizing my understanding of other AGI researchers as should-plus-diff.  I saw them, not as themselves, not as their probable causal histories, but as their departures from what I thought they should be.</p> <p>In the universe where everything works the way it common-sensically ought to, everything about the study of Artificial General Intelligence is driven by the one overwhelming fact of the indescribably huge effects: initial conditions and unfolding patterns whose consequences will resound for as long as causal chains continue out of Earth, until all the stars and galaxies in the night sky have burned down to cold iron, and maybe long afterward, or forever into infinity if the true laws of physics should happen to permit that.  To deliberately thrust your mortal brain onto that stage, as it plays out on ancient Earth the first root of life, is an act so far beyond "audacity" as to set the word on fire, an act which can only be excused by the terrifying knowledge that the empty skies offer no higher authority.</p> <p>It had occurred to me well before this point, that most of those who proclaimed themselves to have AGI projects, were not only failing to be what an AGI researcher should be, but in fact, didn't seem to have any such dream to live up to.</p> <p>But that was just my living in the should-universe.  It was the creationist who broke me of that.  My mind finally gave up on constructing the diff.</p> <p>When Scott Aaronson was 12 years old, he: "set myself the modest goal of writing a BASIC program that would pass the Turing Test by learning from experience and following Asimov's Three Laws of Robotics.  I coded up a really nice tokenizer and user interface, and only got stuck on the subroutine that was supposed to understand the user's question and output an intelligent, Three-Laws-obeying response."  It would be pointless to try and construct a diff between Aaronson<sub>12</sub> and what an AGI researcher should be.  You've got to explain Aaronson<sub>12</sub> in forward-extrapolation mode:  He thought it would be cool to make an AI and didn't quite understand why the problem was difficult.</p> <p>It was yonder creationist who let me see AGI researchers for themselves, and not as departures from my ideal.</p> <p>A creationist AGI researcher?  Why not?  Sure, you can't <em>really</em> be enough of an <a href="http://www.overcomingbias.com/2007/04/expert_at_versu.html">expert on</a> [http://www.overcomingbias.com/2007/04/expert_at_versu.html] thinking to build an AGI, or enough of an <a href="http://www.overcomingbias.com/2007/04/expert_at_versu.html">expert at</a> [http://www.overcomingbias.com/2007/04/expert_at_versu.html] thinking to find the truth amidst deep dark scientific chaos, while still being, in this day and age, a creationist.  But to think that his creationism is an <em>anomaly</em>, is should-universe thinking, as if desirable future outcomes could structure the present.  Most scientists have the meme that <a href="0009.html">a scientist's religion doesn't have anything to do with their research</a> [http://lesswrong.com/lw/gv/outside_the_laboratory/]. Someone who thinks that it would be cool to solve the "human-level" AI problem and create a little voice in a box that answers questions, and who <a href="0461.html">dreams they have a solution</a> [http://lesswrong.com/lw/tf/dreams_of_ai_design/], isn't going to stop and say:  "Wait!  I'm a creationist!  I guess that would make it pretty silly for me to try and build an AGI."</p> <p>The creationist is only an extreme example.  A much larger fraction of AGI wannabes would speak with reverence of the "spiritual" and the possibility of various <a href="0477.html">fundamental mentals</a> [http://lesswrong.com/lw/tv/excluding_the_supernatural/]. If someone lacks the whole cognitive edifice of reducing mental events to nonmental constituents, the edifice that decisively indicts the entire <a href="0477.html">supernatural</a> [http://lesswrong.com/lw/tv/excluding_the_supernatural/], then of course they're not likely to be expert on cognition to the degree that would be required to synthesize true AGI.  But neither are they likely to have any particular idea that they're missing something.  They're just going with the flow of the memetic water in which they swim.  They've got friends who talk about spirituality, and it sounds pretty appealing to them.  They know that Artificial General Intelligence is a big important problem in their field, worth lots of applause if they can solve it.  They wouldn't see anything incongruous about an AGI researcher talking about the possibility of <a href="0478.html">psychic powers</a> [http://lesswrong.com/lw/tw/psychic_powers/] or Buddhist reincarnation.  That's a separate matter, isn't it?</p> <p>(Someone in the audience is bound to observe that Newton was a Christian.  I reply that <a href="http://ansuz.sooke.bc.ca/bonobo-conspiracy/?i=569">Newton didn't have such a difficult problem, since he only had to invent first-year undergraduate stuff</a> [http://ansuz.sooke.bc.ca/bonobo-conspiracy/?i=569].  The two observations are around equally sensible; if you're going to be anachronistic, you should be anachronistic on both sides of the equation.)</p> <p>But that's still all just should-universe thinking.</p> <p>That's still just describing people in terms of what they aren't.</p> <p>Real people are not formed of absences.  Only people who have an ideal can be described as a departure from it, the way that I see myself as a departure from what an Eliezer Yudkowsky should be.</p> <p>The really striking fact about the researchers who show up at AGI conferences, is that they're so... I don't know how else to put it...</p> <p>...ordinary.</p> <p>Not at the intellectual level of the <a href="0493.html">big mainstream names</a> [http://lesswrong.com/lw/ub/competent_elites/] in Artificial Intelligence.  Not at the level of <a href="http://www-formal.stanford.edu/jmc/">John</a> [http://www-formal.stanford.edu/jmc/] <a href="http://en.wikiquote.org/wiki/John_McCarthy">McCarthy</a> [http://en.wikiquote.org/wiki/John_McCarthy] or <a href="http://norvig.com/">Peter Norvig</a> [http://norvig.com/] (whom I've both met).</p> <p>More like... around, say, the level of above-average scientists, which I yesterday compared to the level of partners at a non-big-name venture capital firm.  Some of whom might well be Christians, or even creationists if they don't work in evolutionary biology.</p> <p>The attendees at AGI conferences aren't <em>literally</em> average mortals, or even average scientists.  The average attendee at an AGI conference is visibly one level up from the average attendee at <a href="0493.html">that random mainstream AI conference I talked about yesterday</a> [http://lesswrong.com/lw/ub/competent_elites/].</p> <p>Of course there are exceptions.  The last AGI conference I went to, I encountered one bright young fellow who was fast, intelligent, and spoke fluent Bayesian.  Admittedly, he didn't actually <em>work</em> in AGI as such.  He worked at a hedge fund.</p> <p>No, seriously, there are exceptions.  Steve Omohundro is one example of someone who&#8212;well, I'm not exactly sure of his level, but I don't get any particular sense that he's below Peter Norvig or John McCarthy.</p> <p>But even if you just poke around on Norvig or McCarthy's website, and you've achieved sufficient level yourself to discriminate what you see, you'll get a sense of a formidable mind.  Not in terms of accomplishments&#8212;that's not a fair comparison with someone younger or tackling a more difficult problem&#8212;but just in terms of the way they talk.  If you then look at the website of a typical AGI-seeker, even one heading up their own project, you won't get an equivalent sense of formidability.</p> <p>Unfortunately, that kind of eyeball comparison does require that one be of sufficient level to distinguish those levels.  It's easy to sympathize with people who can't eyeball the difference:  If anyone with a PhD seems really bright to you, or any professor at a university is someone to respect, then you're not going to be able to eyeball the tiny academic subfield of AGI and determine that most of the inhabitants are above-average scientists for mainstream AI, but below the intellectual firepower of the top names in mainstream AI.</p> <p>But why would <em>that</em> happen?  Wouldn't the AGI people be humanity's best and brightest, answering the greatest need?  Or at least those daring souls for whom mainstream AI was not enough, who sought to challenge their wits against the greatest reservoir of chaos left to modern science?</p> <p>If you forget the should-universe, and think of the selection effect in the is-universe, it's not difficult to understand.  Today, AGI attracts people who fail to comprehend the difficulty of AGI.  Back in the earliest days, a bright mind like John McCarthy would tackle AGI because no one knew the problem was difficult.  In time and with regret, he realized he couldn't do it.  Today, someone on the level of Peter Norvig knows their own competencies, what they can do and what they can't; and they go on to achieve fame and fortune (and Research Directorship of Google) within mainstream AI.</p> <p>And then...</p> <p>Then there are the completely hopeless ordinary programmers who wander onto the AGI mailing list wanting to build a really big semantic net.</p> <p>Or the postdocs moved by some (non-Singularity) dream of themselves presenting the first "human-level" AI to the world, who also <a href="0461.html">dream an AI design</a> [http://lesswrong.com/lw/tf/dreams_of_ai_design/], and can't let go of that.</p> <p>Just normal people with no notion that it's wrong for an AGI researcher to be normal.</p> <p>Indeed, like most normal people who don't spend their lives making a desperate effort to reach up toward an impossible ideal, they will be offended if you suggest to them that someone in their position needs to be a little less imperfect.</p> <p>This misled the living daylights out of me when I was young, because I compared myself to other people who declared their intentions to build AGI, and ended up way too impressed with myself; when I should have been comparing myself to Peter Norvig, or <a href="0492.html">reaching up toward E. T. Jaynes</a> [http://lesswrong.com/lw/ua/the_level_above_mine/].  (For I did not then perceive the sheer, blank, towering wall of Nature.)</p> <p>I don't mean to bash normal AGI researchers into the ground.  They are not evil.  They are not ill-intentioned.  They are not even dangerous, as individuals.  Only the mob of them is dangerous, that can learn from each other's partial successes and accumulate hacks as a community.</p> <p>And that's why I'm discussing all this&#8212;because it is a fact without which it is not possible to understand the overall strategic situation in which humanity finds itself, the present state of the gameboard.  It is, for example, the reason why I don't panic when yet another AGI project announces they're going to have general intelligence in five years.  It also says that you can't necessarily extrapolate the FAI-theory comprehension of future researchers from present researchers, if a breakthrough occurs that repopulates the field with Norvig-class minds.</p> <p>Even an average human engineer is at least six levels higher than the <a href="0149.html">blind idiot god</a> [http://lesswrong.com/lw/kr/an_alien_god/], natural selection, that managed to cough up the Artificial Intelligence called humans, by retaining its lucky successes and compounding them.  And the mob, if it retains its lucky successes and shares them, may also cough up an Artificial Intelligence, with around the same degree of precise control.  But it is only the collective that I worry about as dangerous&#8212;the individuals don't seem that formidable.</p> <p>If you yourself speak fluent Bayesian, and you distinguish a person-concerned-with-AGI as speaking fluent Bayesian, then you should consider that person as excepted from this whole discussion.</p> <p>Of course, among people who declare that they want to solve the AGI problem, the supermajority don't speak fluent Bayesian.</p> <p>Why would they?  Most people don't.</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age"><em>Yudkowsky's Coming of Age</em></a> [http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age]</p> <p style="text-align:right">Next post: "<a href="0496.html">The Magnitude of His Own Folly</a> [http://lesswrong.com/lw/ue/the_magnitude_of_his_own_folly/]"</p> <p style="text-align:right">Previous post: "<a href="0493.html">Competent Elites</a> [http://lesswrong.com/lw/ub/competent_elites/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq22.html">Sequence 22: Yudkowsky's Coming of Age</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0493.html">Competent Elites</a></p></td><td><p><i>Next: </i><a href="0496.html">The Magnitude of His Own Folly</a></p></td></tr></table><p><i>Referenced by: </i><a href="0493.html">Competent Elites</a> &#8226; <a href="0496.html">The Magnitude of His Own Folly</a> &#8226; <a href="0505.html">On Doing the Impossible</a> &#8226; <a href="0552.html">Failure By Affective Analogy</a> &#8226; <a href="0574.html">Artificial Mysterious Intelligence</a> &#8226; <a href="0579.html">What I Think, If Not Why</a> &#8226; <a href="0594.html">Nonperson Predicates</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/">Above-Average AI Scientists</a></p></body></html>