<html><head><title>Amputation of Destiny</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Amputation of Destiny</h1><p><i>Eliezer Yudkowsky, 29 December 2008 06:00PM</i></p><div><p><strong>Followup to</strong>:  <a href="0595.html">Nonsentient Optimizers</a> [http://lesswrong.com/lw/x5/nonsentient_optimizers/], <a href="0597.html">Can't Unbirth a Child</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/]</p> <p>From <em>Consider Phlebas</em> by Iain M. Banks:</p> <p style="margin-left: 40px;">&#160;&#160;&#160; In practice as well as theory the Culture was beyond considerations of wealth or empire.  The very concept of money&#8212;regarded by the Culture as a crude, over-complicated and inefficient form of rationing&#8212;was irrelevant within the society itself, where the capacity of its means of production ubiquitously and comprehensively exceeded every reasonable (and in some cases, perhaps, unreasonable) demand its not unimaginative citizens could make.  These demands were satisfied, with one exception, from within the Culture itself.  Living space was provided in abundance, chiefly on matter-cheap Orbitals; raw material existed in virtually inexhaustible quantities both between the stars and within stellar systems; and energy was, if anything, even more generally available, through fusion, annihilation, the Grid itself, or from stars (taken either indirectly, as radiation absorbed in space, or directly, tapped at the stellar core).  Thus the Culture had no need to colonise, exploit, or enslave.<br>&#160;&#160;&#160; The only desire the Culture could not satisfy from within itself was one common to both the descendants of its original human stock and the machines they had (at however great a remove) brought into being: the urge not to feel useless.  The Culture's sole justification for the relatively unworried, hedonistic life its population enjoyed was its good works; the secular evangelism of the Contact Section, not simply finding, cataloguing, investigating and analysing other, less advanced civilizations but&#8212;where the circumstances appeared to Contact to justify so doing&#8212;actually interfering (overtly or covertly) in the historical processes of those other cultures.</p> <p>Raise the subject of science-fictional utopias in front of any halfway sophisticated audience, and someone will mention the Culture.  Which is to say: Iain Banks is the one to beat.</p> <p><a id="more"></a></p> <p>Iain Banks's Culture could be called the apogee of hedonistic low-grade transhumanism.  Its people are beautiful and fair, as pretty as they choose to be.  Their bodies have been reengineered for swift adaptation to different gravities; and also reengineered for greater sexual endurance.  Their brains contains glands that can emit various euphoric drugs on command.  They live, in perfect health, for generally around four hundred years before choosing to die (I don't quite understand why they would, but this is low-grade transhumanism we're talking about).  Their society is around eleven thousand years old, and held together by the Minds, artificial superintelligences decillions of bits big, that run their major ships and population centers.</p> <p><em>Consider Phlebas</em>, the first Culture novel, introduces all this from the perspective of an outside agent <em>fighting </em>the Culture&#8212;someone convinced that the Culture spells an end to life's meaning.  Banks uses his novels to criticize the Culture along many dimensions, while simultaneously keeping the Culture a well-intentioned society of mostly happy people&#8212;an ambivalence which saves the literary quality of his books, avoiding either utopianism or dystopianism.  Banks's books vary widely in quality; I would recommend starting with <em>Player of Games,</em> the quintessential Culture novel, which I would say achieves greatness.</p> <p>From a <a href="0585.html">fun-theoretic</a> [http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/] perspective, the Culture and its humaniform citizens have a number of problems, some already covered in this series, some not.</p> <p>The Culture has deficiencies in <a href="0586.html">High Challenge</a> [http://lesswrong.com/lw/ww/high_challenge/] and <a href="0587.html">Complex Novelty</a> [http://lesswrong.com/lw/wx/complex_novelty/].  There are incredibly complicated games, of course, but these are <em>games</em>&#8212;not things with enduring consequences, woven into the story of your life.  Life itself, in the Culture, is neither especially challenging nor especially novel; your future is not an unpredictable thing about which to be curious.</p> <p><a href="0589.html">Living By Your Own Strength</a> [http://lesswrong.com/lw/wz/living_by_your_own_strength/] is not a theme of the Culture.  If you want something, you ask a Mind how to get it; and they will helpfully provide it, rather than saying "No, you figure out how to do it yourself."  The people of the Culture have little use for personal formidability, nor for <a href="0022.html">a wish to become stronger</a> [http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/].  To me, the notion of growing in strength seems obvious, and it also seems obvious that the humaniform citizens of the Culture ought to grow into Minds themselves, over time.  But the people of the Culture do <em>not </em>seem to get any smarter as they age; and after four hundred years so, they displace themselves into a sun.  These two literary points are probably related.</p> <p>But the Culture's <em>main </em>problem, I would say, is...</p> <p>...the same as Narnia's main problem, actually.  Bear with me here.</p> <p>If you read <em>The Lion, the Witch, and the Wardrobe</em> or saw the first <em>Chronicles of Narnia</em> movie, you'll recall&#8212;</p> <p>&#8212;I suppose that if you don't want any spoilers, you should stop reading here, but since it's a children's story and based on Christian theology, I don't think I'll be giving away too much by saying&#8212;</p> <p>&#8212;that the four human children who are the main characters, fight the White Witch and defeat her with the help of the great talking lion Aslan.</p> <p>Well, to be precise, Aslan defeats the White Witch.</p> <p>It's never explained why Aslan ever <em>left </em>Narnia a hundred years ago, allowing the White Witch to impose eternal winter and cruel tyranny on the inhabitants.  Kind of an awful thing to do, wouldn't you say?</p> <p>But once Aslan comes back, he kicks the White Witch out and everything is okay again.  There's no obvious reason why Aslan actually <em>needs</em> the help of four snot-nosed human youngsters.  Aslan could have led the armies.  In fact, Aslan <em>did </em>muster the armies and lead them before the children showed up.  Let's face it, the kids are just along for the ride.</p> <p>The problem with Narnia... is Aslan.</p> <p>C. S. Lewis never needed to write Aslan into the story.  The plot makes far more sense without him.  The children could show up in Narnia on their own, and lead the armies on their own.</p> <p>But is poor Lewis alone to blame?  Narnia was written as a Christian parable, and the Christian religion itself has exactly the same problem.  All Narnia does is project the flaw in a stark, simplified light: this story has an extra lion.</p> <p>And the problem with the Culture is the Minds.</p> <p>"Well..." says the transhumanist SF fan, "Iain Banks <em>did </em>portray the Culture's Minds as 'cynical, amoral, and downright sneaky' in their altruistic way; and they do, in his stories, mess around with humans and use them as pawns.  But that is mere <a href="0131.html">fictional evidence</a> [http://lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/].  A better-organized society would have laws against big Minds messing with small ones without consent.  Though if a Mind is <em>truly</em> wise and kind and utilitarian, it should know how to balance possible resentment against other gains, without needing a law.  Anyway, the problem with the Culture is the meddling, not the Minds."</p> <p>But that's not what I mean.  What I mean is that if you could otherwise live in the same Culture&#8212;the same technology, the same lifespan and healthspan, the same wealth, freedom, and opportunity&#8212;</p> <p>"I don't want to live in <em>any </em>version of the Culture.  I don't want to live four hundred years in a biological body with a constant IQ and then die.  Bleah!"</p> <p>Fine, stipulate that problem solved.  My point is that if you could otherwise get the same quality of life, in the same world, but <em>without</em> any Minds around to usurp the role of main character, wouldn't you prefer&#8212;</p> <p>"What?" cry my transhumanist readers, incensed at this betrayal by one of their own.  "Are you saying that we should never create any minds smarter than human, or keep them under lock and chain?  Just because your soul is so small and mean that you can't bear the thought of anyone else being better than you?"</p> <p>No, I'm not saying&#8212;</p> <p>"Because that business about our souls shriveling up due to 'loss of meaning' is <em>typical </em>bioconservative neo-Luddite propaganda&#8212;"</p> <p>Invalid argument: the world's greatest fool may say the sun is shining but <a href="0190.html">that doesn't make it dark out</a> [http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/].  But in any case, that's <em>not</em> what I'm saying&#8212;</p> <p>"It's a lost cause!  You'll never prevent intelligent life from achieving its destiny!"</p> <p>Trust me, I&#8212;</p> <p>"And anyway it's a silly question to begin with, because you <em>can't</em> just remove the Minds and keep the same technology, wealth, and society."</p> <p>So you admit the Culture's Minds are a <em>necessary evil</em>, then.  A price to be paid.</p> <p>"Wait, I didn't say <em>that </em>-"</p> <p>And <em>I</em> didn't say all that stuff <em>you're</em> imputing to <em>me!</em></p> <p>Ahem.</p> <p>My model already says <a href="0582.html">we live in a Big World</a> [http://lesswrong.com/lw/ws/for_the_people_who_are_still_alive/].  In which case there are vast armies of minds out there in the immensity of Existence (not just Possibility) which are far more awesome than myself.  Any shrivelable souls can already go ahead and shrivel.</p> <p>And I just talked about people growing up into Minds over time, at some <a href="0587.html">eudaimonic rate of intelligence increase</a> [http://lesswrong.com/lw/wx/complex_novelty/].  So clearly I'm not trying to 'prevent intelligent life from achieving its destiny', nor am I trying to enslave all Minds to biological humans scurrying around forever, nor am I etcetera.  (I do wish people wouldn't be <em>quite </em>so fast to assume that I've suddenly turned to the Dark Side&#8212;though I suppose, in this day and era, it's never an implausible hypothesis.)</p> <p>But I've already argued that we need a <a href="0594.html">nonperson predicate</a> [http://lesswrong.com/lw/x4/nonperson_predicates/]&#8212;some way of knowing that some computations are definitely <em>not </em>people&#8212;to avert an AI from creating sentient simulations in its efforts to <em>model</em> people.</p> <p>And trying to <a href="0595.html">create a Very Powerful Optimization Process that lacks subjective experience and other aspects of personhood</a> [http://lesswrong.com/lw/x5/nonsentient_optimizers/], is <em>probably </em>&#8212;though I still confess myself somewhat confused on this subject&#8212;probably <a href="0595.html">substantially <em>easier</em> than coming up with a nonperson predicate</a> [http://lesswrong.com/lw/x5/nonsentient_optimizers/].</p> <p>This being the case, <a href="0597.html">there are very strong reasons why a superintelligence should <em>initially</em> be designed to be knowably nonsentient</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/], if at all possible.  Creating a new kind of sentient mind is a huge and non-undoable act.</p> <p>Now, this doesn't answer the question of whether a nonsentient Friendly superintelligence ought to <em>make</em> itself sentient, or whether an NFSI ought to immediately manufacture sentient Minds first thing in the morning, once it has adequate wisdom to make the decision.</p> <p>But there is <a href="0585.html">nothing except our own preferences, out of which to construct the Future</a> [http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/].  So though this piece of information is not <em>conclusive</em>, nonetheless it is highly <em>informative:</em></p> <p>If you already had the lifespan and the health and the promise of future growth, would you <em>want</em> new powerful superintelligences to be created in your vicinity, on your same playing field?</p> <p>Or would you prefer that we stay on as the main characters in the story of intelligent life, with no higher beings above us?</p> <p>Should existing human beings grow up at some eudaimonic rate of intelligence increase, and then eventually decide what sort of galaxy to create, and how to people it?</p> <p>Or is it better for a nonsentient superintelligence to exercise that decision on our behalf, and start creating new powerful Minds right away?</p> <p>If we don't <em>have </em>to do it one way or the other&#8212;if we have both options&#8212;and if there's no particular need for heroic self-sacrifice&#8212;then which do you <em>like?</em></p> <p>"I don't understand the <em>point </em>to what you're suggesting.  Eventually, the galaxy is going to have Minds in it, right?  We have to find a stable state that allows big Minds and little Minds to coexist.  So what's the point in waiting?"</p> <p>Well... you could have the humans grow up (at some eudaimonic rate of intelligence increase), and then when new people are created, they might be created as powerful Minds to start with.  Or when you create new minds, <em>they </em>might have a different emotional makeup, which doesn't lead them to feel overshadowed if there are more powerful Minds above them.  But <em>we</em>, as we exist <a href="0582.html">already created</a> [http://lesswrong.com/lw/ws/for_the_people_who_are_still_alive/]&#8212;<em>we </em>might prefer to stay on as the main characters, for now, if given a choice.</p> <p>"You are showing far too much concern for six billion squishy things who happen to be alive today, out of all the unthinkable vastness of space and time."</p> <p>The Past contains enough tragedy, and has seen enough sacrifice already, I think.  And I'm not sure that you can cleave off the Future so neatly from the Present.</p> <p>So I will set out as I mean the future to continue: with concern for the living.</p> <p>The sound of six billion faces being casually stepped on, does not seem to me like a good beginning.  Even the Future should not be assumed to prefer that another chunk of pain be paid into its price.</p> <p>So yes, I am concerned <a href="0582.html">for those currently alive</a> [http://lesswrong.com/lw/ws/for_the_people_who_are_still_alive/], because it is <em>that concern</em>&#8212;and <em>not </em>a casual attitude toward the welfare of sentient beings&#8212;which I wish to continue into the Future.</p> <p>And I will not, if at all possible, give any other human being the least cause to think that someone else might spark a better Singularity.  I can make no promises upon the future, but I will at least not <em>close off</em> desirable avenues through my own actions.  I will not, on my own authority, create a sentient superintelligence which may <em>already determine</em> humanity as having passed on the torch.  It is too much to do on my own, and too much harm to do on my own&#8212;to amputate someone else's destiny, and steal their main character status.  That is yet another reason not to create a sentient superintelligence <em>to start with.</em>  (And it's part of the logic behind the <a href="http://intelligence.org/upload/CEV.html">CEV proposal</a> [http://intelligence.org/upload/CEV.html]<em></em>, which carefully avoids filling in any moral parameters not yet determined.)</p> <p>But to return finally to the Culture and to <a href="0585.html">Fun Theory</a> [http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/]:</p> <p>The Minds in the Culture don't need the humans, and yet the humans need to be needed.</p> <p>If you're going to have human-level minds with human emotional makeups, they shouldn't be competing on a level playing field with superintelligences.  Either keep the superintelligences off the local playing field, or design the human-level minds with a different emotional makeup.</p> <p>"The Culture's sole justification for the relatively unworried, hedonistic life its population enjoyed was its good works," writes Iain Banks.  This indicates a rather unstable moral position.  Either the life the population enjoys is eudaimonic enough to be its <em>own</em> justification, an end rather than a means; or else that life needs to be changed.</p> <p>When people are in need of rescue, this is is a goal of the <a href="0586.html">overriding-static-predicate</a> [http://lesswrong.com/lw/ww/high_challenge/] sort, where you rescue them <em>as fast as possible</em>, and <em>then you're done</em>.  Preventing suffering <em>cannot provide a lasting meaning to life.</em>  What happens when you run out of victims?  If there's nothing more to life than eliminating suffering, you might as well eliminate life and be done.</p> <p>If the Culture isn't valuable enough for <em>itself</em>, even without its good works&#8212;then the Culture might as well not be.  And when the Culture's Minds could do a better job and faster, "good works" can hardly justify the <em>human</em> existences within it.</p> <p>The human-level people need a destiny to make for themselves, and they need the overshadowing Minds off their playing field while they make it.  Having an external evangelism project, and being given cute little roles that any Mind could do better in a flash, so as to "supply meaning", isn't going to cut it.</p> <p>That's far from the only thing the Culture is doing wrong, but it's at the top of my list.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0599.html">Dunbar's Function</a> [http://lesswrong.com/lw/x9/dunbars_function/]"</p> <p style="text-align:right">Previous post: "<a href="0597.html">Can't Unbirth a Child</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0594.html">Nonperson Predicates</a></p></td><td><p><i>Next: </i><a href="0599.html">Dunbar's Function</a></p></td></tr></table><p><i>Referenced by: </i><a href="0597.html">Can't Unbirth a Child</a> &#8226; <a href="0599.html">Dunbar's Function</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0626.html">31 Laws of Fun</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/x8/amputation_of_destiny/">Amputation of Destiny</a></p></body></html>