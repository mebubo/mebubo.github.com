<html><head><title>Nonperson Predicates</title></head><body><h1>Nonperson Predicates</h1><p><i>Eliezer Yudkowsky, 27 December 2008 01:47AM</i></p><div><p><strong>Followup to</strong>:  <a href="0283.html">Righting a Wrong Question</a> [http://lesswrong.com/lw/oh/righting_a_wrong_question/], <a href="0309.html">Zombies! Zombies?</a> [http://lesswrong.com/lw/p7/zombies_zombies/], <a href="0368.html">A Premature Word on AI</a> [http://lesswrong.com/lw/qu/a_premature_word_on_ai/], <a href="0505.html">On Doing the Impossible</a> [http://lesswrong.com/lw/un/on_doing_the_impossible/]</p> <p>There is a subproblem of <a href="http://yudkowsky.net/singularity/ai-risk">Friendly AI</a> [http://yudkowsky.net/singularity/ai-risk] which is so scary that I usually don't talk about it, because very few would-be AI designers would react to it appropriately&#8212;that is, by saying, "Wow, that does sound like an <em>interesting </em>problem", instead of finding one of many subtle ways to scream and run away.</p> <p>This is the problem that if you create an AI and tell it to model the world around it, it may form models of people that are people themselves.  Not necessarily the <em>same</em> person, but people nonetheless.</p> <p>If you look up at the night sky, and see the tiny dots of light that move over days and weeks&#8212;<span lang="grc-Latn" style="white-space: normal; text-decoration: none;" xml:lang="grc-Latn" title="grc transliteration"><em>plan&#275;toi</em></span>, the Greeks called them, "wanderers"&#8212;and you try to predict the movements of those planet-dots as best you can...</p> <p>Historically, humans went through a journey as long and as wandering as the planets themselves, to find an accurate model.  In the beginning, the models were things of cycles and epicycles, not much resembling the true Solar System.</p> <p>But eventually we found laws of gravity, and finally built models&#8212;even if they were just on paper&#8212;that were <em>extremely</em> accurate so that Neptune could be deduced by looking at the unexplained perturbation of Uranus from its expected orbit.  This required moment-by-moment modeling of where a simplified version of Uranus would be, and the other known planets.  Simulation, not just abstraction.  Prediction through simplified-yet-still-detailed pointwise similarity.</p> <p>Suppose you have an AI that is around human beings.  And like any Bayesian trying to explain its enivornment, the AI goes in quest of <em>highly accurate models </em>that predict what it sees of humans.</p> <p>Models that predict/explain why people do the things they do, say the things they say, want the things they want, think the things they think, and even why people talk about "the <a href="0080.html">mystery</a> [http://lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/] of <a href="0309.html">subjective experience</a> [http://lesswrong.com/lw/p7/zombies_zombies/]".</p> <p>The model that most precisely predicts these facts, may well be a 'simulation' detailed enough to <em>be </em>a person in its own right.</p> <p><a id="more"></a></p> <p>A highly detailed model <em>of </em>me, may not <em>be </em>me.  But it will, at least, be a model which (for purposes of prediction via similarity) thinks <em>itself </em>to be Eliezer Yudkowsky.  It will be a model that, when cranked to find my behavior if asked "Who are you and are you conscious?", says "I am Eliezer Yudkowsky and I seem have subjective experiences" <a href="0309.html">for much the same reason I do</a> [http://lesswrong.com/lw/p7/zombies_zombies/].</p> <p>If that doesn't worry you, (re)read "<a href="0309.html">Zombies! Zombies?</a> [http://lesswrong.com/lw/p7/zombies_zombies/]".</p> <p>It seems likely (though not certain) that this happens <em>automatically</em>, whenever a mind of sufficient power to find the right answer, and not <em>otherwise</em> disinclined to create a sentient being trapped within itself, tries to model a human as accurately as possible.</p> <p>Now you could wave your hands and say, "Oh, by the time the AI is smart enough to do that, it will be smart enough not to".  (This is, in general, a phrase useful in running away from Friendly AI problems.)  But do you know this for a fact?</p> <p>When dealing with things that confuse you, it is wise to widen your confidence intervals.  Is a human mind the simplest possible mind that can be sentient?  What if, in the course of trying to model its own programmers, a relatively younger AI manages to create a sentient simulation trapped within itself?  How soon do you have to start worrying?  Ask yourself that fundamental question, "What do I think I know, and how do I think I know it?"</p> <p>You could wave your hands and say, "Oh, it's more important to get the job done quickly, then to worry about such relatively minor problems; the end justifies the means.  Why, look at all these problems the Earth has right now..."  (This is also a general way of running from Friendly AI problems.)</p> <p>But we may consider and discard many hypotheses in the course of finding the truth, and we are but slow humans.  What if an AI creates millions, billions, trillions of alternative hypotheses, models that are actually people, who die when they are disproven?</p> <p>If you accidentally kill a few trillion people, or permit them to be killed&#8212;you could say that the weight of the Future outweighs this evil, perhaps.  But the absolute weight of the sin would not be light.  If you would balk at killing a million people with a nuclear weapon, you should balk at this.</p> <p>You could wave your hands and say, "The model will contain abstractions over various uncertainties within it, and this will prevent it from being conscious even though it produces well-calibrated probability distributions over what you will say when you are asked to talk about consciousness."  To which I can only reply, "That would be very convenient if it were true, but how the hell do you <em>know </em>that?"  An element of a model marked 'abstract' is still there as a computational token, and the interacting causal system may still be sentient.</p> <p>For these purposes, we do not, in principle, need to crack the entire Hard Problem of Consciousness&#8212;the <a href="0281.html">confusion</a> [http://lesswrong.com/lw/of/dissolving_the_question/] that we name "subjective experience".  We only need to understand enough of it to know when a process is <em>not</em> conscious, <em>not </em>a person, <em>not </em>something deserving of the rights of citizenship.  In practice, I suspect you can't <em>halfway </em>stop being confused&#8212;but in theory, half would be enough.</p> <p>We need a <em>nonperson predicate</em>&#8212;a predicate that returns 1 for anything that is a person, and can return 0 or 1 for anything that is not a person.  This is a "nonperson predicate" because <em>if </em>it returns 0, <em>then </em>you know that something is definitely not a person.</p> <p>You can have more than one such predicate, and if <em>any </em>of them returns 0, you're ok.  It just had better never return 0 on anything that <em>is</em> a person, however many nonpeople it returns 1 on.</p> <p>We can even hope that the vast majority of models the AI needs, will be swiftly and trivially approved by a predicate that quickly answers 0.  And that the AI would only need to resort to more specific predicates in case of modeling actual people.</p> <p>With a good toolbox of nonperson predicates in hand, we could exclude all "model citizens"&#8212;all beliefs that are themselves people&#8212;from the set of hypotheses our Bayesian AI may invent to try to model its person-containing environment.</p> <p>Does that sound odd?  Well, one has to handle the problem somehow.  I am open to better ideas, though I will be a bit skeptical about any suggestions for how to proceed that let us <a href="0574.html">cleverly avoid solving the damn mystery</a> [http://lesswrong.com/lw/wk/artificial_mysterious_intelligence/].</p> <p>So do I <em>have</em> a nonperson predicate?  No.  At least, no nontrivial ones.</p> <p>This is a challenge that I have not even tried to talk about, with <a href="0494.html">those folk who think themselves ready to challenge the problem of true AI</a> [http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/].  For they seem to have the standard reflex of running away from difficult problems, and are challenging AI only because <a href="0461.html">they think their amazing insight has already solved it</a> [http://lesswrong.com/lw/tf/dreams_of_ai_design/].  Just mentioning the problem of Friendly AI by itself, or of precision-grade AI design, is enough to send them fleeing into the night, screaming "It's too hard!  It can't be done!"  If I tried to explain that their job duties might impinge upon the sacred, mysterious, holy Problem of Subjective Experience&#8212;</p> <p>&#8212;I'd actually expect to get blank stares, mostly, followed by some <em>instantaneous </em>dismissal which requires no further effort on their part.  I'm not sure of what the exact dismissal would be&#8212;maybe, "Oh, none of the hypotheses my AI considers, could <em>possibly </em>be a person?"  I don't know; I haven't bothered trying.  But it has to be a dismissal which rules out all possibility of their having to <a href="0574.html">actually solve the damn problem</a> [http://lesswrong.com/lw/wk/artificial_mysterious_intelligence/], because most of them would think that they are smart enough to build an AI&#8212;indeed, smart enough to have already solved the key part of the problem&#8212;but not smart enough to solve the Mystery of Consciousness, which still <em>looks</em> scary to them.</p> <p>Even if they thought of trying to solve it, they would be afraid of <em>admitting </em>they were trying to solve it.  Most of these people cling to the shreds of their <a href="0004.html">modesty</a> [http://lesswrong.com/lw/gq/the_proper_use_of_humility/], trying at one and the same time to have solved the AI problem while still being humble ordinary blokes.  (There's a grain of truth to that, but at the same time: who the hell do they think they're kidding?)  They know without words that their audience sees the Mystery of Consciousness as a <em>sacred untouchable problem</em>, <a href="0366.html">reserved for some future superbeing</a> [http://lesswrong.com/lw/qs/einsteins_superpowers/].  They don't want people to think that they're claiming <a href="0366.html">an Einsteinian aura of destiny</a> [http://lesswrong.com/lw/qs/einsteins_superpowers/] by trying to solve the problem.  So it is easier to dismiss the problem, and not believe a proposition that would be uncomfortable to explain.</p> <p>Build an AI?  Sure!  Make it Friendly?  Now that you point it out, sure!  But trying to come up with a "nonperson predicate"?  That's just way above the difficulty level they signed up to handle.</p> <p>But <a href="0080.html">a blank map does not correspond to a blank territory</a> [http://lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/].  <a href="0282.html">Impossible confusing questions correspond to places where your own thoughts are tangled</a> [http://lesswrong.com/lw/og/wrong_questions/], not to places where the environment itself contains magic.  <a href="0366.html">Even difficult problems do not require an aura of destiny to solve</a> [http://lesswrong.com/lw/qs/einsteins_superpowers/].  And the first step to solving one is <a href="0505.html">not running away from the problem like a frightened rabbit</a> [http://lesswrong.com/lw/un/on_doing_the_impossible/], but instead sticking long enough to learn something.</p> <p>So let us not run away from this problem.  <a href="0282.html">I doubt it is even difficult in any absolute sense, just a place where my brain is tangled.</a> [http://lesswrong.com/lw/og/wrong_questions/]  I suspect, based on some prior experience with similar challenges, that you can't <em>really </em>be good enough to build a Friendly AI, and still be tangled up in your own brain like that.  So it is not necessarily any <em>new </em>effort&#8212;over and above that required <em>generally </em>to build a mind while knowing exactly what you are about.</p> <p>But in any case, I am not screaming and running away from the problem.  And I hope that you, dear longtime reader, will not faint at the audacity of my trying to solve it.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0595.html">Nonsentient Optimizers</a> [http://lesswrong.com/lw/x5/nonsentient_optimizers/]"</p> <p style="text-align:right">Previous post: "<a href="0593.html">Devil's Offers</a> [http://lesswrong.com/lw/x3/devils_offers/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0593.html">Devil's Offers</a></p></td><td><p><i>Next: </i><a href="0598.html">Amputation of Destiny</a></p></td></tr></table><p><i>Referenced by: </i><a href="0593.html">Devil's Offers</a> &#8226; <a href="0595.html">Nonsentient Optimizers</a> &#8226; <a href="0598.html">Amputation of Destiny</a> &#8226; <a href="0603.html">Growing Up is Hard</a> &#8226; <a href="0610.html">Continuous Improvement</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/x4/nonperson_predicates/">Nonperson Predicates</a></p></body></html>