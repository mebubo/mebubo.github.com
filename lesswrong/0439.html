<html><head><title>Anthropomorphic Optimism</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Anthropomorphic Optimism</h1><p><i>Eliezer Yudkowsky, 04 August 2008 08:17PM</i></p><div><p><strong>Followup to</strong>:  <a href="0434.html">Humans in Funny Suits</a> [http://lesswrong.com/lw/so/humans_in_funny_suits/], <a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]</p> <p>The core fallacy of anthropomorphism is expecting something to be predicted by the black box of your brain, when its casual structure is so different from that of a human brain, as to give you no license to expect any such thing.</p> <p><a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/] (as previously covered in the evolution sequence) was a rather extreme error by a group of early (pre-1966) biologists, including Wynne-Edwards, Allee, and Brereton among others, who believed that predators would voluntarily restrain their breeding to avoid overpopulating their habitat and exhausting the prey population.</p> <p>The proffered theory was that if there were multiple, geographically separated groups of e.g. foxes, then <em>groups</em> of foxes that best restrained their breeding, would send out colonists to replace crashed populations.  And so, over time, group selection would promote restrained-breeding genes in foxes.</p> <p>I'm not going to repeat <a href="0154.html">all the problems that developed with this scenario</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]. Suffice it to say that there was no empirical evidence to start with; that no empirical evidence was ever uncovered; that, in fact, predator populations crash all the time; and that for group selection pressure to overcome a countervailing individual selection pressure, turned out to be very nearly mathematically impossible.</p> <p>The theory having turned out to be completely incorrect, we may ask if, perhaps, the originators of the theory were doing something wrong.</p> <p><a id="more"></a></p> <p>"Why be so uncharitable?" you ask.  "<a href="0356.html">In advance of doing the experiment</a> [http://lesswrong.com/lw/qi/faster_than_science/], how could they know that group selection couldn't overcome individual selection?"</p> <p>But later on, Michael J. Wade went out and<span style="font-style: italic;"> </span><em>actually</em> created in the laboratory the nigh-impossible conditions for group selection.  Wade repeatedly selected insect subpopulations for low population numbers.  Did the insects evolve to restrain their breeding, and live in quiet peace with enough food for all, as the group selectionists had envisioned?</p> <p>No; the adults adapted to cannibalize eggs and larvae, especially female larvae.</p> <p><em>Of course</em> selecting for small subpopulation sizes would not select for individuals who restrained their <em>own</em> breeding.  It would select for individuals who ate <em>other</em> individuals' children.  Especially the girls.</p> <p>Now, why might the group selectionists have <em>not</em> thought of that possibility?</p> <p>Suppose you were a member of a tribe, and you knew that, in the near future, your tribe would be subjected to a resource squeeze.  You might propose, as a solution, that no couple have more than one child - after the first child, the couple goes on birth control.  Saying, "Let's all individually have as many children as we can, but then hunt down and cannibalize each other's children, especially the girls," would not even <em>occur to you as a possibility.</em></p> <p>Think of a preference ordering over solutions, relative to your goals.  You want a solution as high in this preference ordering as possible.  How do you find one?  With a brain, of course!  Think of your brain as a <em>high-ranking-solution-generator</em> - a search process that produces solutions that rank high in your innate preference ordering.</p> <p>The solution space on all real-world problems is generally fairly large, which is why you need an <em>efficient</em> brain that doesn't even <em>bother to formulate</em> the vast majority of low-ranking solutions.</p> <p>If your tribe is faced with a resource squeeze, you could try hopping everywhere on one leg, or chewing off your own toes.  These "solutions" obviously wouldn't work and would incur large costs, as you can see upon examination - but in fact your brain is too efficient to waste time considering such poor solutions; it doesn't generate them in the first place.  Your brain, in its search for high-ranking solutions, flies directly to parts of the solution space like "Everyone in the tribe gets together, and agrees to have no more than one child per couple until the resource squeeze is past."</p> <p>Such a <em>low-ranking</em> solution as "Everyone have as many kids as possible, then cannibalize the girls" would not be <em>generated in your search process.</em></p> <p>But the ranking of an option as "low" or "high" is <a href="0284.html">not an inherent property</a> [http://lesswrong.com/lw/oi/mind_projection_fallacy/] of the option, it is a property of the optimization process that does the preferring.  And different optimization processes will search in different orders.</p> <p>So far as <em>evolution</em> is concerned, individuals reproducing to the fullest and then cannibalizing others' daughters, is a no-brainer; whereas individuals voluntarily restraining their own breeding for the good of the group, is absolutely ludicrous.  Or to say it less anthropomorphically, the first set of alleles would rapidly replace the second in a population.  (And natural selection has no obvious search order here - these two alternatives seem around equally simple as mutations).</p> <p>Suppose that one of the biologists had said, "If a predator population has only finite resources, evolution will craft them to voluntarily restrain their breeding - that's how <em>I'd</em> do it if <em>I</em> were in charge of building predators."  This would be anthropomorphism outright, the lines of reasoning naked and exposed:  <em>I</em> would do it this way, therefore I infer that <em>evolution </em>will do it this way.</p> <p>One does occasionally encounter the fallacy outright, in my line of work.  But suppose you say to the one, "An AI will not necessarily work like you do".  Suppose you say to this hypothetical biologist, "Evolution doesn't work like you do."  What will the one say in response?  I can tell you a reply you will <em>not</em> hear:  "Oh my! I didn't realize that!  One of the steps of my inference was invalid; I will throw away the conclusion and start over from scratch."</p> <p>No: what you'll hear <em>instead</em> is a reason why <a href="0397.html">any AI has to reason the same way</a> [http://lesswrong.com/lw/rn/no_universally_compelling_arguments/] as the speaker.  Or a reason why natural selection, following entirely different criteria of optimization and using entirely different methods of optimization, ought to do <em>the same thing</em> that would occur to a human as a good idea.</p> <p>Hence the elaborate idea that group selection would favor predator groups where the individuals voluntarily forsook reproductive opportunities.</p> <p>The group selectionists went just as far astray, in their predictions, as someone committing the fallacy outright.  Their <a href="0114.html">final conclusions</a> [http://lesswrong.com/lw/js/the_bottom_line/] were the same as if they were assuming outright that evolution necessarily thought like themselves.  But they erased what had been written <em>above</em> the bottom line of their argument, <em>without</em> erasing the actual bottom line, and wrote in new <a href="0116.html">rationalizations</a> [http://lesswrong.com/lw/ju/rationalization/].  Now the fallacious reasoning is disguised; the <em>obviously</em> flawed step in the inference has been hidden - even though <a href="0148.html">the conclusion remains exactly the same; and hence, in the real world, exactly as wrong</a> [http://lesswrong.com/lw/kq/fake_justification/].</p> <p>But why would any scientist do this?  In the end, the data came out against the group selectionists and they were embarrassed.</p> <p>As I remarked in <a href="0157.html">Fake Optimization Criteria</a> [http://lesswrong.com/lw/kz/fake_optimization_criteria/], we humans seem to have evolved an instinct for arguing that <em>our</em> preferred policy arises from practically <em>any</em> criterion of optimization.  Politics was a feature of the ancestral environment; we are descended from those who argued most persuasively that the <em>tribe's</em> interest - not just their own interest - required that their hated rival Uglak be executed.  We certainly aren't descended from Uglak, who failed to argue that <a href="0433.html">his tribe's moral code</a> [http://lesswrong.com/lw/sn/interpersonal_morality/] - not just his own obvious self-interest - required his survival.</p> <p>And because we can more persuasively argue, for what we honestly believe, we have evolved an instinct to honestly believe that other people's goals, and our tribe's moral code, truly do imply that they should do things <em>our</em> way for <em>their</em> benefit.</p> <p>So the group selectionists, imagining this beautiful picture of predators restraining their breeding, instinctively rationalized why natural selection ought to do things <em>their</em> way, even according to natural selection's own purposes. The foxes will be fitter if they restrain their breeding!  No, really! They'll even outbreed other foxes who don't restrain their breeding! Honestly!</p> <p>The problem with trying to argue natural selection into doing things your way, is that evolution does not contain that which could be moved by your arguments.  Evolution does not work like you do - not even to the extent of having any element that could listen to or <em>care about</em> your painstaking explanation of why evolution ought to do things your way.  Human arguments are not even <em>commensurate</em> with the internal structure of natural selection as an optimization process - human arguments aren't used in promoting alleles, as human arguments would play a causal role in human politics.</p> <p>So instead of <em>successfully</em> persuading natural selection to do things their way, the group selectionists were simply embarrassed when reality came out differently.</p> <p>There's a fairly heavy subtext here about Unfriendly AI.</p> <p>But the point generalizes: this is the problem with optimistic reasoning <em>in general.</em>  What is optimism?  It is ranking the possibilities by your own preference ordering, and selecting an outcome high in that preference ordering, and somehow that outcome ends up as your prediction.  What kind of elaborate rationalizations were generated along the way, is probably <a href="0119.html">not so relevant as one might fondly believe</a> [http://lesswrong.com/lw/jx/we_change_our_minds_less_often_than_we_think/]; look at the cognitive history and it's optimism in, optimism out.  But Nature, or whatever other process is under discussion, is not <em>actually, causally</em> choosing between outcomes by ranking them in your preference ordering and picking a high one.  So the brain fails to synchronize with the environment, and the prediction fails to match reality.</p></div> <hr><p><i>Referenced by: </i><a href="0440.html">Contaminated by Optimism</a> &#8226; <a href="0444.html">Sorting Pebbles Into Correct Heaps</a> &#8226; <a href="0456.html">Invisible Frameworks</a> &#8226; <a href="0459.html">Magical Categories</a> &#8226; <a href="0460.html">Three Fallacies of Teleology</a> &#8226; <a href="0461.html">Dreams of AI Design</a> &#8226; <a href="0465.html">Dreams of Friendliness</a> &#8226; <a href="0469.html">The True Prisoner's Dilemma</a> &#8226; <a href="0475.html">Points of Departure</a> &#8226; <a href="0477.html">Excluding the Supernatural</a> &#8226; <a href="0529.html">Efficient Cross-Domain Optimization</a> &#8226; <a href="0584.html">Visualizing Eutopia</a> &#8226; <a href="0602.html">The Uses of Fun (Theory)</a> &#8226; <a href="0629.html">Value is Fragile</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/st/anthropomorphic_optimism/">Anthropomorphic Optimism</a></p></body></html>