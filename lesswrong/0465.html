<html><head><title>Dreams of Friendliness</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Dreams of Friendliness</h1><p><i>Eliezer Yudkowsky, 31 August 2008 01:20AM</i></p><div><p><strong>Continuation of</strong>:  <a href="0464.html">Qualitative Strategies of Friendliness</a> [http://lesswrong.com/lw/ti/qualitative_strategies_of_friendliness/]</p> <p><a href="0464.html">Yesterday</a> [http://lesswrong.com/lw/ti/qualitative_strategies_of_friendliness/] I described three classes of deep problem with qualitative-physics-like strategies for building nice AIs - e.g., <a href="0459.html">the AI is reinforced by smiles, and happy people smile, therefore the AI will tend to act to produce happiness</a> [http://lesswrong.com/lw/td/magical_categories/].  In shallow form, three instances of the three problems would be:</p> <ol><li>Ripping people's faces off and wiring them into smiles;</li> <li>Building lots of tiny agents with happiness counters set to large numbers;</li> <li>Killing off the human species and replacing it with a form of sentient life that has no objections to being happy all day in a little jar.</li></ol> <p>And the deep forms of the problem are, roughly:</p> <ol><li>A superintelligence will search out alternate causal pathways to its goals than the ones you had in mind;</li> <li>The boundaries of moral categories are not <a href="0458.html">predictively natural entities</a> [http://lesswrong.com/lw/tc/unnatural_categories/];</li> <li>Strong optimization for only some humane values, does not imply a good total outcome.</li></ol> <p>But there are other ways, and deeper ways, of viewing the failure of qualitative-physics-based Friendliness strategies.</p> <p>Every now and then, someone proposes the Oracle AI strategy:  "Why not just have a superintelligence that <em>answers human questions</em>, instead of <em>acting autonomously</em> in the world?"</p> <p>Sounds pretty safe, doesn't it?  What could possibly go wrong?</p><a id="more"></a><p>Well... if you've got any respect for Murphy's Law, the power of superintelligence, and human stupidity, then you can probably think of quite a few things that could go wrong with this scenario.  Both in terms of how a naive implementation could fail - e.g., universe tiled with tiny users asking tiny questions and receiving fast, non-resource-intensive answers - and in terms of what could go wrong even if the basic scenario worked.</p> <p>But let's just talk about the structure of the AI.</p> <p>When someone reinvents the Oracle AI, the most common opening remark runs like this:</p> <p>"Why not just have the AI answer questions, instead of trying to <em>do</em> anything?  Then it wouldn't need to be Friendly.  It wouldn't need any goals at all.  It would just answer questions."</p> <p>To which the reply is that the AI needs goals in order to decide how to think: that is, the AI has to act as a powerful optimization process in order to plan its acquisition of knowledge, effectively distill sensory information, pluck "answers" to particular questions out of the space of all possible responses, and of course, to improve its own source code up to the level where the AI is a powerful intelligence.  All these events are "improbable" relative to random organizations of the AI's RAM, so the AI has to hit a narrow target in the space of possibilities to make superintelligent answers come out.</p> <p>Now, why might one think that an Oracle didn't need goals?  Because on a human level, the term "goal" seems to refer to those times when you said, "I want to be promoted", or "I want a cookie", and when someone asked you "Hey, what time is it?" and you said "7:30" that didn't seem to involve any goals.  Implicitly, you wanted to answer the question; and implicitly, you had a whole, complicated, functionally optimized brain that let you answer the question; and implicitly, you were able to do so because you looked down at your highly optimized watch, that you bought with money, using your skill of turning your head, that you acquired by virtue of curious crawling as an infant.  But that all takes place in the invisible background; it didn't <em>feel</em> like you wanted anything.</p> <p>Thanks to <a href="0437.html">empathic inference</a> [http://lesswrong.com/lw/sr/the_comedy_of_behaviorism/], which uses your own brain as an unopened black box to predict other black boxes, it can feel like "question-answering" is a detachable thing that comes loose of all the optimization pressures behind it - even the existence of a pressure to answer questions!</p> <p>Problem 4:  Qualitative reasoning about AIs often revolves around some nodes described by empathic inferences.  This is a bad thing: for <a href="0459.html">previously described reasons</a> [http://lesswrong.com/lw/td/magical_categories/]; and because it leads you to omit other nodes of the graph and their prerequisites and consequences; and because you may find yourself thinking things like, "But the AI has to <em>cooperate</em> to get a cookie, so now it will be <em>cooperative</em>" where "cooperation" is a boundary in concept-space drawn the way <a href="0439.html">you would prefer to draw it</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/]... etc.</p> <p>Anyway: the AI needs a goal of answering questions, and that has to give rise to subgoals of choosing efficient problem-solving strategies, improving its code, and acquiring necessary information.  You can quibble about terminology, but the optimization pressure has to be there, and it has to be very powerful, measured in terms of how small a target it can hit within a large design space.</p> <p>Powerful optimization pressures are scary things to be around.  Look at what natural selection inadvertently did to itself - dooming the very molecules of DNA - in the course of <a href="http://intelligence.org/blog/2007/07/10/the-power-of-intelligence">optimizing a few Squishy Things</a> [http://intelligence.org/blog/2007/07/10/the-power-of-intelligence] to make hand tools and outwit each other politically.  Humans, though we were optimized only according to the criterion of replicating ourselves, now have <a href="0159.html">their own psychological drives executing as adaptations</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/].  The result of humans optimized for replication is not just herds of humans; we've altered much of Earth's land area with our technological creativity.  We've even created some knock-on effects that we wish we hadn't, because our minds aren't powerful enough to foresee all the effects of the most powerful technologies we're smart enough to create.</p> <p>My point, however, is that when people visualize qualitative FAI strategies, they generally assume that only one thing is going on, the normal / modal / desired thing.  (See also: <a href="0102.html">planning fallacy</a> [http://lesswrong.com/lw/jg/planning_fallacy/].)  This doesn't always work even for picking up a rock and throwing it.  But it works rather a lot better for throwing rocks than unleashing powerful optimization processes.</p> <p>Problem 5:  When humans use qualitative reasoning, they tend to visualize a single line of operation as typical - everything operating the same way it usually does, no exceptional conditions, no interactions not specified in the graph, all events firmly inside their boundaries.  This works a lot better for dealing with boiling kettles, than for dealing with minds faster and smarter than your own.</p> <p>If you can manage to create a full-fledged Friendly AI with full coverage of humane (renormalized human) values, then the AI is visualizing the consequences of its acts, caring about the consequences you care about, and avoiding plans with consequences you would prefer to exclude.  A powerful optimization process, much more powerful than you, that <em>doesn't</em> share your values, is a very scary thing - even if it only "wants to answer questions", and even if it doesn't just tile the universe with tiny agents having simple questions answered.</p> <p>I don't mean to be insulting, but human beings have enough trouble controlling the technologies that they're smart enough to invent themselves.</p> <p>I sometimes wonder if maybe part of the problem with modern civilization is that politicians can press the buttons on nuclear weapons that they couldn't have invented themselves - not that it would be any better if we gave physicists political power that they weren't smart enough to obtain themselves - but the point is, our button-pressing civilization has an awful lot of people casting spells that they couldn't have written themselves.  I'm not saying this is a bad thing and we should stop doing it, but it <em>does</em> have consequences.  The thought of humans exerting detailed control over literally <em>superhuman</em> capabilities - wielding, with human minds, and in the service of merely human strategies, powers that no human being could have invented - doesn't fill me with easy confidence.</p> <p>With a full-fledged, full-coverage Friendly AI acting in the world - the impossible-seeming full case of the problem - the AI itself is managing the consequences.</p> <p>Is the Oracle AI thinking about the consequences of answering the questions you give it?  Does the Oracle AI care about those consequences the same way you do, applying <em>all</em> the same values, to warn you if <em>anything</em> of value is lost?</p> <p>What need has an Oracle for human questioners, if it knows what questions we <em>should</em> ask?  Why not just unleash the <em>should</em> function?</p> <p>See also the notion of an "<a href="http://en.wikipedia.org/wiki/AI-complete">AI-complete</a> [http://en.wikipedia.org/wiki/AI-complete]" problem.  Analogously, any Oracle into which you can type the English question "What is the code of an AI that always does the right thing?" must be FAI-complete.</p> <p>Problem 6:  Clever qualitative-physics-type proposals for bouncing this thing off the AI, to make it do that thing, in a way that initially seems to avoid the Big Scary Intimidating Confusing Problems that are <em>obviously</em> associated with full-fledged Friendly AI, tend to just run into exactly the same problem in <em>slightly less obvious</em> ways, concealed in Step 2 of the proposal.</p> <p>(And likewise you run right back into the intimidating problem of precise self-optimization, so that the Oracle AI can execute a billion self-modifications one after the other, and still <em>just</em> answer questions at the end; you're not avoiding that basic challenge of Friendly AI either.)</p> <p>But the deepest problem with qualitative physics is revealed by a proposal that comes earlier in the standard conversation, at the point when I'm talking about side effects of powerful optimization processes on the world:</p> <p>"We'll just keep the AI in a solid box, so it can't have any effects on the world <em>except</em> by how it talks to the humans."</p> <p>I explain the <a href="http://yudkowsky.net/essays/aibox.html">AI-Box Experiment</a> [http://yudkowsky.net/essays/aibox.html] (see also <a href="0358.html">That Alien Message</a> [http://lesswrong.com/lw/qk/that_alien_message/]); even granting the untrustworthy premise that a superintelligence can't think of <em>any</em> way to pass the walls of the box which you weren't smart enough to cover, human beings are not secure systems.  Even against other humans, often, let alone a superintelligence that might be able to hack through us like Windows 98; when was the last time you downloaded a security patch to your brain?</p> <p>"Okay, so we'll just give the AI the goal of <em>not having any effects on the world except from how it answers questions.</em>  Sure, that requires some FAI work, but the goal system as a whole sounds much simpler than your Coherent Extrapolated Volition thingy."</p> <p>What - <em>no</em> effects?</p> <p>"Yeah, sure.  If it has any effect on the world apart from talking to the programmers through the legitimately defined channel, the utility function assigns that infinite negative utility.  What's wrong with that?"</p> <p>When the AI thinks, that has a physical embodiment.  Electrons flow through its transistors, moving around.  If it has a hard drive, the hard drive spins, the read/write head moves.  That has <em>gravitational</em> effects on the outside world.</p> <p>"What?  Those effects are too small!  They don't count!"</p> <p>The physical effect is just as real as if you shot a cannon at something - yes, might not notice, but that's just because our vision is bad at small length-scales.  Sure, the effect is to move things around by 10^whatever Planck lengths, instead of the 10^more Planck lengths that you would consider as "counting".  But spinning a hard drive can move things just outside the computer, or just outside the room, by <a href="0311.html">whole neutron diameters</a> [http://lesswrong.com/lw/p9/the_generalized_antizombie_principle/] -</p> <p>"So?  Who cares about a neutron diameter?"</p> <p>- and by quite standard chaotic physics, that effect is liable to blow up.  The butterfly that flaps its wings and causes a hurricane, etc.  That effect may not be easily <em>controllable</em> but that doesn't mean the chaotic effects of small perturbations are not <em>large.</em></p> <p>But in any case, your proposal was to give the AI a goal of having <em>no</em> effect on the world, apart from effects that proceed through talking to humans.  And this is impossible of fulfillment; so no matter what it does, the AI ends up with infinite negative utility - how is its behavior defined in this case?  (In this case I picked a silly initial suggestion - but one that I <em>have</em> heard made, as if infinite negative utility were like an exclamation mark at the end of a command given a human employee.  Even an unavoidable <em>tiny probability</em> of infinite negative utility trashes the goal system.)</p> <p>Why <em>would</em> anyone possibly think that a physical object like an AI, in our highly interactive physical universe, containing hard-to-shield forces like gravitation, could avoid <em>all</em> effects on the outside world?</p> <p>And this, I think, reveals what may be the deepest way of looking at the problem:</p> <p>Problem 7:  Human beings model a world made up of objects, attributes, and noticeworthy events and interactions, identified by their categories and values.  This is only our own weak grasp on reality; the real universe doesn't look like that.  Even if a different mind saw a similar <em>kind</em> of exposed surface to the world, it would still see a <em>different</em> exposed surface.</p> <p>Sometimes human thought seems a lot like it tries to grasp the universe as... well, as this big XML file, <tt>AI.goal == smile</tt>, <tt>human.smile == yes</tt>, that sort of thing.  Yes, I <em>know</em> human world-models are more complicated than XML.  (And yes, I'm also aware that what I wrote looks more like Python than literal XML.)  But even so.</p> <p>What was the one thinking, who proposed an AI whose <a href="0459.html">behaviors would be reinforced by human smiles</a> [http://lesswrong.com/lw/td/magical_categories/], and who reacted with indignation to the idea that a superintelligence could "mistake" a tiny molecular smileyface for a "real" smile?  Probably something along the lines of, "But in this case, <tt>human.smile == 0</tt>, so how could a <em>superintelligence</em> possibly believe <tt>human.smile == 1</tt>?"</p> <p>For the weak grasp that our mind obtains on the high-level surface of reality, <a href="0284.html">seems to us like the very substance of the world itself</a> [http://lesswrong.com/lw/oi/mind_projection_fallacy/].</p> <p>Unless we make a conscious effort to think of <a href="0289.html">reductionism</a> [http://lesswrong.com/lw/on/reductionism/], and even then, it's not as if thinking "<a href="0291.html">Reductionism!</a> [http://lesswrong.com/lw/op/fake_reductionism/]" gives us a sudden apprehension of <a href="0379.html">quantum mechanics</a> [http://lesswrong.com/lw/r5/the_quantum_physics_sequence/].</p> <p>So if you have this, as it were, XML-like view of reality, then it's easy enough to think you can give the AI a goal of having no effects on the outside world; the "effects" are like discrete rays of <tt>effect</tt> leaving the AI, that result in noticeable events like killing a cat or something, and the AI doesn't want to do this, so it just switches the effect-rays off; and by the assumption of default independence, nothing else happens.</p> <p>Mind you, I'm not saying that you couldn't build an Oracle.  I'm saying that the problem of giving it a goal of "don't do anything to the outside world" "except by answering questions" "from the programmers" "the way the programmers meant them", in such fashion as to actually end up with an Oracle that works anything like the little XML-ish model in your head, is a big nontrivial Friendly AI problem.  The real world doesn't have little discreet effect-rays leaving the AI, and the real world doesn't have ontologically fundamental <tt>programmer.question</tt> objects, and "the way the programmers meant them" isn't a <a href="0458.html">natural category</a> [http://lesswrong.com/lw/tc/unnatural_categories/].</p> <p>And this is more important for dealing with superintelligences than rocks, because the superintelligences are going to parse up the world in a different way.  They may not perceive reality directly, but they'll still have the power to perceive it differently.  A superintelligence might not be able to tag every atom in the solar system, but it could tag every biological cell in the solar system (consider that each of your cells contains its own mitochondrial power engine and a complete copy of your DNA).  It used to be that human beings didn't even <em>know</em> they were made out of cells.  And if the universe is a bit more complicated than we think, perhaps the superintelligence we build will make a few discoveries, and then slice up the universe into parts we didn't know existed - to say nothing of us being able to model them in our own minds!  How does the instruction to "do the right thing" cross that kind of gap?</p> <p>There is no nontechnical solution to Friendly AI.</p> <p>That is:  There is no solution that operates on the level of qualitative physics and empathic models of agents.</p> <p>That's all just a dream in XML about a universe of quantum mechanics.  And maybe that dream works fine for manipulating rocks over a five-minute timespan; and sometimes okay for getting individual humans to do things; it often doesn't seem to give us much of a grasp on human societies, or planetary ecologies; and as for optimization processes more powerful than you are... it really isn't going to work.</p> <p>(Incidentally, the most epically silly example of this that I can recall seeing, was a proposal to (IIRC) keep the AI in a box and give it faked inputs to make it believe that it could punish its enemies, which would keep the AI satisfied and make it go on working for us.  Just some random guy with poor grammar on an email list, but still one of the most epic FAIls I recall seeing.) </p></div> <hr><p><i>Referenced by: </i><a href="0503.html">My Bayesian Enlightenment</a> &#8226; <a href="0507.html">Shut up and do the impossible!</a> &#8226; <a href="0517.html">Protected From Myself</a> &#8226; <a href="0519.html">Ethical Injunctions</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/tj/dreams_of_friendliness/">Dreams of Friendliness</a></p></body></html>