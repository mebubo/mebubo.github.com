<html><head><title>Singletons Rule OK</title></head><body><h1>Singletons Rule OK</h1><p><i>Eliezer Yudkowsky, 30 November 2008 04:45PM</i></p><div><p><strong>Reply to</strong>:  <a href="http://www.overcomingbias.com/2008/11/total-tech-wars.html">Total Tech Wars</a> [http://www.overcomingbias.com/2008/11/total-tech-wars.html]</p> <p>How <em>does</em> one end up with a persistent disagreement between two rationalist-wannabes who are both aware of Aumann's Agreement Theorem and its implications?</p> <p>Such a case is likely to turn around two axes: object-level incredulity ("no matter <em>what</em> AAT says, proposition X can't<em> really</em> be true") and meta-level distrust ("they're trying to be rational despite their emotional commitment, but are they really capable of that?").</p> <p>So far, Robin and I have focused on the object level in trying to hash out our disagreement.  Technically, I can't speak for Robin; but at least in my <em>own</em> case, I've acted thus because I anticipate that a meta-level argument about trustworthiness wouldn't lead anywhere interesting.  Behind the scenes, I'm doing what I can to make sure my brain is actually capable of updating, and presumably Robin is doing the same.</p> <p>(The linchpin of my own current effort in this area is to tell myself that I ought to be learning something while having this conversation, and that I shouldn't miss any scrap of original thought in it - the <a href="0069.html">Incremental Update </a> [http://lesswrong.com/lw/ij/update_yourself_incrementally/]technique. Because I can genuinely believe that a conversation like this should produce new thoughts, I can turn that feeling into genuine attentiveness.)</p> <p>Yesterday, Robin inveighed hard against what he called "total tech wars", and what I call "winner-take-all" scenarios:</p><blockquote><p>Robin:  "If you believe the other side is totally committed to total victory, that surrender is unacceptable, and that all interactions are zero-sum, you may conclude your side must never cooperate with them, nor tolerate much internal dissent or luxury."</p></blockquote><p>Robin and I both have emotional commitments and we both acknowledge the danger of that.  There's <a href="0039.html">nothing irrational about feeling</a> [http://lesswrong.com/lw/hp/feeling_rational/], <em>per se;</em> only <em>failure to update</em> is blameworthy.  But Robin seems to be <em>very</em> strongly against winner-take-all technological scenarios, and I don't understand why.</p> <p>Among other things, I would like to ask if Robin has a <a href="0270.html">Line of Retreat</a> [http://lesswrong.com/lw/o4/leave_a_line_of_retreat/] set up here - if, regardless of how he estimates the <em>probabilities,</em> he can <em>visualize what he would do if</em> a winner-take-all scenario were true.</p><a id="more"></a><p>Yesterday Robin wrote:</p><blockquote><p><span id="comment-140838978-content">"Eliezer, if everything is at stake then 'winner take all' is 'total war'; it doesn't really matter if they shoot you or just starve you to death."</span></p></blockquote><p>We both have our emotional commitments, but I don't quite understand this reaction.</p> <p>First, to me it's obvious that a "winner-take-all" <em>technology</em> should be defined as one in which, <em>ceteris paribus,</em> a local entity tends to end up with the <em>option</em> of becoming one kind of <a href="http://www.nickbostrom.com/fut/singleton.html">Bostromian singleton</a> [http://www.nickbostrom.com/fut/singleton.html] - the decisionmaker of a global order in which there is a single decision-making entity at the highest level.  (A superintelligence with unshared nanotech would count as a singleton; a federated world government with its own military would be a different kind of singleton; or you can imagine something like a galactic operating system with a root account controllable by 80% majority vote of the populace, etcetera.)</p> <p>The winner-take-all <em>option</em> is created by properties of the technology landscape, which is not a moral stance.  Nothing is said about an agent with that <em>option,</em> <em>actually</em> becoming a singleton.  Nor about <em>using</em> that power to shoot people, or reuse their atoms for something else, or grab all resources and let them starve (though "all resources" should include their atoms anyway).</p> <p>Nothing is yet said about various patches that could try to avert a <em>technological</em> scenario that contains upward cliffs of progress - e.g. binding agreements enforced by source code examination or continuous monitoring, in advance of the event.  (Or if you think that rational agents <a href="0470.html">cooperate on the Prisoner's Dilemma</a> [http://lesswrong.com/lw/to/the_truly_iterated_prisoners_dilemma/], so much work might not be required to coordinate.)</p> <p>Superintelligent agents <em>not</em> in a humanish <a href="0443.html">moral reference frame</a> [http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/] - AIs that are just maximizing paperclips or <a href="0444.html">sorting pebbles</a> [http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/] - who happen on the option of becoming a Bostromian Singleton, and who have <em>not</em> previously executed any somehow-binding treaty; will <em>ceteris paribus</em> choose to grab all resources in service of their utility function, including the atoms now composing humanity.  I don't see how you could reasonably deny this!  It's a straightforward decision-theoretic choice between payoff 10 and payoff 1000!</p> <p>But conversely, there are <a href="0396.html">possible agents in mind design space</a> [http://lesswrong.com/lw/rm/the_design_space_of_mindsingeneral/] who, given the <em>option</em> of becoming a singleton, will <em>not</em> kill you, starve you, reprogram you, tell you how to live your life, or even meddle in your destiny unseen.  See <a href="http://www.nickbostrom.com/fut/singleton.html">Bostrom's (short) paper</a> [http://www.nickbostrom.com/fut/singleton.html] on the possibility of good and bad singletons of various types.</p> <p>If Robin thinks it's <em>impossible</em> to have a Friendly AI or maybe even any sort of benevolent superintelligence at all, even the descendants of human uploads - if Robin is assuming that superintelligent agents <em>will</em> act according to roughly selfish motives, and that <em>only</em> economies of trade are necessary and sufficient to prevent holocaust - then Robin may have no <a href="0270.html">Line of Retreat</a> [http://lesswrong.com/lw/o4/leave_a_line_of_retreat/] open, as I try to argue that AI has an upward cliff built in.</p> <p>And in this case, it might be time well spent, to first address the question of whether Friendly AI is a reasonable thing to try to accomplish, so as to create that line of retreat.  Robin and I are both trying hard to be rational despite emotional commitments; but there's no particular reason to <em>needlessly</em> place oneself in the position of trying to persuade, or trying to accept, that everything of value in the universe is certainly doomed.</p> <p>For me, it's particularly hard to understand Robin's position in this, because for me the <em>non-</em>singleton future is the one that is obviously abhorrent.</p> <p>If you have lots of entities with root permissions on matter, any of whom has the physical capability to attack any other, then you have entities spending huge amounts of precious negentropy on defense and deterrence.  If there's no centralized system of property rights in place for selling off the universe to the highest bidder, then you have a race to <a href="http://hanson.gmu.edu/filluniv.pdf">burn the cosmic commons</a> [http://hanson.gmu.edu/filluniv.pdf], and the degeneration of the vast majority of all agents into <a href="http://hanson.gmu.edu/hardscra.pdf">rapacious hardscrapple frontier</a> [http://hanson.gmu.edu/hardscra.pdf] replicators.</p> <p>To me this is a vision of <em>futility</em> - one in which a future light cone that <em>could</em> have been full of happy, safe agents having complex fun, is mostly wasted by agents trying to seize resources and defend them so they can send out seeds to seize more resources.</p> <p>And it should also be mentioned that any future in which slavery or child abuse is <em>successfully</em> prohibited, is a world that has <em>some</em> way of preventing agents from doing certain things with their computing power.  There are vastly worse possibilities than slavery or child abuse opened up by future technologies, which I flinch from referring to even as much as I did in the previous sentence.  There are things I don't want to happen to <em>anyone</em> - including a population of a septillion captive minds running on a star-powered Matrioshka Brain that is owned, and <em>defended</em> against all rescuers, by the mind-descendant of Lawrence Bittaker (serial killer, aka "Pliers").  I want to <em>win</em> against the horrors that exist in this world and the horrors that could exist in tomorrow's world - to have them never happen ever again, or, for the <em>really</em> awful stuff, never happen in the first place.  And that victory requires the Future to have certain <em>global</em> properties.</p> <p>But there are other ways to get singletons besides falling up a technological cliff.  So that would be my Line of Retreat:  If minds can't self-improve quickly enough to take over, then try for the path of uploads setting up a centralized Constitutional operating system with a root account controlled by majority vote, or something like that, to prevent their descendants from <em>having</em> to burn the cosmic commons.</p> <p>So for me, <em>any satisfactory outcome</em> seems to necessarily involve, if not a singleton, the existence of certain stable <em>global</em> properties upon the future - sufficient to <em>prevent</em> burning the cosmic commons, <em>prevent</em> life's degeneration into rapacious hardscrapple frontier replication, and <em>prevent</em> supersadists torturing septillions of helpless dolls in private, obscure star systems.</p> <p>Robin has written about burning the cosmic commons and rapacious hardscrapple frontier existences.  This doesn't imply that Robin approves of these outcomes.  But Robin's strong rejection even of winner-take-all <em>language</em> and <em>concepts,</em> seems to suggest that our emotional commitments are something like 180 degrees opposed.  Robin seems to feel the same way about singletons as I feel about &#194;&#377;singletons.</p> <p>But <em>why?</em>  I don't think our real values are that strongly opposed - though we may have verbally-described and attention-prioritized those values in different ways.</p></div> <hr><p><i>Referenced by: </i><a href="0593.html">Devil's Offers</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/wc/singletons_rule_ok/">Singletons Rule OK</a></p></body></html>