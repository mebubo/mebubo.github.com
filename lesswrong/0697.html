<html><head><title>Selecting Rationalist Groups</title></head><body><h1>Selecting Rationalist Groups</h1><p><i>Eliezer Yudkowsky, 02 April 2009 04:21PM</i></p><div><p><strong>Previously in series</strong>:  <a href="0695.html">Purchase Fuzzies and Utilons Separately</a> [http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/]<br><strong>Followup to</strong>:  <a href="0166.html">Conjuring an Evolution To Serve You</a> [http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/]</p> <p><a href="http://www.greythumb.org/blog/index.php?/archives/80-Eugenics-doesnt-work.-Ask-why,-asshole..html">GreyThumb.blog</a> [http://www.greythumb.org/blog/index.php?/archives/80-Eugenics-doesnt-work.-Ask-why,-asshole..html] offered an interesting comparison of poor animal breeding practices and the fall of Enron, which I previously posted on <a href="0166.html">in some detail</a> [http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/].  The essential theme was that <em>individual</em> selection on chickens for the chicken in each generation who laid the most eggs, produced <em>highly competitive</em> chickens&#8212;the most dominant chickens that pecked their way to the top of the pecking order at the expense of other chickens.  The chickens subjected to this individual selection for egg-laying prowess needed their beaks clipped, or housing in individual cages, or they would peck each other to death.</p> <p>Which is to say: <em>individual </em>selection is selecting on the wrong criterion, because what the farmer actually <em>wants</em> is high egg production from <em>groups</em> of chickens.</p> <p>While <a href="0154.html">group selection</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/] is nearly impossible in ordinary biology, it is easy to impose in the laboratory: and breeding the best <em>groups,</em> rather than the best <em>individuals,</em> increased average days of hen survival from 160 to 348, and egg mass per bird from 5.3 to 13.3 kg.</p> <p>The analogy being to the way that Enron evaluated its employees every year, fired the bottom 10%, and gave the top individual performers huge raises and bonuses.  Jeff Skilling fancied himself as exploiting the <a href="0150.html">wondrous power of evolution</a> [http://lesswrong.com/lw/ks/the_wonder_of_evolution/], it seems.</p> <p>If you look over my accumulated essays, you will observe that the art contained therein is almost entirely individual in nature... for around the same reason that it all focuses on confronting impossibly tricky questions:  That's what I was <em>doing</em> when I thought up all this stuff, and for the most part I worked in solitude.  But this is not inherent in the Art, <a href="0675.html">not reflective of what a true martial art of rationality would be like</a> [http://lesswrong.com/lw/2c/a_sense_that_more_is_possible/] if many people had contributed to its development along many facets.</p> <p>Case in point:  At <a href="http://lesswrong.com/lw/6f/bay_area_oblw_meetup_today_sunday_march_29_at_5pm/">the recent LW / OB meetup</a> [http://lesswrong.com/lw/6f/bay_area_oblw_meetup_today_sunday_march_29_at_5pm/], we played Paranoid Debating, a game that tests group rationality.  As is only appropriate, this game was not the invention of any single person, but was collectively thought up in a series of suggestions by <a href="http://www.overcomingbias.com/2007/01/a_game_for_self.html">Nick Bostrom</a> [http://www.overcomingbias.com/2007/01/a_game_for_self.html], <a href="http://www.acceleratingfuture.com/steven/?p=96">Black Belt Bayesian</a> [http://www.acceleratingfuture.com/steven/?p=96], <a href="http://www.acceleratingfuture.com/tom/?p=34">Tom McCabe</a> [http://www.acceleratingfuture.com/tom/?p=34], and <a href="0678.html">steven0461</a> [http://lesswrong.com/lw/2s/3_levels_of_rationality_verification/#284].<a id="more"></a></p> <p>In the game's final form, Robin Gane-McCalla asked us questions like "How many Rhode Islands would fit into Alaska?" and a group of (in this case) four rationalists tried to pool their knowledge and figure out the answer... except that before the round started, we each drew facedown from a set of four cards, containing one spade card and one red card.  Whoever drew the red card got the job of trying to mislead the group.  Whoever drew the spade showed the card and became the spokesperson, who had to select the final answer.  It was interesting, trying to play this game, and realizing how little I'd practiced basic skills like trying to measure the appropriateness of another's confidence or figure out who was lying.</p> <p>A bit further along, at the suggestion of Steve Rayhawk, and slightly simplified by myself, we named 60% confidence intervals for the quantity with lower and upper bounds; Steve fit a Cauchy distribution to the interval ("because it has a fatter tail than a Gaussian") and we were scored according to the log of our probability density on the true answer, except for the red-card drawer, who got the negative of this number.</p> <p>The Paranoid Debating game worked surprisingly well&#8212;at least <em>I</em> had fun, despite somehow managing to draw the red card three out of four times.  I can totally visualize doing this at some corporate training event or even at parties.  The red player is technically acting as an individual and learning to practice deception, but perhaps practicing deception (in this controlled, ethically approved setting) might help you be a little less gullible in turn.  As Zelazny observes, there is a difference in the arts of discovering lies and finding truth.</p> <p>In a real institution... you would probably want to optimize less for fun, and more for work-relevance: something more like Black Belt Bayesian's original suggestion of <a href="http://www.acceleratingfuture.com/steven/?p=96">The Aumann Game</a> [http://www.acceleratingfuture.com/steven/?p=96], no red cards.  But where both B<sup>3</sup> and <a href="http://www.acceleratingfuture.com/tom/?p=34">Tom McCabe</a> [http://www.acceleratingfuture.com/tom/?p=34] originally thought in terms of scoring individuals, I would suggest forming people into groups and scoring the groups.  An institution's performance is the sum of its groups more directly than it is the sum of its individuals&#8212;though of course there are interactions between groups as well.  Find people who, in general, seem to have a statistical tendency to belong to high-performing groups&#8212;these are the ones who contribute much to the group, who are persuasive with <em>good</em> arguments.</p> <p>I wonder if there are any hedge funds that practice "trio trading", by analogy with pair programming?</p> <p><a href="http://www.overcomingbias.com/2006/12/agreeing_to_agr.html">Hal Finney</a> [http://www.overcomingbias.com/2006/12/agreeing_to_agr.html] called Aumann's Agreement Theorem "the most interesting, surprising, and challenging result in the field of human bias: that mutually respectful, honest, and rational debaters cannot disagree on any factual matter once they know each other's opinions".  It is not just my own essays that are skewed toward individual application; the whole trope of Traditional Rationality seems to me skewed the same way.  It's the individual heretic who is the hero, and Authority the untrustworthy villain whose main job is not to resist the heretic too much, to be properly defeated.  Science is cast as a competition between theories in an arena with rules designed to let the strongest contender win.  Of course, it may be that I am selective in my memory, and that if I went back and read my childhood books again, I would notice more on group tactics that originally slipped my attention... but really, Aumann's Agreement Theorem doesn't get enough attention.</p> <p>Of course <em>most </em>Bayesian math is not widely known&#8212;the Agreement Theorem is no exception here.  But even the intuitively obvious counterpart of the Agreement Theorem, the treatment of others' beliefs as evidence, receives little shrift in Traditional Rationality.  This may have something to do with Science developing in the midst of insanity and in defiance of Authority; that is a historical fact about how Science developed.  But if the high performers of a rationality dojo need to practice the same sort of <a href="0205.html">lonely dissent</a> [http://lesswrong.com/lw/mb/lonely_dissent/]... well, that must not be a very effective rationality dojo.</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="0726.html"><em>The Craft and the Community</em></a> [http://lesswrong.com/lw/cz/the_craft_and_the_community/]</p> <p style="text-align:right">Next post: "<a href="0699.html">Incremental Progress and the Valley</a> [http://lesswrong.com/lw/7k/incremental_progress_and_the_valley/]"</p> <p style="text-align:right">Previous post: "<a href="0695.html">Purchase Fuzzies and Utilons Separately</a> [http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq16.html">Sequence 16: The Craft and the Community</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0695.html">Purchase Fuzzies and Utilons Separately</a></p></td><td><p><i>Next: </i><a href="0699.html">Incremental Progress and the Valley</a></p></td></tr></table><p><i>Referenced by: </i><a href="0695.html">Purchase Fuzzies and Utilons Separately</a> &#8226; <a href="0699.html">Incremental Progress and the Valley</a> &#8226; <a href="0704.html">Whining-Based Communities</a> &#8226; <a href="0726.html">The Craft and the Community</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/77/selecting_rationalist_groups/">Selecting Rationalist Groups</a></p></body></html>