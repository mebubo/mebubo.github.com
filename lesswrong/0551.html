<html><head><title>Failure By Analogy</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Failure By Analogy</h1><p><i>Eliezer Yudkowsky, 18 November 2008 02:54AM</i></p><div><p><strong>Previously in series</strong>:  <a href="0549.html">Logical or Connectionist AI?</a> [http://lesswrong.com/lw/vv/logical_or_connectionist_ai/]<br><strong>Followup to</strong>:  <a href="0393.html">Surface Analogies and Deep Causes</a> [http://lesswrong.com/lw/rj/surface_analogies_and_deep_causes/]</p> <blockquote><p>"One of [the Middle Ages'] characteristics was that 'reasoning by analogy' was rampant; another characteristic was almost total intellectual stagnation, and we now see why the two go together. A reason for mentioning this is to point out that, by developing a keen ear for unwarranted analogies, one can detect a lot of medieval thinking today."<br>  &#160;&#160; &#160;&#160; -- <a href="http://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html">Edsger W. Dijkstra</a> [http://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html]</p> <p>&lt;geoff&gt; neural nets are over-rated<br>&lt;starglider&gt; Their potential is overrated.<br>&lt;geoff&gt; their potential is us <br>  &#160;&#160; &#160;&#160; -- #sl4</p></blockquote><p>Wasn't it in some sense reasonable to have high hopes of neural networks?  After all, they're <a href="0461.html">just like the human brain</a> [http://lesswrong.com/lw/tf/dreams_of_ai_design/], which is also massively parallel, distributed, asynchronous, and -</p> <p>Hold on.  Why not analogize to an earthworm's brain, instead of a human's?</p><a id="more"></a><p>A backprop network with sigmoid units... actually doesn't much resemble biology <em>at all</em>.  Around as much as a voodoo doll resembles its victim.  The surface shape may look vaguely similar in extremely superficial aspects at a first glance.  But the interiors and behaviors, and basically the whole thing apart from the surface, are nothing <em>at all</em> alike.  All that biological neurons have in common with gradient-optimization ANNs is... the spiderwebby look.</p> <p> And who says that the spiderwebby look is the <em>important</em> fact about biology?  Maybe the performance of biological brains has nothing to do with being made <em>out of neurons</em>, and everything to do with the <a href="0164.html">cumulative selection pressure</a> [http://lesswrong.com/lw/l6/no_evolutions_for_corporations_or_nanodevices/] put into the design.  Just like how the performance of biological brains has little to do with proteins being held together by van der Waals forces, instead of the much stronger covalent bonds that hold together silicon.  Sometimes evolution gets stuck with poor building material, and it can't refactor because it can't execute simultaneous changes to migrate the design.  If biology does some neat tricks with chemistry, it's because of the greater design pressures exerted by natural selection, not because the building materials are so wonderful.</p> <p>Maybe neurons are just what brains <em>happen</em> to be made out of, because the <a href="0149.html">blind idiot god</a> [http://lesswrong.com/lw/kr/an_alien_god/] is <a href="0151.html">too stupid</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/] to sit down and invent transistors.  All the modules get made out of neurons because that's all there <em>is,</em> even if the cognitive work would be much better-suited to a 2GHz CPU.</p> <blockquote><p>"Early attempts to make flying machines often did things like attaching beak onto the front, or trying to make a wing which would flap like a bird's wing.  (This extraordinary persistent idea is found in Leonardo's notebooks and in a textbook on airplane design published in 1911.)  It is easy for us to smile at such naivete, but one should realize that it made good sense at the time.  What birds did was incredible, and nobody really knew how they did it.  It always seemed to involve feathers and flapping.  Maybe the beak was critical for stability..."<br>  &#160;&#160; &#160;&#160; -- Hayes and Ford, "Turing Test Considered Harmful"</p></blockquote> <p>So... why <em>didn't</em> the flapping-wing designs work?  Birds flap wings and they fly.  The flying machine flaps its wings.  <em>Why, oh why, doesn't it fly?</em></p> <p>Because... well... it just doesn't.  This kind of analogic reasoning is not binding on Reality.</p> <p>One of the basic tests to apply to reasoning that sounds kinda-good is "How shocked can I justifiably be if Reality comes back and says 'So what'?"</p> <p>For example:  Suppose that, after keeping track of the motions of the planets for centuries, and after confirming the underlying theory (General Relativity) to 14 decimal places, we predict where Mercury will be on July 1st, 2009.  So we have a prediction, but that's not the same thing as a fact, right?  Anyway, we look up in the sky on July 1st, 2009, and Reality says "So what!" - the planet Mercury has shifted outward to the same orbit as Mars.</p> <p>In a case like this, I would be highly indignant and would probably sue Reality for breach of contract.</p> <p>But suppose alternatively that, in the last twenty years, real estate prices have never gone down.  You say, "Real estate prices have never gone down - therefore, they won't go down next year!"  And next year, Reality says "So what?"  It seems to me that you have no right to be shocked.  You have used an argument to which Reality can easily say "So what?" </p><blockquote><p>"Nature is the ultimate bigot, because it is obstinately and intolerantly devoted to its own prejudices and absolutely refuses to yield to the most persuasive rationalizations of humans."<br>  &#160;&#160; &#160;&#160; -- J. R. Molloy</p></blockquote> <p>It's actually pretty hard to find arguments <em>so</em> persuasive that <em>even Reality</em> finds them binding.  This is why Science is more difficult - why it's harder to successfully predict reality - than medieval scholars once thought.</p> <p>One class of persuasive arguments that Reality quite often ignores is the Law of Similarity - that is, the argument that things which look similar ought to behave similarly.</p> <p>A medieval alchemist puts lemon glazing onto a lump of lead.  The lemon glazing is yellow, and gold is yellow.  It seems like it <em>ought</em> to work... but the lead obstinately refuses to turn into gold.  Reality just comes back and says, "So what?  Things can be similar in some aspects without being similar in other aspects."</p> <p>You should be <em>especially</em> suspicious when someone says, "I am building X, which will do P, because it is similar to Y, which also does P."</p> <p>An abacus performs addition; and the beads of solder on a circuit board bear a certain surface resemblance to the beads on an abacus. Nonetheless, <a href="0393.html">the circuit board does not perform addition <em>because</em> we can find a surface similarity to the abacus</a> [http://lesswrong.com/lw/rj/surface_analogies_and_deep_causes/]. The Law of Similarity and Contagion is not relevant.   The circuit board would work in just the same fashion if every abacus upon Earth vanished in a puff of smoke, or if the beads of an abacus looked nothing like solder.  A computer chip is not powered by its <em>similarity</em> to anything else, it just <em>is.</em>  It exists in its own right, for its own reasons.</p> <p>The Wright Brothers calculated that their plane would fly - before it ever flew - using reasoning that took no account whatsoever of their aircraft's similarity to a bird.  They did look at birds (and I have looked at neuroscience) but the final calculations did not mention birds (I am fairly confident in asserting).  A working airplane does not fly <em>because</em> it has wings "just like a bird".  An airplane flies because it is an airplane, a thing that exists in its own right; and it would fly just as high, no more and no less, if no bird had ever existed.</p> <p>The general form of failing-by-analogy runs something like this:</p> <ul><li>You want property P.</li> <li>X has property P.</li> <li>You build Y, which has one or two surface similarities S to X.</li> <li>You argue that Y resembles X and should also P.</li> <li>Yet there is no reasoning which you can do on Y as a thing-in-itself to show that it will have property P, regardless of whether or not X had ever existed.</li></ul> <p>Analogical reasoning of this type is a very <em>weak</em> form of understanding.  Reality often says "So what?" and ignores the argument.</p> <p>The one comes to us and says:  "Calculate how many synaptic transmissions per second take place in the human brain.  This is the computing power required for human-equivalent intelligence.  Raise enough venture capital to buy a supercomputer which performs the same number of floating-point operations per second.  Intelligence is bound to emerge from a machine that powerful."</p> <p>So you reply:  "I'm sorry, I've never seen a human brain and I don't know anything about them.  So, <em>without</em> talking about a human brain, can you explain how you calculated that 10^17 floating-point operations per second is the exact amount necessary and sufficient to yield human-equivalent intelligence?" </p> <p>And the one says:  "..." </p> <p>You ask:  "Say, what is this property of 'human-equivalent intelligence' which you expect to get?  Can you explain it to me without pointing to a human?" </p> <p>And the one says:  "..." </p> <p>You ask:  "What makes you think that large amounts of computing power have something to do with 'intelligence', anyway?  Can you answer without pointing to the example of the human brain?  Pretend that I've never seen an 'intelligence' and that I have no reason as yet to believe any such thing can exist." </p> <p>But you get the idea.</p> <p>Now imagine that you go to the Wright Brothers and say:  "I've never seen a bird.  Why does your aircraft have 'wings'?  And what is it you mean by 'flying'?"</p> <p>And the Wright Brothers respond:  "Well, by flying, we mean that this big heavy object is going to rise off the ground and move through the air without being supported.  Once the plane is moving forward, <a href="http://jef.raskincenter.org/published/coanda_effect.html">the wings accelerate air downward, which generates lift that keeps the plane aloft</a> [http://jef.raskincenter.org/published/coanda_effect.html]."</p> <p>If two processes have forms that are nearly <em>identical</em>, including internal structure that is similar to as many decimal places as you care to reason about, then you may be able to almost-prove results from one to the other.  But if there is even one difference in the internal structure, then any number of other similarities may be rendered void.  Two deterministic computations with identical data and identical rules <em>will</em> yield identical outputs.  But if a single input bit is flipped from zero to one, the outputs are no longer required to have <em>anything</em> in common.  The strength of analogical reasoning can be destroyed by a single perturbation. </p> <p>Yes, <em>sometimes</em> analogy works.  But the more complex and dissimilar the objects are, the less likely it is to work.  The narrower the conditions required for success, the less likely it is to work.  The more complex the machinery doing the job, the less likely it is to work.  The more shallow your understanding of the object of the analogy, the more you are looking at its surface characteristics rather than its deep mechanisms, the less likely analogy is to work. </p> <p>Analogy might work for something on the order of:  "I crossed this river using a fallen log last time, so if I push another log across it, I might be able to cross it again."  It doesn't work for creating objects of the order of complexity of, say, a toaster oven.  (And hunter-gatherer bands face many rivers to cross, but not many toaster ovens to rewire.) </p> <p>Admittedly, analogy often works in mathematics - much <em>better</em> than it does in science, in fact.  In mathematics you can go back and <em>prove</em> the idea which analogy originally suggested.  In mathematics, you get quick feedback about which analogies worked and which analogies didn't, and soon you pick up the pattern.  And in mathematics you can always see the entire insides of things; you are not stuck examining the surface of an opaque mystery.  Mathematical proposition A may be analogous to mathematical proposition B, which suggests the method; but afterward you can go back and prove A in its own right, regardless of whether or not B is true.  In some cases you may need proposition B as a lemma, but certainly not all cases. </p> <p>Which is to say: despite the <em>misleading surface similarity</em>, the "analogies" which mathematicians use are not analogous to the "analogies" of alchemists, and you cannot reason from the success of one to the success of the other. </p></div> <hr><p><i>Referenced by: </i><a href="0552.html">Failure By Affective Analogy</a> &#8226; <a href="0574.html">Artificial Mysterious Intelligence</a> &#8226; <a href="0576.html">Disjunctions, Antipredictions, Etc.</a> &#8226; <a href="0616.html">Getting Nearer</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/vx/failure_by_analogy/">Failure By Analogy</a></p></body></html>