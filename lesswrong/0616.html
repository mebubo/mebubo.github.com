<html><head><title>Getting Nearer</title></head><body><h1>Getting Nearer</h1><p><i>Eliezer Yudkowsky, 17 January 2009 09:28AM</i></p><div><p><strong>Reply to</strong>:  <a href="http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html">A Tale Of Two Tradeoffs</a> [http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html]</p><p>I'm not comfortable with compliments of the direct, personal sort, the "Oh, you're such a nice person!" type stuff that nice people are able to say with a straight face.  Even if it would make people like me more - even if it's socially expected - I have trouble bringing myself to do it.  So, when I say that I read Robin Hanson's "Tale of Two Tradeoffs", and then realized I would spend the rest of my mortal existence typing thought processes as "Near" or "Far", I hope this statement is received as a due substitute for any gushing compliments that a normal person would give at this point.</p><p>Among other things, this clears up a major puzzle that's been lingering in the back of my mind for a while now.  Growing up as a rationalist, I was always telling myself to "Visualize!" or "Reason by simulation, <a href="0551.html">not by analogy</a> [http://lesswrong.com/lw/vx/failure_by_analogy/]!" or "Use causal models, not similarity groups!"  And those who ignored this principle seemed easy prey to <a href="0180.html">blind enthusiasms</a> [http://lesswrong.com/lw/lm/affective_death_spirals/], wherein one says that <a href="0552.html">A is good because it is like B which is also good</a> [http://lesswrong.com/lw/vy/failure_by_affective_analogy/], and the like.</p><p>But later, I learned about the <a href="0102.html">Outside View versus the Inside View</a> [http://lesswrong.com/lw/jg/planning_fallacy/], and that people asking "What rough class does this project fit into, and when did projects like this finish last time?" were much more accurate and much less optimistic than people who tried to visualize the when, where, and how of their projects.  And this didn't seem to fit very well with my injunction to "Visualize!"</p><p>So <em>now</em> I think I understand what this principle was actually doing - it was keeping me in Near-side mode and away from Far-side thinking.  And it's not that Near-side mode works so well in any absolute sense, but that Far-side mode is so much more pushed-on by ideology and wishful thinking, and so <em>casual</em> in accepting its conclusions (devoting less computing power before halting).</p><a id="more"></a> <p>An example of this might be the balance between offensive and defensive nanotechnology, where I started out by - basically - just <em>liking</em> nanotechnology; until I got involved in a discussion about the particulars of nanowarfare, and noticed that people were postulating crazy things to make defense win.  Which made me realize and say, "Look, the balance between offense and defense has been tilted toward offense ever since the invention of nuclear weapons, and military nanotech could <em>use</em> nuclear weapons, and I don't see how you're going to build a molecular barricade against <em>that</em>."</p><p>Are the particulars of that discussion likely to be, well, <em>correct?</em>  Maybe not.  But so long as I wasn't thinking of <em>any</em> particulars, my brain had free reign to just... import whatever affective valence the word "nanotechnology" had, and use that as a snap judgment of everything.</p><p>You can still be biased about particulars, of course.  You can insist that nanotech couldn't possibly be radiation-hardened enough to manipulate U-235, which someone tried as a response (<em>fyi: this is extremely silly</em>).  But in my case, at least, something about <em>thinking in particulars</em>...</p><p>...just snapped me out of the trance, somehow.</p><p>When you're thinking using very abstract categories - rough classes low on computing power - about things distant from you, then you're also - if Robin's hypothesis is correct - more subject to ideological bias.  Together this implies you can cherry-pick those very loose categories to put X together with whatever "similar" Y is ideologically convenient, as in the old saw that "<a href="0300.html">atheism is a religion</a> [http://lesswrong.com/lw/oy/is_humanism_a_religionsubstitute/]" (and not playing tennis is a sport).</p><p>But the most frustrating part of all, is the <em>casualness</em> of it - the way that ideologically convenient Far thinking is just thrown together out of whatever ingredients come to hand.  The <a href="http://www.overcomingbias.com/2008/12/we-agree-get-froze.html">ten-second dismissal of cryonics</a> [http://www.overcomingbias.com/2008/12/we-agree-get-froze.html], without any attempt to visualize how much information is preserved by vitrification and could be retrieved by a molecular-level scan.  Cryonics just gets casually, perceptually classified as "not scientifically verified" and tossed out the window.  Or "what if you wake up in Dystopia?" and tossed out the window.  Far thinking is <em>casual</em> - that's the most frustrating aspect about trying to argue with it. </p><p>This seems like an argument for writing fiction with lots of concrete details <em>if you want people to take a subject seriously and think about it in a less biased way</em>.  This is not something I would have thought based on <a href="0131.html">my previous view</a> [http://lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/].</p><p>Maybe cryonics advocates really <em>should</em> focus on writing fiction stories that turn on the gory details of cryonics, or viscerally depict the regret of someone who didn't persuade their mother to sign up.  (Or offering prizes to professionals who do the same; writing fiction is hard, writing SF is harder.)</p><p>But I'm worried that, for whatever reason, reading concrete fiction is a special case that <em>doesn't work</em> to get people to do Near-side thinking.</p><p>Or there are <em>some </em>people who are inspired to Near-side thinking by fiction, and only these can actually be helped by reading science fiction.</p><p>Maybe there are people who encounter big concrete detailed fictions process them in a Near way - the sort of people who notice plot holes.  And others who just "take it all in stride", casually, so that however much concrete fictional "information" they encounter, they only process it using casual "Far" thinking.  I wonder if this difference has more to do with upbringing or genetics.  Either way, it may lie at the core of the partial yet statistically outstanding correlation between careful futurists and science fiction fans.</p><p>I expect I shall be thinking about this for a while.</p></div> <hr><p><i>Referenced by: </i><a href="0641.html">(Moral) Truth in Fiction?</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/xq/getting_nearer/">Getting Nearer</a></p></body></html>