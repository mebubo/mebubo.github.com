<html><head><title>Visualizing Eutopia</title></head><body><h1>Visualizing Eutopia</h1><p><i>Eliezer Yudkowsky, 16 December 2008 06:39PM</i></p><div><p><strong>Followup to</strong>:  <a href="0583.html">Not Taking Over the World</a> [http://lesswrong.com/lw/wt/not_taking_over_the_world/]</p> <p style="margin-left: 40px;">"Heaven is a city 15,000 miles square or 6,000 miles around. One side is 245 miles longer than the length of the Great Wall of China. Walls surrounding Heaven are 396,000 times higher than the Great Wall of China and eight times as thick. Heaven has twelve gates, three on each side, and has room for 100,000,000,000 souls. There are no slums. The entire city is built of diamond material, and the streets are paved with gold. All inhabitants are honest and there are no locks, no courts, and no policemen."<br>  -- Reverend Doctor George Hawes, in a sermon</p> <p>Yesterday I asked my esteemed co-blogger Robin what he would do with "<a href="0583.html">unlimited power</a> [http://lesswrong.com/lw/wt/not_taking_over_the_world/]", in order to reveal something of his character.  Robin said that he would (a) be very careful and (b) ask for advice.  I asked him what advice <em>he </em>would give himself.  Robin said it was a difficult question and he wanted to wait on considering it until it actually happened.  So overall he ran away from the question like a startled squirrel.</p> <p>The character thus revealed is a virtuous one: it shows common sense.  A lot of people jump after the prospect of absolute power like it was a coin they found in the street.</p> <p>When you think about it, though, it says a lot about human nature that this is a difficult question.  I mean - <em>most </em>agents with utility functions shouldn't have such a hard time describing their perfect universe.</p> <p>For a long time, I too ran away from the question like a startled squirrel.  First <a href="0484.html">I claimed that superintelligences would inevitably do what was right</a> [http://lesswrong.com/lw/u2/the_sheer_folly_of_callow_youth/], relinquishing moral responsibility in toto.  After that, I propounded various schemes to <em>shape </em>a nice superintelligence, and let <em>it</em> decide what should be done with the world.</p> <p>Not that there's anything wrong with that.  Indeed, this is <a href="http://intelligence.org/upload/CEV.html">still the plan</a> [http://intelligence.org/upload/CEV.html].  But it still meant that I, personally, was ducking the question.</p> <p>Why?  Because I expected to fail at answering.  Because I thought that any attempt for humans to visualize a better future was going to end up recapitulating the Reverend Doctor George Hawes: apes thinking, "Boy, if I had <em>human intelligence</em> I sure could get a lot more bananas."</p> <p><a id="more"></a></p> <p>But trying to get a <em>better</em> answer to a question out of a superintelligence, is a different matter from entirely ducking the question yourself.  The point at which I stopped ducking was the point at which I realized that it's actually quite difficult to get a good answer to something out of a superintelligence, while simultaneously having literally <em>no</em> idea how to answer yourself.</p> <p>When you're dealing with <em>confusing and difficult</em> questions - as opposed to those that are straightforward but numerically tedious - it's quite suspicious to have, on the one hand, a procedure that executes to reliably answer the question, and, on the other hand, no idea of how to answer it yourself.</p> <p>If you could write a computer program that you knew would reliably output a satisfactory answer to "Why does anything exist in the first place?" or "Why do I find myself in a universe giving rise to experiences that are ordered rather than chaotic?", then shouldn't you be able to at least try executing the same procedure yourself?</p> <p>I suppose there could be some section of the procedure where you've got to do a septillion operations and so you've just got no choice but to wait for superintelligence, but really, that sounds rather suspicious in cases like these.</p> <p>So it's not that I'm planning to use the output of my own intelligence to take over the universe.  But I did realize at some point that it was too suspicious to entirely duck the question while trying to make a computer knowably solve it.  It didn't even seem all that <em>morally cautious</em>, once I put in those terms.  You <em>can </em>design an arithmetic chip using purely abstract reasoning, but would you be <em>wise </em>to never try an arithmetic problem yourself?</p> <p>And when I did finally try - well, that caused me to update in various ways.</p> <p>It does make a difference to <em>try </em>doing arithmetic yourself, instead of just trying to design chips that do it for you.  So I found.</p> <p>Hence my bugging Robin about it.</p> <p>For it seems to me that Robin asks too little of the future.  It's all very well to plead that you are only forecasting, but if you display greater revulsion to the idea of a Friendly AI than to the idea of rapacious hardscrapple frontier folk...</p> <p>I thought that Robin might be asking too little, due to not visualizing <em>any</em> future in enough detail.  Not <em>the</em> future but <em>any</em> future.  I'd hoped that if Robin had allowed himself to visualize his "perfect future" in more detail, rather than focusing on all the compromises he thinks he has to make, he might see that there were futures more desirable than the rapacious hardscrapple frontier folk.</p> <p>It's hard to see on an emotional level why a genie might be a good thing to have, if you haven't acknowledged any wishes that need granting.  It's like not feeling the temptation of cryonics, if you haven't thought of anything the Future contains that might be worth seeing.</p> <p>I'd also hoped to persuade Robin, if his wishes were <em>complicated</em> enough, that there were attainable good futures that could not come about by letting things go their own way.  So that he might begin to see the future as I do, as a dilemma between extremes:  The default, loss of control, followed by a Null future containing little or no utility.  Versus extremely precise steering through "impossible" problems to get to any sort of Good future whatsoever.</p> <p>This is mostly a matter of appreciating how even the desires we call "simple" actually contain many bits of information.  Getting past <a href="0439.html">anthropomorphic optimism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/], to realize that a Future not strongly steered by our utility functions is likely to contain little or no utility, for the same reason it's hard to hit a distant target while shooting blindfolded...</p> <p>But if your "desired future" remains mostly unspecified, <em>that </em>may encourage too much optimism as well.</p></div> <hr><p><i>Referenced by: </i><a href="0585.html">Prolegomena to a Theory of Fun</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/wu/visualizing_eutopia/">Visualizing Eutopia</a></p></body></html>