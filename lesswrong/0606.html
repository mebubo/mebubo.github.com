<html><head><title>Emotional Involvement</title></head><body><h1>Emotional Involvement</h1><p><i>Eliezer Yudkowsky, 06 January 2009 10:23PM</i></p><div><p><strong>Followup to</strong>:  <a href="0159.html">Evolutionary Psychology</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/], <a href="0161.html">Thou Art Godshatter</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/], <a href="0422.html">Existential Angst Factory</a> [http://lesswrong.com/lw/sc/existential_angst_factory/]</p> <p>Can your emotions get involved in a video game?  Yes, but not much.  Whatever sympathetic echo of triumph you experience on destroying the Evil Empire in a video game, it's probably not remotely close to the feeling of triumph you'd get from saving the world in real life.  I've played video games powerful enough to bring tears to my eyes, but they still aren't as powerful as the feeling of significantly helping just one single real human being.</p> <p>Because when the video game is finished, and you put it away, the events within the game have no long-term consequences.</p> <p>Maybe if you had a major epiphany while playing...  But even then, only your <em>thoughts </em>would matter; the mere fact that you <em>saved the world</em>, inside the game, wouldn't count toward anything in the continuing story of your life.</p> <p>Thus fails the <a href="http://www.k-1.com/Orwell/site/work/essays/fun.html">Utopia</a> [http://www.k-1.com/Orwell/site/work/essays/fun.html] of <a href="0586.html">playing lots of really cool video games forever</a> [http://lesswrong.com/lw/ww/high_challenge/].  Even if the games are <a href="0586.html">difficult</a> [http://lesswrong.com/lw/ww/high_challenge/], <a href="0587.html">novel</a> [http://lesswrong.com/lw/wx/complex_novelty/], and <a href="0588.html">sensual</a> [http://lesswrong.com/lw/wy/sensual_experience/], this is still the idiom of life chopped up into a series of disconnected episodes with no lasting consequences.  A life in which equality of consequences is forcefully ensured, or in which little is at stake because all desires are instantly fulfilled without individual work&#8212;these likewise will appear as flawed Utopias of dispassion and angst.  "Rich people with nothing to do" syndrome.  A life of disconnected episodes and unimportant consequences is a life of weak passions, of emotional uninvolvement.</p> <p>Our emotions, for all the <a href="0159.html">obvious evolutionary reasons</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/], tend to associate to events that had major reproductive consequences in the ancestral environment, and to invoke the strongest passions for events with the biggest consequences:</p> <p>Falling in love... birthing a child... finding food when you're starving... getting wounded... being chased by a tiger... your child being chased by a tiger... finally killing a hated enemy...</p> <p><a id="more"></a></p> <p>Our life stories are not now, and will not be, what they once were.</p> <p>If one is to be conservative in the short run about changing minds, then we can get at least <em>some</em> mileage from changing the environment.  A windowless office filled with highly repetitive non-novel challenges isn't any more conducive to emotional involvement than video games; it may be part of real life, but it's a very<em> flat</em> part.  The occasional exciting global economic crash that you had no personal control over, does not particularly modify this observation.</p> <p>But we don't want to go back to the <em>original</em> savanna, the one where you got a leg chewed off and then starved to death once you couldn't walk.  There are things we care about tremendously in the sense of hating them so much that we want to drive their frequency down to zero, not by the most interesting way, just as quickly as possible, whatever the means.  If you drive the thing it binds to down to zero, where is the emotion after that?</p> <p>And there are emotions we might want to think twice about keeping, in the long run.  Does racial prejudice accomplish <em>anything</em> worthwhile?  I pick this as a target, not because it's a convenient whipping boy, but because unlike e.g. "boredom" it's actually pretty hard to think of a reason transhumans would want to keep this neural circuitry around.  Readers who take this as a challenge are strongly advised to remember that <a href="http://yudkowsky.net/singularity/simplified">the point of the question</a> [http://yudkowsky.net/singularity/simplified] is not to <a href="http://www.overcomingbias.com/2008/12/showoff-bias.html">show off how clever and counterintuitive you can be</a> [http://www.overcomingbias.com/2008/12/showoff-bias.html].</p> <p>But if you lose emotions <em>without replacing</em> them, whether by changing minds, or by changing life stories, then the world gets a little less <em>involving</em> each time; there's that much less material for passion.  And your mind and your life become that much <em>simpler,</em> perhaps, because there are fewer forces at work&#8212;maybe even threatening to collapse you into an expected pleasure maximizer.  <em>If</em> you don't replace what is removed.</p> <p>In the long run, if humankind is to make a new life for itself...</p> <p>We, and our descendants, will need some <a href="0604.html">new emotions</a> [http://lesswrong.com/lw/xe/changing_emotions/].</p> <p>This is the aspect of self-modification in which one must above all take care&#8212;modifying your goals.  Whatever you <em>want,</em> becomes more likely to <em>happen;</em> to ask what we ought to make ourselves want, is to ask what the future should <em>be.</em></p> <p>Add emotions at random&#8212;bind positive reinforcers or negative reinforcers to random situations and ways the world could be&#8212;and you'll just end up <a href="0444.html">doing what is prime instead of what is good</a> [http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/].  So adding a bunch of random emotions does not seem like the way to go.</p> <p>Asking what happens <em>often</em>, and binding happy emotions to that, so as to increase happiness&#8212;or asking what seems <em>easy,</em> and binding happy emotions to that&#8212;making isolated video games artificially more <em>emotionally involving,</em> for example&#8212;</p> <p>At that point, it seems to me, you've pretty much given up on eudaimonia and moved to maximizing happiness; you might as well <a href="0586.html">replace brains with pleasure centers</a> [http://lesswrong.com/lw/ww/high_challenge/], and civilizations with hedonium plasma.</p> <p>I'd suggest, rather, that one start with the idea of new major events in a transhuman life, and then bind emotions to those major events and the sub-events that surround them.  What sort of major events might a transhuman life embrace?  Well, this is the point at which I usually stop speculating.  "Science!  They should be excited by science!" is something of a bit-too-obvious and I dare say "nerdy" answer, as is "Math!" or "Money!"  (Money is just <a href="0531.html">our civilization's equivalent of expected utilon balancing</a> [http://lesswrong.com/lw/vd/intelligence_in_economics/] anyway.)  <em>Creating</em> a child&#8212;as in my favored saying, "If you can't design an intelligent being from scratch, you're not old enough to have kids"&#8212;is one candidate for a major transhuman life event, and anything you had to do along the way to creating a child would be a candidate for new emotions.  This might or might not have anything to do with sex&#8212;though I find that thought appealing, being something of a traditionalist.  All sorts of interpersonal emotions carry over for as far as my own human eyes can see&#8212;the joy of making allies, say; interpersonal emotions get more complex (and challenging) along with the people, which makes them an even richer source of future fun.  Falling in love?  Well, it's not as if we're trying to construct the Future out of anything other than our preferences&#8212;so do you <em>want</em> that to carry over?</p> <p>But again&#8212;this is usually the point at which I stop speculating.  It's hard enough to visualize human Eutopias, let alone transhuman ones.</p> <p>The essential idiom I'm suggesting is something akin to how evolution gave humans <a href="0161.html">lots of local reinforcers</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/] for things that <em>in the ancestral environment</em> related to evolution's overarching goal of inclusive reproductive fitness.  Today, office work might be highly relevant to someone's sustenance, but&#8212;even leaving aside the lack of <a href="0586.html">high challenge</a> [http://lesswrong.com/lw/ww/high_challenge/] and <a href="0587.html">complex novelty</a> [http://lesswrong.com/lw/wx/complex_novelty/]&#8212;and that it's not <a href="0588.html">sensually involving</a> [http://lesswrong.com/lw/wy/sensual_experience/] because we don't have native brainware to support the domain&#8212;office work is not <em>emotionally </em>involving because office work wasn't <em>ancestrally</em> relevant.  If office work had been around for millions of years, we'd find it a little less hateful, and experience a little more triumph on filling out a form, one suspects.</p> <p>Now you might run away shrieking from the dystopia I've just depicted&#8212;but that's because you don't see office work as eudaimonic in the first place, one suspects.  And because of the lack of high challenge and complex novelty involved.  In an "absolute" sense, office work would seem somewhat <em>less</em> tedious than gathering fruits and eating them.</p> <p>But the idea isn't necessarily to have fun doing office work.  Just like it's not necessarily the idea to have your emotions activate for video games instead of real life.</p> <p>The idea is that once you construct an existence / life story that seems to make sense, then it's all right to bind emotions to the parts of that story, with strength proportional to their long-term impact.  The anomie of today's world, where we simultaneously (a) engage in office work and (b) lack any passion in it, does not need to carry over: you should either fix one of those problems, or the other.</p> <p>On a higher, more abstract level, this carries over the idiom of <a href="0160.html">reinforcement over instrumental correlates of terminal values</a> [http://lesswrong.com/lw/l2/protein_reinforcement_and_dna_consequentialism/].  In principle, this is something that a purer optimization process wouldn't do.  You need neither happiness nor sadness to maximize expected utility.  You only need to know which actions result in which consequences, and update that pure probability distribution as you learn through observation; something akin to "reinforcement" falls out of this, but without <a href="0172.html">the risk of losing purposes</a> [http://lesswrong.com/lw/le/lost_purposes/], without any pleasure or pain.  An agent like this is simpler than a human and more powerful&#8212;if you think that your emotions give you a supernatural advantage in optimization, you've entirely failed to understand the math of this domain.  For a pure optimizer, the "advantage" of starting out with one more emotion bound to instrumental events is like being told one more abstract belief about which policies maximize expected utility, except that the belief is very hard to update based on further experience.</p> <p>But it does not seem to me, that a mind which <em>has</em> the most value, is the same kind of mind that most <em>efficiently optimizes</em> values outside it.  The interior of a true expected utility maximizer might be pretty boring, and I even suspect that you can <a href="0597.html">build them to not be sentient</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/].</p> <p>For as far as my human eyes can see, I don't know what kind of mind I <em>should </em>value, if that mind lacks pleasure and happiness and emotion in the everyday events of its life.  Bearing in mind that we are constructing this Future using our own preferences, not having it handed to us by some inscrutable external author.</p> <p>If there's some better way of <em>being</em> (not just <em>doing</em>) that stands somewhere outside this, I have not yet understood it well enough to <em>prefer </em>it.  But if so, then all this discussion of emotion would be as moot as it would be for an expected utility maximizer&#8212;one which was not valued at all for itself, but only valued for that which it maximized.</p> <p>It's just hard to see why we would <em>want</em> to become something like that, bearing in mind that morality is not an <a href="0408.html">inscrutable light</a> [http://lesswrong.com/lw/ry/is_morality_given/] handing down <a href="0401.html">awful edicts</a> [http://lesswrong.com/lw/rr/the_moral_void/] from <a href="0421.html">somewhere outside us</a> [http://lesswrong.com/lw/sb/could_anything_be_right/].</p> <p>At any rate&#8212;the hell of a life of disconnected episodes, where your actions don't connect strongly to anything you strongly care about, and nothing that you do all day invokes any passion&#8212;this <a href="0422.html">angst </a> [http://lesswrong.com/lw/sc/existential_angst_factory/]seems avertible, however often it pops up in poorly written Utopias.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0608.html">Serious Stories</a> [http://lesswrong.com/lw/xi/serious_stories/]"</p> <p style="text-align:right">Previous post: "<a href="0604.html">Changing Emotions</a> [http://lesswrong.com/lw/xe/changing_emotions/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0604.html">Changing Emotions</a></p></td><td><p><i>Next: </i><a href="0608.html">Serious Stories</a></p></td></tr></table><p><i>Referenced by: </i><a href="0604.html">Changing Emotions</a> &#8226; <a href="0608.html">Serious Stories</a> &#8226; <a href="0611.html">Eutopia is Scary</a> &#8226; <a href="0614.html">Justified Expectation of Pleasant Surprises</a> &#8226; <a href="0622.html">Higher Purpose</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0626.html">31 Laws of Fun</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/xg/emotional_involvement/">Emotional Involvement</a></p></body></html>