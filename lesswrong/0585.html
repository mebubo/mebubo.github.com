<html><head><title>Prolegomena to a Theory of Fun</title></head><body><h1>Prolegomena to a Theory of Fun</h1><p><i>Eliezer Yudkowsky, 17 December 2008 11:33PM</i></p><div><p><em></em><strong>Followup to</strong>:  <a href="0443.html">Joy in the Merely Good</a> [http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/]</p> <p>Raise the topic of cryonics, uploading, or just medically extended lifespan/healthspan, and some bioconservative neo-Luddite is bound to ask, in portentous tones:</p> <p style="margin-left: 40px;">"But what will people <em>do </em>all day?"</p> <p>They don't try to <a href="http://intelligence.org/blog/2007/10/14/the-meaning-that-immortality-gives-to-life/">actually answer the question</a> [http://intelligence.org/blog/2007/10/14/the-meaning-that-immortality-gives-to-life/].  That is not a bioethicist's role, in the scheme of things.  They're just there to collect credit for the <a href="0127.html">Deep Wisdom</a> [http://lesswrong.com/lw/k5/cached_thoughts/] of asking the question.  It's enough to <em>imply </em>that the question is unanswerable, and therefore, we should all drop dead.</p> <p>That <a href="0190.html">doesn't mean</a> [http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/] it's a <em>bad </em>question.</p> <p>It's not an <em>easy </em>question to answer, either.  The primary experimental result in hedonic psychology&#8212;the study of happiness&#8212;is that people don't <em>know </em>what makes them happy.</p> <p>And there are many exciting results in this new field, which go a long way toward explaining the emptiness of classical Utopias.  But it's worth remembering that<em> human</em> hedonic psychology is not enough for us to consider, if we're asking whether a million-year lifespan could be worth living.</p> <p>Fun Theory, then, is the field of knowledge that would deal in questions like:</p> <ul> <li>"How much fun is there in the universe?"</li> <li>"Will we ever run out of fun?"</li> <li>"Are we having fun yet?"</li> <li>"Could we be having more fun?"</li> </ul> <p><a id="more"></a></p> <p>One major set of experimental results in hedonic psychology has to do with <em>overestimating the impact</em> of life events on happiness.  Six months after the event, lottery winners aren't as happy as they expected to be, and quadriplegics aren't as sad.  A parent who loses a child isn't as sad as they think they'll be, a few years later.  If you look at one moment snapshotted out of their lives a few years later, that moment isn't likely to be about the lost child.  Maybe they're playing with one of their surviving children on a swing.  Maybe they're just listening to a nice song on the radio.</p> <p>When people are asked to imagine how happy or sad an event will make them, they anchor on <em>the moment of first receiving the news,</em> rather than realistically imagining the process of daily life years later.</p> <p>Consider what the Christians made of their Heaven, meant to be literally <em>eternal.</em>  Endless rest, the glorious presence of God, and occasionally&#8212;in the <a href="0584.html">more clueless sort of sermon</a> [http://lesswrong.com/lw/wu/visualizing_eutopia/]&#8212;golden streets and diamond buildings.  Is this eudaimonia?  It doesn't even seem very <em>hedonic</em>.</p> <p>As someone who said his share of prayers back in his Orthodox Jewish childhood upbringing, I can personally testify that praising God is an enormously boring activity, even if you're still young enough to truly believe in God.  The part about praising God is there as an <a href="0097.html">applause light</a> [http://lesswrong.com/lw/jb/applause_lights/] that no one is allowed to contradict: it's something theists <a href="0054.html">believe they <em>should</em> enjoy</a> [http://lesswrong.com/lw/i4/belief_in_belief/], even though, if you ran them through an fMRI machine, you probably wouldn't find their pleasure centers lighting up much.</p> <p>Ideology is one major wellspring of flawed Utopias, containing things that the imaginer believes <em>should </em>be enjoyed, rather than things that would actually be enjoyable.</p> <p>And eternal <em>rest?</em>  What could possibly be more boring than eternal <em>rest?</em></p> <p>But to an exhausted, poverty-stricken medieval peasant, the Christian Heaven sounds like <em>good news in the moment of being first informed:</em>  You can lay down the plow and rest!  Forever!  Never to work again!</p> <p>It'd get boring after... what, a week?  A day?  An hour?<br><span style="font-style: italic;"> </span></p> <p>Heaven is not configured as a nice place to <em>live.</em>  It is rather memetically optimized to be a nice place for an exhausted peasant to <em>imagine.</em>  It's not like some Christians <em>actually </em>got a chance to live in various Heavens, and voted on how well they liked it after a year, and then they kept the best one.  The Paradise that survived was the one that was <em>retold,</em> not lived.</p> <p>Timothy Feriss observed, "<em>Living</em> like a millionaire requires <em>doing</em> interesting things and not just owning enviable things."  <a href="0584.html">Golden streets and diamond walls</a> [http://lesswrong.com/lw/wu/visualizing_eutopia/] would fade swiftly into the background, once <em>obtained </em>&#8212;but so long as you <a href="0301.html">don't actually <em>have</em> gold</a> [http://lesswrong.com/lw/oz/scarcity/], it stays desirable.</p> <p>And there's two lessons required to get past such failures; and these lessons are in some sense opposite to one another.</p> <p>The first lesson is that humans are terrible judges of what will <em>actually </em>make them happy, in the real world and the living moments.  Daniel Gilbert's <em>Stumbling on Happiness</em> is the most famous popular introduction to the research.</p> <p>We need to be ready to correct for such biases&#8212;the world that is fun to <em>live in</em>, may not be the world that sounds good when spoken into our ears.</p> <p>And the second lesson is that there's <em>nothing</em> in the universe out of which to construct Fun Theory, except that which we want for ourselves or prefer to become.</p> <p>If, <em>in fact</em>, you <em>don't</em> like praying, then there's no higher God than yourself to tell you that you <em>should</em> enjoy it.  We sometimes do things we don't like, but that's still our own choice.  There's no <em>outside</em> force to scold us for making the wrong decision.</p> <p>This is something for transhumanists to keep in mind&#8212;not because we're tempted to pray, of course, but because there are so many other logical-sounding solutions we wouldn't really <em>want.</em></p> <p>The transhumanist philosopher <a href="http://en.wikipedia.org/wiki/David_Pearce_%28philosopher%29">David Pearce</a> [http://en.wikipedia.org/wiki/David_Pearce_%28philosopher%29] is an advocate of what he calls the <a href="http://www.hedweb.com/">Hedonistic Imperative</a> [http://www.hedweb.com/]:  The eudaimonic life is the one that is as pleasurable as possible.  So even happiness attained through drugs is good?  Yes, in fact:  Pearce's motto is "Better Living Through Chemistry".</p> <p>Or similarly:  When giving a small informal talk once on the Stanford campus, I raised the topic of Fun Theory in the post-talk mingling.  And someone there said that his ultimate objective was to experience delta pleasure.  That's "delta" as in the Dirac delta&#8212;roughly, an infinitely high spike (that happens to be integrable).  "Why?" I asked.  He said, "Because that means I win."</p> <p>(I replied, "How about if you get two times delta pleasure?  Do you win twice as hard?")</p> <p>In the transhumanist lexicon, "orgasmium" refers to simplified brains that are just pleasure centers experiencing huge amounts of stimulation&#8212;a happiness counter containing a large number, plus whatever the minimum surrounding framework to <em>experience</em> it.  You can imagine a whole galaxy tiled with orgasmium.  Would this be a good thing?</p> <p>And the vertigo-inducing thought is this&#8212;if you would <em>prefer</em> not to become orgasmium, then why <em>should</em> you?</p> <p>Mind you, there are many reasons why something that sounds unpreferred at first glance, might be worth a closer look.  That was the <em>first</em> lesson.  Many Christians <em>think </em>they want to go to Heaven.</p> <p>But when it comes to the question, "Don't I <em>have</em> to want to be as happy as possible?" then the answer is simply "No.  If you don't prefer it, why go there?"</p> <p>There's nothing <em>except </em>such preferences out of which to construct Fun Theory&#8212;a second look is still a look, and must still be constructed out of preferences at some level.</p> <p>In the era of my foolish youth, when <a href="0480.html">I went into an affective death spiral around intelligence</a> [http://lesswrong.com/lw/ty/my_childhood_death_spiral/], I thought that the <a href="0443.html">mysterious "right" thing</a> [http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/] that <a href="0484.html">any superintelligence would inevitably do</a> [http://lesswrong.com/lw/u2/the_sheer_folly_of_callow_youth/], would be to upgrade every nearby mind to superintelligence as fast as possible.  Intelligence was good; therefore, more intelligence was better.</p> <p>Somewhat later I imagined the scenario of <em>unlimited</em> computing power, so that no matter how smart you got, you were still just as far from infinity as ever.  That got me thinking about a journey rather than a destination, and <em>allowed </em>me to think "What <em>rate </em>of intelligence increase would be fun?"</p> <p>But the real break came when I <a href="0432.html">naturalized my understanding of morality</a> [http://lesswrong.com/lw/sm/the_meaning_of_right/], and value stopped being a mysterious attribute of unknown origins.</p> <p>Then if there was no outside light in the sky to order me to do things&#8212;</p> <p>The thought occurred to me that I didn't actually <em>want</em> to bloat up immediately into a superintelligence, <em>or</em> have my world transformed instantaneously and completely into something incomprehensible.  I'd prefer to have it happen gradually, with time to stop and smell the flowers along the way.</p> <p>It felt like a very guilty thought, but&#8212;</p> <p>But there was nothing <em>higher </em>to <em>override </em>this preference.</p> <p>In which case, if the Friendly AI project succeeded, there would be a day after the Singularity to wake up to, and myself to wake up to it.</p> <p>You may not see why this would be a vertigo-inducing concept.  Pretend you're Eliezer<sub>2003</sub> who has spent the last seven years talking about how it's forbidden to try to look beyond the Singularity&#8212;because the AI is smarter than you, and if you knew what it would do, you would have to be that smart yourself&#8212;</p> <p>&#8212;but what if you don't <em>want</em> the world to be made suddenly incomprehensible?  Then there might be something to understand, that next morning, <em>because</em> you don't <em>actually want</em> to wake up in an incomprehensible world, any more than you <em>actually want</em> to suddenly be a superintelligence, or turn into orgasmium.</p> <p>I can only analogize the experience to a theist who's suddenly told that they <em>can</em> know the mind of God, and it turns out to be only twenty lines of Python.</p> <p>You may find it hard to sympathize.  Well, Eliezer<sub>1996</sub>, who originally made the mistake, was <a href="0483.html">smart but methodologically inept</a> [http://lesswrong.com/lw/u1/a_prodigy_of_refutation/], as I've mentioned a few times.</p> <p>Still, expect to see some outraged comments on this very blog post, from commenters who think that it's <em>selfish and immoral</em>, and above all a <em>failure of imagination,</em> to talk about human-level minds still running around the day after the Singularity.</p> <p>That's the frame of mind I used to occupy&#8212;that the things I wanted were selfish, and that I shouldn't think about them too much, or at all, because I would need to sacrifice them for something higher.</p> <p>People who talk about an existential pit of meaninglessness in a universe devoid of meaning&#8212;I'm pretty sure they don't understand morality in naturalistic terms.  There <em>is</em> vertigo involved, but it's <em>not</em> the vertigo of meaninglessness<em>.</em></p> <p>More like a theist who is <a href="0421.html">frightened</a> [http://lesswrong.com/lw/sb/could_anything_be_right/] that someday God will <a href="http://www.thevillageatheist.co.uk/genocide.html">order him to murder children</a> [http://www.thevillageatheist.co.uk/genocide.html], and then he realizes that there <em>is</em> no God and his fear of being ordered to murder children <em><a href="0156.html">was morality</a> [http://lesswrong.com/lw/ky/fake_morality/]</em>.  It's a strange relief, mixed with the realization that you've been very silly, as the last remnant of outrage at your own selfishness fades away.</p> <p>So the first step toward Fun Theory is that, so far as I can tell, it looks basically <em>okay </em>to make our future light cone&#8212;all the galaxies that we can get our hands on&#8212;into a place that is <em>fun</em> rather than <em>not fun.</em></p> <p>We don't need to transform the universe into something we feel <em>dutifully obligated</em> to create, but isn't really much fun&#8212;in the same way that a Christian would feel dutifully obliged to enjoy heaven&#8212;or that some strange folk think that creating orgasmium is, logically, the rightest thing to do.</p> <p>Fun is okay.  It's allowed.  It doesn't get any better than fun.</p> <p>And then we can turn our attention to the question of what <em>is</em> fun, and how to have it.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0586.html">High Challenge</a> [http://lesswrong.com/lw/ww/high_challenge/]"</p> <p style="text-align:right">(start of sequence)</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="seq14.html">Sequence 14: Metaethics</a></p></td><td><p><i>Next: </i><a href="0586.html">High Challenge</a></p></td></tr></table><p><i>Referenced by: </i><a href="0586.html">High Challenge</a> &#8226; <a href="0598.html">Amputation of Destiny</a> &#8226; <a href="0601.html">Free to Optimize</a> &#8226; <a href="0610.html">Continuous Improvement</a> &#8226; <a href="0611.html">Eutopia is Scary</a> &#8226; <a href="0619.html">Interpersonal Entanglement</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0626.html">31 Laws of Fun</a> &#8226; <a href="0690.html">Can Humanism Match Religion's Output?</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/">Prolegomena to a Theory of Fun</a></p></body></html>