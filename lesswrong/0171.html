<html><head><title>The Hidden Complexity of Wishes</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>The Hidden Complexity of Wishes</h1><p><i>Eliezer Yudkowsky, 24 November 2007 12:12AM</i></p><div><p><strong>Followup to</strong>:  <a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/], <a href="0157.html">Fake Optimization Criteria</a> [http://lesswrong.com/lw/kz/fake_optimization_criteria/], <a href="0162.html">Terminal Values and Instrumental Values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/], <a href="0167.html">Artificial Addition</a> [http://lesswrong.com/lw/l9/artificial_addition/], <a href="0170.html">Leaky Generalizations</a> [http://lesswrong.com/lw/lc/leaky_generalizations/] </p> <blockquote> <p><span>"I wish to live in the locations of my choice, in a physically healthy, uninjured, and apparently normal version of my current body containing my current mental state, a body which will heal from all injuries at a rate three sigmas faster than the average given the medical technology available to me, and which will be protected from any diseases, injuries or illnesses causing disability, pain, or degraded functionality or any sense, organ, or bodily function for more than ten days consecutively or fifteen days in any year..."<br>  &#160;&#160; &#160;&#160; &#160;&#160;  -- <a href="http://homeonthestrange.com/phpBB2/viewforum.php?f=4">The Open-Source Wish Project</a> [http://homeonthestrange.com/phpBB2/viewforum.php?f=4], <a href="http://homeonthestrange.com/phpBB2/viewtopic.php?t=95">Wish For Immortality 1.1</a> [http://homeonthestrange.com/phpBB2/viewtopic.php?t=95]</span></p> </blockquote> <p>There are three kinds of genies:  Genies to whom you can safely say "I wish for you to do what I should wish for"; genies for which <em>no</em> wish is safe; and <a href="0166.html">genies that aren't very powerful or intelligent</a> [http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/].</p> <p><a id="more"></a></p> <p>Suppose your aged mother is trapped in a burning building, and it so happens that you're in a wheelchair; you can't rush in yourself.  You could cry, "Get my mother out of that building!" but there would be no one to hear.</p> <p>Luckily you have, in your pocket, an Outcome Pump.  This handy device squeezes the flow of time, pouring probability into some outcomes, draining it from others.</p> <p>The Outcome Pump is not sentient.  It contains a tiny time machine, which resets time <em>unless</em> a specified outcome occurs.  For example, if you hooked up the Outcome Pump's sensors to a coin, and specified that the time machine should keep resetting until it sees the coin come up heads, and then you actually flipped the coin, <em>you</em> would see the coin come up heads.  (The physicists say that any future in which a "reset" occurs is inconsistent, and therefore never happens in the first place - so you aren't actually killing any versions of yourself.)</p> <p>Whatever proposition you can manage to input into the Outcome Pump, <em>somehow happens,</em> though not in a way that violates the laws of physics.  If you try to input a proposition that's <em>too</em> unlikely, the time machine will suffer a spontaneous mechanical failure before that outcome ever occurs.</p> <p>You can also redirect probability flow in more quantitative ways using the "future function" to scale the temporal reset probability for different outcomes.  If the temporal reset probability is 99% when the coin comes up heads, and 1% when the coin comes up tails, the odds will go from 1:1 to 99:1 in favor of tails.  If you had a mysterious machine that spit out money, and you wanted to maximize the amount of money spit out, you would use reset probabilities that diminished as the amount of money increased.  For example, spitting out $10 might have a 99.999999% reset probability, and spitting out $100 might have a 99.99999% reset probability.  This way you can get an outcome that tends to be as high as possible in the future function, even when you don't know the best attainable maximum.</p> <p>So you desperately yank the Outcome Pump from your pocket - your mother is still trapped in the burning building, remember? - and try to describe your goal: <em>get your mother out of the building!</em></p> <p>The user interface doesn't take English inputs.  The Outcome Pump isn't sentient, remember?  But it does have 3D scanners for the near vicinity, and built-in utilities for pattern matching.  So you hold up a photo of your mother's head and shoulders; match on the photo; use object contiguity to select your mother's whole body (not just her head and shoulders); and define the <em>future function</em> using your mother's distance from the building's center.  The further she gets from the building's center, the less the time machine's reset probability.</p> <p>You cry "Get my mother out of the building!", for luck, and press Enter.</p> <p>For a moment it seems like nothing happens.  You look around, waiting for the fire truck to pull up, and rescuers to arrive - or even just a strong, fast runner to haul your mother out of the building -</p> <p><em>BOOM!</em>  With a thundering roar, the gas main under the building explodes.  As the structure comes apart, in what seems like slow motion, you glimpse your mother's shattered body being hurled high into the air, traveling fast, rapidly increasing its distance from the former center of the building.</p> <p>On the side of the Outcome Pump is an Emergency Regret Button.  All future functions are automatically defined with a huge negative value for the Regret Button being pressed - a temporal reset probability of nearly 1 - so that the Outcome Pump is extremely unlikely to do anything which upsets the user enough to make them press the Regret Button.  You can't ever remember pressing it.  But you've barely started to reach for the Regret Button (and what good will it do now?) when a flaming wooden beam drops out of the sky and smashes you flat.</p> <p>Which wasn't really what you wanted, but scores very high in the defined future function...</p> <p>The Outcome Pump is a genie of the second class.  <em>No</em> wish is safe.</p> <p>If someone asked you to get their poor aged mother out of a burning building, you might help, or you might pretend not to hear.  But it wouldn't even <em>occur</em> to you to explode the building.  "Get my mother out of the building" <em>sounds</em> like a much safer wish than it really is, because you don't even <em>consider </em>the plans that you assign extreme negative values.</p> <p>Consider again the <a href="0154.html">Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]: Some early biologists asserted that group selection for low subpopulation sizes would produce individual restraint in breeding; and yet actually enforcing group selection in the laboratory produced cannibalism, especially of immature females.  It's <a href="0071.html">obvious in hindsight</a> [http://lesswrong.com/lw/il/hindsight_bias/] that, given strong selection for small subpopulation sizes, cannibals will outreproduce individuals who voluntarily forego reproductive opportunities.  But eating little girls is such an <em>un-aesthetic</em> solution that Wynne-Edwards, Allee, Brereton, and the other group-selectionists simply didn't think of it.  They only saw the solutions they would have used themselves.</p> <p>Suppose you try to patch the future function by specifying that the Outcome Pump should not explode the building: outcomes in which the building materials are distributed over too much volume, will have ~1 temporal reset probabilities.</p> <p>So your mother falls out of a second-story window and breaks her neck.  The Outcome Pump took a different path through time that still ended up with your mother outside the building, and it still wasn't what you wanted, and it still wasn't a solution that would occur to a human rescuer.</p> <p>If only the <a href="http://homeonthestrange.com/phpBB2/viewforum.php?f=4">Open-Source Wish Project</a> [http://homeonthestrange.com/phpBB2/viewforum.php?f=4] had developed a Wish To Get Your Mother Out Of A Burning Building:</p> <blockquote> <p>"I wish to move my mother (defined as the woman who shares half my genes and gave birth to me) to outside the boundaries of the building currently closest to me which is on fire; but not by exploding the building; nor by causing the walls to crumble so that the building no longer has boundaries; nor by waiting until after the building finishes burning down for a rescue worker to take out the body..."</p> </blockquote> <p>All these special cases, the seemingly unlimited number of required patches, should remind you of the parable of <a href="0167.html">Artificial Addition</a> [http://lesswrong.com/lw/l9/artificial_addition/] - programming an Arithmetic Expert Systems by explicitly adding ever more assertions like "fifteen plus fifteen equals thirty, but fifteen plus sixteen equals thirty-one instead".</p> <p>How do <em>you</em> exclude the outcome where the building explodes and flings your mother into the sky?  You look ahead, and you foresee that your mother would end up dead, and you don't want that consequence, so you try to forbid the event leading up to it.</p> <p>Your brain isn't hardwired with a specific, prerecorded statement that "Blowing up a burning building containing my mother is a bad idea."  And yet you're trying to prerecord that exact specific statement in the Outcome Pump's future function.  So the wish is exploding, turning into a giant lookup table that records your judgment of every possible path through time.</p> <p>You failed to ask for what you really wanted.  You <em>wanted</em> your mother to go on living, but you <em>wished</em> for her to become more distant from the center of the building.</p> <p>Except that's not all you wanted.  If your mother was rescued from the building but was horribly burned, that outcome would rank lower in your preference ordering than an outcome where she was rescued safe and sound.  So you not only value your mother's life, but also her health.</p> <p>And you value not just her bodily health, but her state of mind. Being rescued in a fashion that traumatizes her - for example, a giant purple monster roaring up out of nowhere and seizing her - is inferior to a fireman showing up and escorting her out through a non-burning route.  (Yes, we're supposed to stick with physics, but maybe a powerful enough Outcome Pump has aliens coincidentally showing up in the neighborhood at exactly that moment.)  You would certainly prefer her being rescued by the monster to her being roasted alive, however.</p> <p>How about a wormhole spontaneously opening and swallowing her to a desert island?  Better than her being dead; but worse than her being alive, well, healthy, untraumatized, and in continual contact with you and the other members of her social network.</p> <p>Would it be okay to save your mother's life at the cost of the family dog's life, if it ran to alert a fireman but then got run over by a car?  Clearly yes, but it would be better <em>ceteris paribus</em> to avoid killing the dog.  You wouldn't want to swap a human life for hers, but what about the life of a convicted murderer?  Does it matter if the murderer dies trying to save her, from the goodness of his heart?  How about two murderers?  If the cost of your mother's life was the destruction of every extant copy, including the memories, of Bach's <em>Little Fugue in G Minor,</em> would that be worth it?  How about if she had a terminal illness and would die anyway in eighteen months?</p> <p>If your mother's foot is crushed by a burning beam, is it worthwhile to extract the rest of her?  What if her head is crushed, leaving her body?  What if her body is crushed, leaving only her head?  What if there's a cryonics team waiting outside, ready to suspend the head?  Is a frozen head a person?  Is Terry Schiavo a person?  How much is a chimpanzee worth?</p> <p>Your brain is not infinitely complicated; there is only a finite <a href="0111.html">Kolmogorov complexity / message length</a> [http://lesswrong.com/lw/jp/occams_razor/] which suffices to describe all the judgments you would make.  But just because this complexity is finite does not make it small.  <a href="0161.html">We value many things</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/], and no they are <em>not</em> reducible to <a href="0169.html">valuing happiness</a> [http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/] or <a href="0159.html">valuing reproductive fitness</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/].</p> <p>There is no safe wish smaller than an entire human morality.  There are too many possible paths through Time.  You can't visualize all the roads that lead to the destination you give the genie.  "Maximizing the distance between your mother and the center of the building" can be done even more effectively by detonating a nuclear weapon.  Or, at higher levels of genie power, flinging her body out of the Solar System.  Or, at higher levels of genie intelligence, doing something that neither you nor I would think of, just like a chimpanzee wouldn't think of detonating a nuclear weapon.  You can't visualize all the paths through time, any more than you can program a chess-playing machine by hardcoding a move for every possible board position.</p> <p>And real life is far more complicated than chess.  You cannot predict, in advance, which of your values will be needed to judge the path through time that the genie takes.  Especially if you wish for something longer-term or wider-range than rescuing your mother from a burning building.</p> <p>I fear the <span><a href="http://homeonthestrange.com/phpBB2/viewforum.php?f=4">Open-Source Wish Project</a> [http://homeonthestrange.com/phpBB2/viewforum.php?f=4] is futile, except as an illustration of how <em>not</em> to think about genie problems.  The only safe genie is a genie that shares all your judgment criteria, and at that point, you can just say "I wish for you to do what I should wish for."  Which simply runs the genie's <em>should</em> function.</span></p> <p><span>Indeed, it shouldn't be necessary to say <em>anything.</em>  To be a safe fulfiller of a wish, a genie must share the same values that led you to make the wish. Otherwise the genie may not choose a path through time which leads to the destination you had in mind, or it may fail to exclude horrible side effects that would lead you to not even consider a plan in the first place.  Wishes are <a href="0170.html">leaky generalizations</a> [http://lesswrong.com/lw/lc/leaky_generalizations/], derived from the huge but finite structure that is your entire morality; only by including this entire structure can you plug all the leaks.<br></span></p> <p><span>With a safe genie, wishing is superfluous.  Just run the genie.</span></p></div> <hr><p><i>Referenced by: </i><a href="0172.html">Lost Purposes</a> &#8226; <a href="0183.html">Fake Fake Utility Functions</a> &#8226; <a href="0340.html">Spooky Action at a Distance: The No-Communication Theorem</a> &#8226; <a href="0389.html">Ghosts in the Machine</a> &#8226; <a href="0391.html">Heading Toward Morality</a> &#8226; <a href="0440.html">Contaminated by Optimism</a> &#8226; <a href="0442.html">Morality as Fixed Computation</a> &#8226; <a href="0458.html">Unnatural Categories</a> &#8226; <a href="0459.html">Magical Categories</a> &#8226; <a href="0491.html">My Naturalistic Awakening</a> &#8226; <a href="0579.html">What I Think, If Not Why</a> &#8226; <a href="0593.html">Devil's Offers</a> &#8226; <a href="0617.html">In Praise of Boredom</a> &#8226; <a href="0629.html">Value is Fragile</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/">The Hidden Complexity of Wishes</a></p></body></html>