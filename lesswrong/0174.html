<html><head><title>The Affect Heuristic</title></head><body><h1>The Affect Heuristic</h1><p><i>Eliezer Yudkowsky, 27 November 2007 07:58AM</i></p><div><p>The <em>affect heuristic</em> is when subjective impressions of goodness/badness act as a heuristic&#8212;a source of fast, perceptual judgments.  Pleasant and unpleasant feelings are central to human reasoning, and the affect heuristic comes with lovely biases&#8212;some of my favorites.</p> <p>Let's start with one of the relatively less crazy biases.  You're about to move to a new city, and you have to ship an antique grandfather clock.  In the first case, the grandfather clock was a gift from your grandparents on your 5th birthday.  In the second case, the clock was a gift from a remote relative and you have no special feelings for it.  How much would you pay for an insurance policy that paid out $100 if the clock were lost in shipping?  According to Hsee and Kunreuther (2000), subjects stated willingness to pay more than twice as much in the first condition.  This may sound rational&#8212;why not pay more to protect the more valuable object?&#8212;until you realize that the insurance doesn't <em>protect</em> the clock, it just pays if the clock is lost, and pays exactly the same amount for either clock.  (And yes, it was stated that the insurance was with an outside company, so it gives no special motive to the movers.)</p> <p>All right, but that doesn't <em>sound</em> too insane.  Maybe you could get away with claiming the subjects were insuring affective outcomes, not financial outcomes&#8212;purchase of consolation.</p> <p>Then how about this?  Yamagishi (1997) showed that subjects judged a disease as more dangerous when it was described as killing 1,286 people out of every 10,000, versus a disease that was 24.14% likely to be fatal.  Apparently the mental image of a thousand dead bodies is much more alarming, compared to a single person who's more likely to survive than not.</p> <p>But wait, it gets worse.</p> <p><a id="more"></a></p> <p>Suppose an airport must decide whether to spend money to purchase some new equipment, while critics argue that the money should be spent on other aspects of airport safety.  Slovic et. al. (2002) presented two groups of subjects with the arguments for and against purchasing the equipment, with a response scale ranging from 0 (would not support at all) to 20 (very strong support).  One group saw the measure described as saving 150 lives.  The other group saw the measure described as saving 98% of 150 lives.  The hypothesis motivating the experiment was that saving 150 lives sounds vaguely good&#8212;is that a lot? a little?&#8212;while saving 98% of something is clearly very good because 98% is so close to the upper bound of the percentage scale.  Lo and behold, saving 150 lives had mean support of 10.4, while saving 98% of 150 lives had mean support of 13.6.</p> <p>Or consider the report of Denes-Raj and Epstein (1994):  Subjects offered an opportunity to win $1 each time they randomly drew a red jelly bean from a bowl, often preferred to draw from a bowl with more red beans and a smaller proportion of red beans.  E.g., 7 in 100 was preferred to 1 in 10.</p> <p>According to Denes-Raj and Epstein, these subjects reported afterward that even though they knew the probabilities were against them, they felt they had a better chance when there were more red beans.  This may sound crazy to you, oh Statistically Sophisticated Reader, but if you think more carefully you'll realize that it makes perfect sense.  A 7% probability versus 10% probability may be bad news, but it's more than made up for by the increased number of red beans.  It's a worse probability, yes, but you're still more likely to <em>win,</em> you see.  You should meditate upon this thought until you attain enlightenment as to how the rest of the planet thinks about probability.</p> <p>Finucane et. al. (2000) tested the theory that people would conflate their judgments about particular good/bad aspects of something into an overall good or bad feeling about that thing.  For example, information about a possible risk, or possible benefit, of nuclear power plants.  Logically, information about risk doesn't have to bear any relation to information about benefits.  If it's a physical fact about a reactor design that it's passively safe (won't go supercritical even if the surrounding coolant systems and so on break down), this doesn't imply that the reactor will necessarily generate less waste, or produce electricity at a lower cost, etcetera.  All these things would be good, but they are not the same good thing.  Nonetheless, Finucane et. al. found that for nuclear reactors, natural gas, and food preservatives, presenting information about high benefits made people perceive lower risks; presenting information about higher risks made people perceive lower benefits; and so on across the quadrants.</p> <p>Finucane et. al. also found that time pressure greatly <em>increased</em> the inverse relationship between perceived risk and perceived benefit, consistent with the general finding that time pressure, poor information, or distraction all increase the dominance of perceptual heuristics over analytic deliberation.</p> <p>Ganzach (2001) found the same effect in the realm of finance.  According to ordinary economic theory, return and risk should correlate <em>positively</em>&#8212;or to put it another way, people pay a premium price for safe investments, which lowers the return; stocks deliver higher returns than bonds, but have correspondingly greater risk.  When judging <em>familiar</em> stocks, analysts' judgments of risks and returns were positively correlated, as conventionally predicted.  But when judging <em>unfamiliar</em> stocks, analysts tended to judge the stocks as if they were generally good or generally bad&#8212;low risk and high returns, or high risk and low returns.</p> <p>For further reading I recommend the fine summary chapter in Slovic et. al. 2002:  "<a href="http://heuristics.behaviouralfinance.net/affect/Slov02.pdf">Rational Actors or Rational Fools: Implications of the Affect Heuristic for Behavioral Economics.</a> [http://heuristics.behaviouralfinance.net/affect/Slov02.pdf]"</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor"><em>Death Spirals and the Cult Attractor</em></a> [http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0175.html">Evaluability (And Cheap Holiday Shopping)</a> [http://lesswrong.com/lw/lh/evaluability_and_cheap_holiday_shopping/]"</p> <hr> <p>Denes-Raj, V., &amp; Epstein, S. (1994). Conflict between intuitive and rational processing: When people behave against their better judgment. <em>Journal of Personality and Social Psychology,</em> 66, 819-829.</p> <p>Finucane, M. L., Alhakami, A., Slovic, P., &amp; Johnson, S. M. (2000). <a href="http://www-abc.mpib-berlin.mpg.de/users/r20/finucane00_the_affect_heuristic.pdf">The affect heuristic in judgments of risks and benefits.</a> [http://www-abc.mpib-berlin.mpg.de/users/r20/finucane00_the_affect_heuristic.pdf] <em>Journal of Behavioral Decision Making,</em> 13, 1-17.</p> <p>Ganzach, Y. (2001). Judging risk and return of financial assets. <em>Organizational Behavior and Human Decision Processes,</em> 83, 353-370.</p> <p>Hsee, C. K. &amp; Kunreuther, H. (2000). <a href="http://faculty.chicagogsb.edu/christopher.hsee/vita/Papers/AffectionEffectInInsurance.pdf">The affection effect in insurance decisions.</a> [http://faculty.chicagogsb.edu/christopher.hsee/vita/Papers/AffectionEffectInInsurance.pdf] <em>Journal of Risk and Uncertainty</em>, 20, 141-159.</p> <p>Slovic, P., Finucane, M., Peters, E. and MacGregor, D. 2002. <a href="http://heuristics.behaviouralfinance.net/affect/Slov02.pdf">Rational Actors or Rational Fools: Implications of the Affect Heuristic for Behavioral Economics.</a> [http://heuristics.behaviouralfinance.net/affect/Slov02.pdf]  <em>Journal of Socio-Economics,</em> 31: 329&#8211;342.</p> <p>Yamagishi, K. (1997). When a 12.86% mortality is more dangerous than 24.14%:  Implications for risk communication. <em>Applied Cognitive Psychology,</em> 11, 495-506.</p></div> <hr><table><tr><th colspan="2"><a href="seq04.html">Sequence 04: Death Spirals and the Cult Attractor</a>:</th></tr><tr><td><p><i>Previous: </i><a href="seq03.html">Sequence 03: Politics is the Mind-Killer</a></p></td><td><p><i>Next: </i><a href="0175.html">Evaluability (And Cheap Holiday Shopping)</a></p></td></tr></table><p><i>Referenced by: </i><a href="0013.html">Policy Debates Should Not Appear One-Sided</a> &#8226; <a href="0099.html">Human Evil and Muddled Thinking</a> &#8226; <a href="0175.html">Evaluability (And Cheap Holiday Shopping)</a> &#8226; <a href="0177.html">The Halo Effect</a> &#8226; <a href="0180.html">Affective Death Spirals</a> &#8226; <a href="0183.html">Fake Fake Utility Functions</a> &#8226; <a href="0212.html">Stop Voting For Nincompoops</a> &#8226; <a href="0213.html">Rational vs. Scientific Ev-Psych</a> &#8226; <a href="0391.html">Heading Toward Morality</a> &#8226; <a href="0463.html">Harder Choices Matter Less</a> &#8226; <a href="0505.html">On Doing the Impossible</a> &#8226; <a href="0552.html">Failure By Affective Analogy</a> &#8226; <a href="0568.html">Recursive Self-Improvement</a> &#8226; <a href="0603.html">Growing Up is Hard</a> &#8226; <a href="0729.html">Special Status Needs Special Support</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/lg/the_affect_heuristic/">The Affect Heuristic</a></p></body></html>