<html><head><title>Three Fallacies of Teleology</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Three Fallacies of Teleology</h1><p><i>Eliezer Yudkowsky, 25 August 2008 10:27PM</i></p><div><p><strong>Followup to</strong>:  <a href="0439.html">Anthropomorphic Optimism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/]</p> <p>Aristotle distinguished between four senses of the Greek word <em>aition</em>, which in English is translated as "cause", though Wikipedia suggests that a better translation is "maker".  Aristotle's theory of the Four Causes, then, might be better translated as the Four Makers.  These were his four senses of <em>aitia</em>:  The material <em>aition</em>, the formal <em>aition,</em> the efficient <em>aition</em>, and the final <em>aition.</em></p> <p>The material <em>aition</em> of a bronze statue is the substance it is made from, bronze.  The formal <em>aition</em> is the substance's form, its statue-shaped-ness.  The efficient <em>aition</em> best translates as the English word "cause"; we would think of the artisan carving the statue, though Aristotle referred to the art of bronze-casting the statue, and regarded the individual artisan as a mere instantiation.</p> <p>The final <em>aition</em> was the goal, or <em>telos,</em> or purpose of the statue, that for the sake of which the statue exists.</p> <p>Though Aristotle considered knowledge of all four <em>aitia</em> as necessary, he regarded knowledge of the <em>telos</em> as the knowledge of highest order.  In this, Aristotle followed in the path of Plato, who had earlier written:</p> <blockquote> <p>Imagine not being able to distinguish the real cause from that without which the cause would not be able to act as a cause.  It is what the majority appear to do, like people groping in the dark; they call it a cause, thus giving it a name that does not belong to it.  That is why one man surrounds the earth with a vortex to make the heavens keep it in place, another makes the air support it like a wide lid.  As for their capacity of being in the best place they could possibly be put, this they do not look for, nor do they believe it to have any divine force...</p> </blockquote> <p><a id="more"></a></p> <p>Suppose that you translate "final <em>aition</em>" as "final cause", and assert directly:</p> <p>"Why do human teeth develop with such regularity, into a structure well-formed for biting and chewing?  You could try to explain this as an incidental fact, but think of how unlikely that would be.  Clearly, the final cause of teeth is the act of biting and chewing.  Teeth develop with regularity, <em>because of</em> the act of biting and chewing - the latter causes the former."</p> <p>A modern-day sophisticated Bayesian will at once remark, "This requires me to draw a circular causal diagram with an arrow going from the future to the past."</p> <p>It's not clear to me to what extent Aristotle appreciated this point - that you could not draw causal arrows from the future to the past.  Aristotle did acknowledge that teeth <em>also</em> needed an efficient cause to develop.  But Aristotle may have believed that the efficient cause could not act without the <em>telos,</em> or was directed by the <em>telos,</em> in which case we again have a reversed direction of causality, a dependency of the past on the future.  I am no scholar of the classics, so it may be only myself who is ignorant of what Aristotle believed on this score.</p> <p>So the first way in which <em>teleological reasoning</em> may be an outright fallacy, is when an arrow is drawn <em>directly</em> from the future to the past.  In every case where a present event <em>seems</em> to happen for the sake of a future end, that future end must be materially <em>represented</em> in the past.</p> <p>Suppose you're <a href="0162.html">driving to the supermarket</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/], and you say that each right turn and left turn happens <em>for the sake of</em> the future event of your being at the supermarket.  Then the actual efficient cause of the turn, consists of:  the <em>representation in your mind</em> of the event of yourself arriving at the supermarket; your mental representation of the street map (not the streets themselves); your brain's <a href="0385.html">planning mechanism</a> [http://lesswrong.com/lw/rb/possibility_and_couldness/] that searches for a plan that represents arrival at the supermarket; and the nerves that translate this plan into the motor action of your hands turning the steering wheel.</p> <p>All these things exist in the past or present; no arrow is drawn from the future to the past.</p> <p>In biology, similarly, we explain the regular formation of teeth, not by letting it be caused <em>directly</em> by the future act of chewing, but by using the theory of natural selection to relate <em>past</em> events of chewing to the organism's <em>current</em> genetic makeup, which physically controls the formation of the teeth.  Thus, we account for the <em>current</em> regularity of the teeth by referring only to <em>past</em> and <em>present</em> events, never to <em>future</em> events.  Such evolutionary reasoning is called "teleonomy", in contrast with teleology.</p> <p>We can see that the <em>efficient</em> cause is primary, not the final cause, by considering what happens when the two come into conflict.  The efficient cause of human taste buds is natural selection on past human eating habits; the final cause of human taste buds is acquiring nutrition.  From the efficient cause, we should expect human taste buds to seek out resources that were scarce in the ancestral environment, like fat and sugar.  From the final cause, we would expect human taste buds to seek out resources scarce in the current environment, like vitamins and fiber.  From the sales numbers on <a href="0017.html">candy bars</a> [http://lesswrong.com/lw/h3/superstimuli_and_the_collapse_of_western/], we can see which wins.  The saying "<a href="0158.html">Individual organisms are best thought of as adaptation-executers rather than as fitness-maximizers</a> [http://lesswrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/]" asserts the primacy of teleonomy over teleology.</p> <p>Similarly, if you have a mistake in your mind about where the supermarket lies, the final event of your arrival at the supermarket, will not reach backward in time to steer your car.  If I know your exact state of mind, I will be able to predict your car's trajectory by <em>modeling your current state of mind,</em> not by supposing that the car is attracted to some particular final destination.  If I know your mind in detail, I can even predict your mistakes, regardless of what you think is your goal.</p> <p>The efficient cause has <a href="0191.html">screened off</a> [http://lesswrong.com/lw/lx/argument_screens_off_authority/] the <em>telos:</em>  If I can model the complete mechanisms at work in the <em>present,</em> I never have to take into account the <em>future</em> in predicting the next time step.</p> <p>So that is the first fallacy of teleology - to make the future a literal cause of the past.</p> <p>Now admittedly, it may be <em>convenient</em> to engage in reasoning that would be fallacious if interpreted literally.  For example:</p> <blockquote> <p>I don't know the exact state of Mary's every neuron.  But I know that she desires to be at the supermarket.  If Mary turns left at the next intersection, she <em>will then be at the supermarket</em> (at time t=1).  <em>Therefore</em> Mary will turn left (at time t=0).</p> </blockquote> <p>But this is only <a href="0256.html">convenient shortcut</a> [http://lesswrong.com/lw/nq/feel_the_meaning/], to let the future affect Mary's present actions.  More rigorous reasoning would say:</p> <blockquote> <p>My model predicts that if Mary turns left she will arrive at the supermarket.  I don't know her every neuron, but I believe Mary has a model similar to mine.  I believe Mary desires to be at the supermarket.  I believe that Mary has a planning mechanism similar to mine, which leads her to take actions that her model predicts will lead to the fulfillment of her desires.  Therefore I predict that Mary will turn left.</p> </blockquote> <p>No <em>direct</em> mention of the <em>actual</em> future has been made.  I predict Mary by imagining myself to have her goals, then putting myself and my planning mechanisms into her shoes, letting my brain do planning-work that is similar to the planning-work I expect Mary to do.  This requires me to talk only about Mary's goal, our models (presumed similar) and our planning mechanisms (presumed similar) - all forces active in the present.</p> <p>And the benefit of this more rigorous reasoning, is that if Mary is mistaken about the supermarket's location, then I do not have to suppose that the future event of her arrival reaches back and steers her correctly anyway.</p> <p><em>Teleological reasoning is anthropomorphic - it <a href="0437.html">uses your own brain as a black box</a> [http://lesswrong.com/lw/sr/the_comedy_of_behaviorism/] to predict external events.  Specifically, teleology uses your brain's planning mechanism as a black box to predict</em><em> a chain of future events, by planning backward from a distant outcome.</em></p> <p>Now we are talking about a highly generalized form of anthropomorphism - and indeed, it is precisely to introduce this generalization that I am talking about teleology!  You know what it's like to <em>feel</em> purposeful.  But when someone says, "water runs downhill so that it will be at the bottom", you don't necessarily imagine little sentient rivulets alive with quiet determination.  Nonetheless, when you ask, "How could the water get to the bottom of the hill?" and plot out a course down the hillside, you're recruiting your own brain's planning mechanisms to do it.  That's what the brain's planner does, after all: it finds a path to a specified destination starting from the present.</p> <p>And if you expect the water to avoid local maxima so it can get all the way to the bottom of the hill - to avoid being trapped in small puddles far above the ground - then <a href="0439.html">your anthropomorphism is going to produce the wrong prediction</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/].  (This is how a lot of mistaken evolutionary reasoning gets done, since evolution has no foresight, and only takes the next greedy local step.)</p> <p>But <em>consider the subtlety</em>: you may have produced a wrong, anthropomorphic prediction of the water without ever thinking of it as a person - without ever visualizing it as having feelings - without even thinking "the water has purpose" or "the water wants to be at the bottom of the hill" - but only saying, as Aristotle did, "the water's <em>telos</em> is to be closer to the center of the Earth".  Or maybe just, "the water runs downhill <em>so that</em> it will be at the bottom".  (Or, "I expect that human taste buds will take into account how much of each nutrient the body needs, and so reject fat and sugar if there are enough calories present, since evolution produced taste buds <em>in order to</em> acquire nutrients.")</p> <p><em>You don't notice instinctively when you're using an aspect of your brain as a black box to predict outside events.  Consequentialism just seems like an ordinary property of the world, something even rocks could do.</em></p> <p>It takes a deliberate act of <a href="0289.html">reductionism</a> [http://lesswrong.com/lw/on/reductionism/] to say:  "But the water has no brain; how can it predict ahead to see itself being trapped in a local puddle, when the future cannot directly affect the past?  How indeed can anything at all happen in the water <em>so that</em> it will, in the future, be at the bottom?  No; I should try to understand the water's behavior using only local causes, found in the immediate past."</p> <p>It takes a deliberate act of reductionism to identify <em>telos</em> as <em>purpose</em>, and <em>purpose</em> as a mental property which is too complicated to be ontologically fundamental.  You don't realize, when you ask "What does this <em>telos</em>-imbued object do next?", that your brain is answering by calling on its own complicated planning mechanisms, that search multiple paths and do means-end reasoning.  Purpose just seems like a simple and basic property; the complexity of your brain that produces the predictions is hidden from you.  It is an act of reductionism to see <em>purpose </em>as requiring a complicated AI algorithm that needs a complicated material embodiment.</p> <p>So this is the second fallacy of teleology - to attribute goal-directed behavior to things that are not goal-directed, perhaps without even thinking of the things as alive and spirit-inhabited, but only thinking, <em>X happens in order to Y.</em>  "In order to" is <em>mentalistic language</em>, even though it doesn't seem to name a blatantly mental property like "fearful" or "thinks it can fly".</p> <p>Remember the sequence on <a href="0385.html">free will</a> [http://lesswrong.com/lw/rb/possibility_and_couldness/]?  The problem, it turned out, was that "could" was a mentalistic property - generated by the planner in the course of labeling states as reachable from the start state.  It seemed like "could" was a physical, ontological property.  When you say "could" it doesn't sound like you're talking about states of mind.  Nonetheless, the mysterious behavior of could-ness turned out to be understandable only by looking at the brain's planning mechanisms.</p> <p>Since mentalistic reasoning uses your own mind as a black box to generate its predictions, it very commonly generates <a href="0282.html">wrong questions</a> [http://lesswrong.com/lw/og/wrong_questions/] and <a href="0080.html">mysterious answers</a> [http://lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/].</p> <p>If you want to accomplish anything related to philosophy, or anything related to Artificial Intelligence, it is necessary to learn to identify mentalistic language and root it <em>all</em> out - which can only be done by analyzing innocent-seeming words like "could" or "in order to" into the complex cognitive algorithms that are their true identities.</p> <p>(If anyone accuses me of "extreme reductionism" for saying this, let me ask how likely it is that we live in an only <em>partially</em> reductionist universe.)</p> <p>The third fallacy of teleology is to commit the <a href="0284.html">Mind Projection Fallacy</a> [http://lesswrong.com/lw/oi/mind_projection_fallacy/] with respect to <em>telos,</em> supposing it to be an inherent property of an object or system.  Indeed, one does this every time one speaks of <em>the</em> purpose  <em>of</em> an event, rather than speaking of some particular agent desiring the consequences of that event.</p> <p>I suspect this is why people have trouble understanding <a href="0159.html">evolutionary psychology</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/] - in particular, why they suppose that all human acts are unconsciously directed toward reproduction.  "Mothers who loved their children outreproduced those who left their children to the wolves" becomes "natural selection produced motherly love <em>in order to</em> ensure the <a href="0163.html">survival of the species</a> [http://lesswrong.com/lw/l5/evolving_to_extinction/]" becomes "<em>the purpose</em> of acts of motherly love is to increase the mother's fitness".  Well, if a mother apparently drags her child off the train tracks because she loves the child, that's also <em>the purpose of</em> the act, right?  So by a <a href="0262.html">fallacy of compression</a> [http://lesswrong.com/lw/nw/fallacies_of_compression/] - a mental model that has one bucket where two buckets are needed - <em>the purpose</em> must be one or the other: either love or reproductive fitness.</p> <p>Similarly with those who hear of evolutionary psychology and conclude that <em>the meaning of life</em> is to increase reproductive fitness - hasn't science <em>demonstrated</em> that this is <em>the purpose</em> of all biological organisms, after all?</p> <p>Likewise with that fellow who concluded that <em>the purpose of</em> the universe is to increase entropy - the universe does so consistently, therefore it must want to do so - and that this must therefore be the meaning of life.  Pretty sad purpose, I'd say!  But of course the speaker did not seem to realize what it <em>means</em> to want to increase entropy as much as possible - what this goal really implies, that you should go around collapsing stars to black holes.  Instead the one focused on a few selected activities that increase entropy, like thinking.  You couldn't ask for a clearer illustration of a <a href="0184.html">fake utility function</a> [http://lesswrong.com/lw/lq/fake_utility_functions/].</p> <p>I call this a "teleological capture" - where someone comes to believe that the <em>telos</em> of X is Y, relative to some agent, or optimization process, or maybe just statistical tendency, from which it follows that any human or other agent who does X must have a purpose of Y in mind.  The evolutionary reason for motherly love becomes its <em>telos,</em> and seems to "capture" the apparent motives of human mothers.  The game-theoretical reason for cooperating on the Iterated Prisoner's Dilemma becomes the <em>telos</em> of cooperation, and seems to "capture" the apparent motives of human altruists, who are thus revealed as being selfish after all.  Charity increases status, which people are known to desire; therefore status is the <em>telos</em> of charity, and "captures" <em>all</em> claims to kinder motives.  Etc. etc. through half of all amateur philosophical reasoning about the meaning of life.</p> <p>These then are three fallacies of teleology:  Backward causality, anthropomorphism, and teleological capture.</p></div> <hr><p><i>Referenced by: </i><a href="0461.html">Dreams of AI Design</a> &#8226; <a href="0462.html">Against Modal Logics</a> &#8226; <a href="0477.html">Excluding the Supernatural</a> &#8226; <a href="0491.html">My Naturalistic Awakening</a> &#8226; <a href="0512.html">Why Does Power Corrupt?</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/te/three_fallacies_of_teleology/">Three Fallacies of Teleology</a></p></body></html>