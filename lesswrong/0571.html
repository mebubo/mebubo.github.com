<html><head><title>Underconstrained Abstractions</title></head><body><h1>Underconstrained Abstractions</h1><p><i>Eliezer Yudkowsky, 04 December 2008 01:58PM</i></p><div><p><strong>Followup to</strong>:  <a href="0553.html">The Weak Inside View</a> [http://lesswrong.com/lw/vz/the_weak_inside_view/]</p> <p><a href="http://www.overcomingbias.com/2008/12/test-near-apply.html">Saith Robin</a> [http://www.overcomingbias.com/2008/12/test-near-apply.html]:</p><blockquote><p>"It is easy, way too easy, to generate new mechanisms, accounts, theories, and abstractions.  To see if such things are <em>useful</em>, we need to vet them, and that is easiest "nearby", where we know a lot.  When we want to deal with or understand things "far", where we know little, we have little choice other than to rely on mechanisms, theories, and concepts that have worked well near.  Far is just the wrong place to try new things."</p></blockquote><p>Well... I understand why one would have that reaction.  But I'm not sure we can <em>really</em> get away with that.</p> <p>When possible, I try to talk in concepts that can be verified with respect to existing history.  When I talk about natural selection not running into a law of diminishing returns on genetic complexity or brain size, I'm talking about something that we can try to verify by looking at the capabilities of other organisms with brains big and small.  When I talk about the boundaries to sharing cognitive content between AI programs, you can look at the field of AI the way it works today and see that, lo and behold, there isn't a lot of cognitive content shared.</p> <p>But in my book this is just <em>one</em> trick in a <em>library</em> of methodologies for dealing with the Future, which is, in general, a hard thing to predict.</p> <p>Let's say that instead of using my complicated-sounding disjunction (many <em>different</em> reasons why the growth trajectory might contain an upward cliff, which don't <em>all</em> have to be true), I instead staked my <em>whole</em> story on the critical threshold of human intelligence.  Saying, "Look how sharp the slope is here!" - well, it would <em>sound</em> like a simpler story.  It would be closer to fitting on a T-Shirt.  And by talking about <em>just</em> that one abstraction and no others, I could make it sound like I was dealing in verified historical facts - humanity's evolutionary history is something that has already happened.</p> <p>But speaking of an abstraction being "verified" by previous history is a tricky thing.  There is this little problem of <em>underconstraint</em> - of there being more than one possible abstraction that the data "verifies".</p><a id="more"></a><p>In "<a href="0559.html">Cascades, Cycles, Insight</a> [http://lesswrong.com/lw/w5/cascades_cycles_insight/]" I said that economics does not seem to me to deal much in the origins of novel knowledge and novel designs, and said, "If I underestimate your power and merely parody your field, by all means inform me what kind of economic study has been done of such things."  This challenge was answered by <a href="0559.html">comments</a> [http://lesswrong.com/lw/w5/cascades_cycles_insight/] directing me to some papers on "endogenous growth", which happens to be the name of theories that don't take productivity improvements as exogenous forces.</p> <p>I've looked at some literature on endogenous growth.  And don't get me wrong, it's probably not too bad as economics.  However, the seminal literature talks about ideas being generated by combining other ideas, so that if you've got N ideas already and you're combining them three at a time, that's a potential N!/((3!)(N - 3!)) new ideas to explore. And then goes on to note that, in this case, there will be vastly more ideas than anyone can explore, so that the rate at which ideas are exploited will depend more on a paucity of explorers than a paucity of ideas.</p> <p>Well... first of all, the notion that "ideas are generated by combining other ideas N at a time" is not exactly an amazing AI theory; it is an economist looking at, essentially, the whole problem of AI, and trying to solve it in 5 seconds or less.  It's not as if any experiment was performed to actually watch ideas recombining.  Try to build an AI around this theory and you will find out in very short order how useless it is as an account of where ideas come from...</p> <p>But more importantly, if the only proposition you actually <em>use</em> in your theory is that there are more ideas than people to exploit them, then this is the only proposition that can even be <em>partially</em> verified by testing your theory.</p> <p>Even if a recombinant growth theory can be fit to the data, then the historical data still underconstrains the <em>many</em> possible abstractions that might describe the number of possible ideas available - any hypothesis that has around "more ideas than people to exploit them" will fit the same data equally well.  You should simply say, "I assume there are more ideas than people to exploit them", not go so far into mathematical detail as to talk about N choose 3 ideas.  It's not that the dangling math here is underconstrained by the <em>previous</em> data, but that you're not even using it <em>going forward.</em></p> <p>(And does it even fit the data?  I have friends in venture capital who would laugh like hell at the notion that there's an unlimited number of really good ideas out there.  Some kind of Gaussian or power-law or something distribution for the goodness of available ideas seems more in order...  I don't object to "endogenous growth" simplifying things for the sake of having one simplified abstraction and seeing if it fits the data well; we all have to do that.  Claiming that the underlying math doesn't <em>just</em> let you build a useful model, but <em>also</em> has a fairly direct correspondence to reality, ought to be a whole 'nother story, in economics - or so it seems to me.)</p> <p>(If I merely misinterpret the endogenous growth literature or underestimate its sophistication, by all means correct me.)</p> <p>The further away you get from highly regular things like atoms, and the closer you get to surface phenomena that are the final products of many moving parts, the more history underconstrains the abstractions that you use.  This is part of what makes futurism difficult.  If there were obviously only one story that fit the data, who would bother to use anything else?</p> <p>Is Moore's Law a story about the increase in computing power <em>over time</em> - the number of transistors on a chip, as a function of how far the planets have spun in their orbits, or how many times a light wave emitted from a cesium atom has changed phase?</p> <p>Or does the same data equally verify a hypothesis about exponential increases in investment in manufacturing facilities and R&amp;D, with an even higher exponent, showing a law of diminishing returns?</p> <p>Or is Moore's Law showing the increase in computing power, as a function of some kind of optimization pressure applied by human researchers, themselves thinking at a certain rate?</p> <p>That last one might seem hard to verify, since we've never watched what happens when a chimpanzee tries to work in a chip R&amp;D lab.  But on some raw, elemental level - would the history of the world <em>really</em> be just the same, proceeding on <em>just exactly</em> the same timeline as the planets move in their orbits, if, for these last fifty years, the researchers themselves had been running on the latest generation of computer chip at any given point?  That sounds to me even sillier than having a financial model in which there's no way to ask what happens if real estate prices go down.</p> <p>And then, when you apply the abstraction going forward, there's the question of whether there's more than one way to apply it - which is one reason why a lot of futurists tend to dwell in great gory detail on the past events that seem to support their abstractions, but just <em>assume</em> a single application forward.</p> <p>E.g. Moravec in '88, spending a lot of time talking about how much "computing power" the human brain seems to use - but much less time talking about whether an AI would use the same amount of computing power, or whether using Moore's Law to extrapolate the first supercomputer of this size is the right way to time the arrival of AI. (Moravec thought we were supposed to have AI around <em>now,</em> based on his calculations - and he <em>under</em>estimated the size of the supercomputers we'd actually have in 2008.)</p> <p>That's another part of what makes futurism difficult - after you've told your story about the past, even if it seems like an abstraction that can be "verified" with respect to the past (but what if you overlooked an alternative story for the same evidence?) that often leaves a lot of slack with regards to exactly what will happen with respect to that abstraction, going forward.</p> <p>So if it's not as simple as <em>just</em> using the one trick of finding abstractions you can easily verify on available data...</p> <p>...what are some other tricks to use?</p></div> <hr><p><i>Referenced by: </i><a href="0575.html">True Sources of Disagreement</a> &#8226; <a href="0576.html">Disjunctions, Antipredictions, Etc.</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/wh/underconstrained_abstractions/">Underconstrained Abstractions</a></p></body></html>