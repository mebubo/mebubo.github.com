<html><head><title>Fake Fake Utility Functions</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Fake Fake Utility Functions</h1><p><i>Eliezer Yudkowsky, 06 December 2007 06:30AM</i></p><div><p><strong>Followup to:</strong> Most of my posts over the last month...</p> <p>Every now and then, you run across someone who has discovered the One Great Moral Principle, of which all other values are a mere derivative consequence.</p> <p>I run across more of these people than you do.  Only in my case, it's people who know <em>the amazingly simple utility function that is all you need to program into an artificial superintelligence</em> and then everything will turn out fine...</p> <p>It's incredible how one little issue can require so much prerequisite material.  My original schedule called for "Fake Utility Functions" to follow "<a href="0148.html">Fake Justification</a> [http://lesswrong.com/lw/kq/fake_justification/]" on Oct 31.</p> <p>Talk about your <a href="0102.html">planning fallacy</a> [http://lesswrong.com/lw/jg/planning_fallacy/].  I've been planning to post on this topic in "just a few days" for the past month.  A fun little demonstration of <a href="0138.html">underestimated inferential distances.</a> [http://lesswrong.com/lw/kg/expecting_short_inferential_distances/]</p> <p>You see, before I wrote this post, it occurred to me that if I wanted to properly explain the problem of fake utility functions, it would be helpful to <em>illustrate</em> a mistake about what a simple optimization criterion implied.  The strongest real-world example I knew was the <a href="0154.html">Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/].  At first I thought I'd mention it in passing, within "Fake Utility Functions", but I decided the <a href="0154.html">Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/] was a long enough story that it needed its own blog post...</p> <p><a id="more"></a></p> <p>So I started to write "<a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]".  A few hours later, I noticed that I hadn't said anything about group selectionism yet.  I'd been too busy introducing basic evolutionary concepts. Select all the introductory stuff, <em>cut</em>, Compose New Post, <em>paste</em>, title... "<a href="0149.html">An Alien God</a> [http://lesswrong.com/lw/kr/an_alien_god/]".  Then keep writing until the "<a href="0149.html">Alien God</a> [http://lesswrong.com/lw/kr/an_alien_god/]" post gets too long, and start taking separate subjects out into their own posts: "<a href="0150.html">The Wonder of Evolution</a> [http://lesswrong.com/lw/ks/the_wonder_of_evolution/]", "<a href="0151.html">Evolutions Are Stupid</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/]", and at this point it became clear that, since I was planning to say a few words on evolution anyway, that was the time.  Besides, a basic familiarity with evolution would help to shake people loose of their human assumptions when it came to visualizing nonhuman optimization processes.</p> <p>So, finally I posted "<a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]". Now I was ready to write "Fake Utility Functions", right?  The post that was supposed to come immediately afterward?  So I thought, but each time I tried to write the post, I ended up recursing on a prerequisite post instead.  Such as "<a href="0155.html">Fake Selfishness</a> [http://lesswrong.com/lw/kx/fake_selfishness/]", "<a href="0156.html">Fake Morality</a> [http://lesswrong.com/lw/ky/fake_morality/]", and "<a href="0157.html">Fake Optimization Criteria</a> [http://lesswrong.com/lw/kz/fake_optimization_criteria/]".</p> <p>When I got to "<a href="0157.html">Fake Optimization Criteria</a> [http://lesswrong.com/lw/kz/fake_optimization_criteria/]", I really thought I could do "Fake Utility Functions" the next day.  But then it occurred to me that I'd never explained why a simple utility function <em>wouldn't</em> be enough.  We are a thousand shards of desire, as I said in "<a href="0161.html">Thou Art Godshatter</a> [http://lesswrong.com/lw/l3/thou_art_godshatter/]".  Only that first required discussing "<a href="0159.html">Evolutionary Psychology</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/]", which required explaining that human minds are "<a href="0158.html">Adaptation-Executers, not Fitness-Maximizers</a> [http://lesswrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/]", plus the difference between "<a href="0160.html">Protein Reinforcement and DNA Consequentialism</a> [http://lesswrong.com/lw/l2/protein_reinforcement_and_dna_consequentialism/]".</p> <p>Furthermore, I'd never really explained the difference between "<a href="0162.html">Terminal Values and Instrumental Values</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/]", without which I could hardly talk about utility functions.</p> <p>Surely <em>now</em> I was ready?  Yet I thought about conversations I'd had over the years, and how people seem to think a simple instruction like "Get my mother out of that burning building!" contains all the motivations that shape a human plan to rescue her, so I thought that first I'd do "<a href="0171.html">The Hidden Complexity of Wishes</a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/]". But, really, the hidden complexity of planning, and all the special cases needed to patch the genie's wish, was part of the general problem of recording outputs without absorbing the process that generates the outputs - as I explained in "<a href="0167.html">Artificial Addition</a> [http://lesswrong.com/lw/l9/artificial_addition/]" and "<a href="0168.html">Truly Part Of You</a> [http://lesswrong.com/lw/la/truly_part_of_you/]".  You don't want to keep the local goal description and discard the nonlocal utility function:  "<a href="0170.html">Leaky Generalizations</a> [http://lesswrong.com/lw/lc/leaky_generalizations/]" and "<a href="0172.html">Lost Purposes</a> [http://lesswrong.com/lw/le/lost_purposes/]".</p> <p>Plus it occurred to me that evolution itself made an interesting genie, so before all that, came "<a href="0166.html">Conjuring An Evolution To Serve You</a> [http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/]".</p> <p>One kind of lost purpose is artificial pleasure, and "happiness" is one of the Fake Utility Functions I run into more often:  "<a href="0169.html">Not for the Sake of Happiness (Alone)</a> [http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/]".  Similarly, it was worth taking the time to establish that fitness is not always your friend ("<a href="0163.html">Evolving to Extinction</a> [http://lesswrong.com/lw/l5/evolving_to_extinction/]") and that not everything in the universe is subject to significant selection pressures ("<a href="0164.html">No Evolutions for Corporations or Nanodevices</a> [http://lesswrong.com/lw/l6/no_evolutions_for_corporations_or_nanodevices/]"), to avoid the Fake Utility Function of "genetic fitness".</p> <p>Right after "<a href="0172.html">Lost Purposes</a> [http://lesswrong.com/lw/le/lost_purposes/]" seemed like a good time to point out the deep link between keeping track of your original goal and keeping track of your original question:  "<a href="0173.html">Purpose and Pragmatism</a> [http://lesswrong.com/lw/lf/purpose_and_pragmatism/]".</p> <p>Into the home stretch!  No, wait, this would be a good time to discuss "<a href="0180.html">Affective Death Spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/]", since that's one of the main things that goes wrong when someone discovers The One True Valuable Thingy - they keep finding nicer and nicer things to say about it.  Well, you can't discuss affective death spirals unless you first discuss "<a href="0174.html">The Affect Heuristic</a> [http://lesswrong.com/lw/lg/the_affect_heuristic/]", but I'd been meaning to do that for a while anyway.  "<a href="0175.html">Evaluability</a> [http://lesswrong.com/lw/lh/evaluability_and_cheap_holiday_shopping/]" illustrates the affect heuristic and leads to an important point about "<a href="0176.html">Unbounded Scales and Futurism</a> [http://lesswrong.com/lw/li/unbounded_scales_huge_jury_awards_futurism/]".  The second key to affective death spirals is "<a href="0177.html">The Halo Effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/]", which we can see illustrated in "<a href="0178.html">Superhero Bias</a> [http://lesswrong.com/lw/lk/superhero_bias/]" and "<a href="0179.html">Mere Messiahs</a> [http://lesswrong.com/lw/ll/mere_messiahs/]".  Then it's on to affective death spirals and how to "<a href="0181.html">Resist the Happy Death Spiral</a> [http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/]" and "<a href="0182.html">Uncritical Supercriticality</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/]".</p> <p>A bonus irony is that "Fake Utility Functions" isn't a grand climax.  It's just one of many <em>Less Wrong</em> posts relevant to my AI work, with plenty more scheduled.  This particular post just turned out to require <em>just a little more</em> prerequisite material which - I thought on each occasion - <em>I would have to write anyway, sooner or later.</em></p> <p>And that's why blogging is difficult, and why it is necessary, at least for me.  I would have been doomed, yea, utterly doomed, if I'd tried to write all this as one publication rather than as a series of blog posts.  One month is <em>nothing</em> for this much material.</p> <p>But now, it's done!  Now, after only slightly more than an extra month of prerequisite material, I can do the blog post originally scheduled for November 1st!</p> <p>Except...</p> <p>Now that I think about it...</p> <p>This post is pretty long already, right?</p> <p>So I'll do the <em>real</em> "Fake Utility Functions" <a href="0184.html">tomorrow</a> [http://lesswrong.com/lw/lq/fake_utility_functions/].</p></div> <hr><p><i>Referenced by: </i><a href="0184.html">Fake Utility Functions</a> &#8226; <a href="0312.html">GAZP vs. GLUT</a> &#8226; <a href="0391.html">Heading Toward Morality</a> &#8226; <a href="0395.html">The Psychological Unity of Humankind</a> &#8226; <a href="0431.html">Setting Up Metaethics</a> &#8226; <a href="0579.html">What I Think, If Not Why</a> &#8226; <a href="0629.html">Value is Fragile</a> &#8226; <a href="0640.html">...And Say No More Of It</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/lp/fake_fake_utility_functions/">Fake Fake Utility Functions</a></p></body></html>