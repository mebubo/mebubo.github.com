<html><head><title>Devil's Offers</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Devil's Offers</h1><p><i>Eliezer Yudkowsky, 25 December 2008 05:00PM</i></p><div><p><em></em><strong>Previously in series</strong>:  <a href="0592.html">Harmful Options</a> [http://lesswrong.com/lw/x2/harmful_options/]</p> <p>An iota of <a href="0131.html">fictional evidence</a> [http://lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/] from <a href="http://books.google.com/books?id=CDjJ7K3bm28C&amp;pg=PA196&amp;lpg=PA196&amp;dq=%22Helion+had+leaned+and+said%22&amp;source=bl&amp;ots=QS-yd1jvWz&amp;sig=twyQUHiB9O-nPsw0BcEAbuAnkgw&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result#PPA196,M1"><em>The Golden Age</em></a> [http://books.google.com/books?id=CDjJ7K3bm28C&amp;pg=PA196&amp;lpg=PA196&amp;dq=%22Helion+had+leaned+and+said%22&amp;source=bl&amp;ots=QS-yd1jvWz&amp;sig=twyQUHiB9O-nPsw0BcEAbuAnkgw&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result#PPA196,M1] by John C. Wright:</p> <p style="margin-left: 40px;">&#160;&#160;&#160; Helion had leaned and said, "Son, once you go in there, the full powers and total command structures of the Rhadamanth Sophotech will be at your command.  You will be invested with godlike powers; but you will still have the passions and distempers of a merely human spirit.  There are two temptations which will threaten you.  First, you will be tempted to remove your human weaknesses by abrupt mental surgery.  The Invariants do this, and to a lesser degree, so do the White Manorials, abandoning humanity to escape from pain.  Second, you will be tempted to indulge your human weakness.  The Cacophiles do this, and to a lesser degree, so do the Black Manorials.  Our society will gladly feed every sin and vice and impulse you might have; and then stand by helplessly and watch as you destroy yourself; because the first law of the Golden Oecumene is that no peaceful activity is forbidden.  Free men may freely harm themselves, provided only that it is only themselves that they harm."<br>&#160;&#160;&#160; Phaethon knew what his sire was intimating, but he did not let himself feel irritated.  Not today.  Today was the day of his majority, his emancipation; today, he could forgive even Helion's incessant, nagging fears.<br>&#160;&#160;&#160; Phaethon also knew that most Rhadamanthines were not permitted to face the Noetic tests until they were octogenerians; most did not pass on their first attempt, or even their second.  Many folk were not trusted with the full powers of an adult until they reached their Centennial.  Helion, despite criticism from the other Silver-Gray branches, was permitting Phaethon to face the tests five years early...</p> <p><a id="more"></a></p> <p style="margin-left: 40px;">&#160;&#160;&#160; Then Phaethon said, "It's a paradox, Father.  I cannot be, at the same time and in the same sense, a child and an adult.  And, if I am an adult, I cannot be, at the same time, free to make my own successes, but not free to make my own mistakes."<br>&#160;&#160;&#160; Helion looked sardonic.  "'Mistake' is such a simple word.  An adult who suffers a moment of foolishness or anger, one rash moment, has time enough to delete or destroy his own free will, memory, or judgment.  No one is allowed to force a cure on him.  No one can restore his sanity against his will.  And so we all stand quietly by, with folded hands and cold eyes, and meekly watch good men annihilate themselves.  It is somewhat... quaint... to call such a horrifying disaster a 'mistake.'"</p> <p>Is this the best Future we could possibly get to&#8212;the Future where you must be absolutely stern and resistant throughout your entire life, because <em>one moment of weakness</em> is enough to betray you to <a href="0017.html">overwhelming temptation</a> [http://lesswrong.com/lw/h3/superstimuli_and_the_collapse_of_western/]?</p> <p>Such flawless perfection would be easy enough for a superintelligence, perhaps&#8212;for a <em>true </em>adult&#8212;but for a human, even a hundred-year-old human, it seems like a dangerous and inhospitable place to live.  Even if you are strong enough to always choose correctly&#8212;maybe you don't want to <em>have </em>to be so strong, always at every moment.</p> <p>This is the great flaw in Wright's otherwise shining Utopia&#8212;that the Sophotechs are <em>helpfully </em>offering up overwhelming temptations to people who would not be at <em>quite </em>so much risk from only <em>themselves</em>.  (Though if not for this flaw in Wright's Utopia, he would have had no story...)</p> <p>If I recall correctly, it was while reading <em>The Golden Age</em> that I generalized the principle "<a href="0589.html">Offering people powers beyond their own is not always helping them.</a> [http://lesswrong.com/lw/wz/living_by_your_own_strength/]"</p> <p>If you couldn't just ask a Sophotech to edit your neural networks&#8212;and you couldn't buy a standard package at the supermarket&#8212;but, rather, had to study neuroscience yourself until you could do it with your own hands&#8212;then that would act as something of a natural limiter.  Sure, there are pleasure centers that would be relatively easy to stimulate; but we don't tell you where they are, so you have to do your own neuroscience.  Or we don't sell you your own neurosurgery kit, so you have to build it yourself&#8212;metaphorically speaking, anyway&#8212;</p> <p>But you see the idea: it is not so terrible a disrespect for free will, to live in a world in which people are free to shoot their feet off <em>through their own strength</em>&#8212;in the hope that by the time they're smart enough to do it <em>under their own power</em>, they're smart enough <em>not </em>to.</p> <p>The more dangerous and destructive the act, the more you require people to do it without external help.  If it's really dangerous, you don't just require them to do their own engineering, but to do their own science.  A <a href="0566.html">singleton</a> [http://lesswrong.com/lw/wc/singletons_rule_ok/] might be justified in <a href="0302.html">prohibiting standardized textbooks</a> [http://lesswrong.com/lw/p0/to_spread_science_keep_it_secret/] in certain fields, so that people have to do their own science&#8212;make their own discoveries, learn to rule out their own stupid hypotheses, and fight their own overconfidence.  Besides, everyone should experience <a href="0294.html">the joy of major discovery</a> [http://lesswrong.com/lw/os/joy_in_discovery/] at least once in their lifetime, and to do this properly, you may have to prevent spoilers from entering the public discourse.  So you're getting <a href="0302.html">three</a> [http://lesswrong.com/lw/p0/to_spread_science_keep_it_secret/] social benefits at once, here.</p> <p>But now I'm trailing off into plots for SF novels, instead of Fun Theory per se.  (It can be fun to muse how I would create the world if I had to order it according to my own childish wisdom, but in real life one rather prefers to <a href="0579.html">avoid that scenario</a> [http://lesswrong.com/lw/wp/what_i_think_if_not_why/].)</p> <p>As a matter of Fun Theory, though, you can imagine a <em>better </em>world than the Golden Oecumene depicted above&#8212;it is not the <em>best </em>world imaginable, fun-theoretically speaking.  We would prefer (if attainable) a world in which people own their own mistakes and their own successes, and yet they are not given loaded handguns on a silver platter, nor do they perish through <a href="0171.html">suicide by genie bottle</a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/].</p> <p>Once you imagine a world in which people can shoot off their own feet <em>through their own strength,</em> are you making that world incrementally better by offering incremental help along the way?</p> <p>It's one matter to prohibit people from using dangerous powers that they have grown enough to acquire naturally&#8212;to literally <em>protect them from themselves.</em>  One expects that if a mind kept getting smarter, at some eudaimonic rate of intelligence increase, then&#8212;if you took the most obvious course&#8212;the mind would eventually become able to edit its own source code, and bliss itself out <a href="0385.html">if it chose to do so</a> [http://lesswrong.com/lw/rb/possibility_and_couldness/].  Unless the mind's growth were steered onto a non-obvious course, or monitors were mandated to prohibit that event...  To protect people <em>from their own powers</em> might take some twisting.</p> <p>To descend from above and <em>offer dangerous powers as an untimely gift</em>, is another matter entirely.  That's why the title of this post is "Devil's Offers", not "Dangerous Choices".</p> <p>And to allow dangerous powers to be sold in a marketplace&#8212;or alternatively to prohibit them from being transferred from one mind to another&#8212;that is somewhere in between.</p> <p>John C. Wright's writing has a particular poignancy for me, for in my <a href="0484.html">foolish youth</a> [http://lesswrong.com/lw/u2/the_sheer_folly_of_callow_youth/] I thought that something very much like this scenario was a good idea&#8212;that a benevolent superintelligence ought to go around offering people lots of options, and doing as it was asked.</p> <p>In retrospect, this was a case of a pernicious distortion where you end up believing things that are easy to market to other people.</p> <p>I know someone who drives across the country on long trips, rather than flying.  Air travel scares him.  Statistics, naturally, show that flying a given distance is much safer than driving it.  But some people fear too much the <em>loss of control</em> that comes from not having their own hands on the steering wheel.  It's a common complaint.</p> <p>The future sounds less scary if you imagine yourself having lots of control over it.  For every awful thing that you imagine happening to you, you can imagine, "But I won't choose that, so it will be all right."</p> <p>And if it's not your own hands on the steering wheel, you think of scary things, and imagine, "What if this is chosen <em>for </em>me, and I can't say no?"</p> <p>But in real life rather than imagination, human choice is a fragile thing.  If the whole field of heuristics and biases teaches us anything, it surely teaches us that.  Nor has it been the verdict of experiment, that humans correctly estimate the flaws of their own decision mechanisms.</p> <p>I flinched away from that thought's implications, not so much because I feared superintelligent paternalism <em>myself</em>, but because I feared what other people would say of that position.  If I believed it, I would have to defend it, so I managed not to believe it.  Instead I told people not to worry, a superintelligence would surely respect their decisions (and even believed it myself).  A very pernicious sort of self-deception.</p> <p>Human governments are made up of humans who are foolish like ourselves, plus they have poor incentives.  Less skin in the game, and <a href="0512.html">specific human brainware to be corrupted by wielding power</a> [http://lesswrong.com/lw/uu/why_does_power_corrupt/].  So we've learned the historical lesson to be wary of ceding control to human bureaucrats and politicians.  We may even be emotionally hardwired to resent the loss of anything we perceive as power.</p> <p>Which is just to say that people are biased, by instinct, by <a href="0434.html">anthropomorphism</a> [http://lesswrong.com/lw/so/humans_in_funny_suits/], and by narrow experience, to <a href="0513.html"><em>under</em>estimate how much they could potentially trust a superintelligence</a> [http://lesswrong.com/lw/uv/ends_dont_justify_means_among_humans/] which lacks a human's corruption circuits, doesn't easily make certain kinds of mistakes, and has strong overlap between its motives and your own interests.</p> <p>Do you trust yourself?  Do you trust yourself to know when to trust yourself?  If you're dealing with a superintelligence kindly enough to care about you at all, rather than disassembling you for raw materials, are you wise to second-guess its choice of <em>who </em>it thinks should decide?  Do you think you have a superior epistemic vantage point here, or what?</p> <p>Obviously we should not trust all agents who claim to be trustworthy&#8212;especially if they are <em>weak </em>enough, relative to us, to <em>need </em>our goodwill.  But I am quite ready to accept that a benevolent superintelligence may not offer certain choices.</p> <p>If you <em>feel safer</em> driving than flying, because that way it's your own hands on the steering wheel, statistics be damned&#8212;</p> <p>&#8212;then maybe it isn't <em>helping </em>you, for a superintelligence to offer you the option of driving.</p> <p>Gravity doesn't ask you if you would like to float up out of the atmosphere into space and die.  But you don't go around complaining that gravity is a tyrant, right?  You can build a spaceship if you work hard and study hard.  It would be a more dangerous world if your six-year-old son could do it in an hour using string and cardboard.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0594.html">Nonperson Predicates</a> [http://lesswrong.com/lw/x4/nonperson_predicates/]"</p> <p style="text-align:right">Previous post: "<a href="0592.html">Harmful Options</a> [http://lesswrong.com/lw/x2/harmful_options/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0592.html">Harmful Options</a></p></td><td><p><i>Next: </i><a href="0594.html">Nonperson Predicates</a></p></td></tr></table><p><i>Referenced by: </i><a href="0592.html">Harmful Options</a> &#8226; <a href="0594.html">Nonperson Predicates</a> &#8226; <a href="0611.html">Eutopia is Scary</a> &#8226; <a href="0614.html">Justified Expectation of Pleasant Surprises</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0626.html">31 Laws of Fun</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/x3/devils_offers/">Devil's Offers</a></p></body></html>