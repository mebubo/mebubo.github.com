<html><head><title>My Childhood Role Model</title></head><body><h1>My Childhood Role Model</h1><p><i>Eliezer Yudkowsky, 23 May 2008 08:51AM</i></p><div><p><strong>Followup to</strong>:  <a href="0358.html">That Alien Message</a> [http://lesswrong.com/lw/qk/that_alien_message/]</p> <p>When I lecture on the Singularity, I often draw a graph of the "scale of intelligence" as it appears in everyday life:</p> <p><a href="http://lesswrong.com/static/imported/2008/05/22/mindscaleparochial.png"><img src="6eae5ca8.png" title="Mindscaleparochial" height="47" width="399" alt="Mindscaleparochial" border="0"></a> [http://lesswrong.com/static/imported/2008/05/22/mindscaleparochial.png]</p> <p>But this is a rather <em>parochial</em> view of intelligence.  Sure, in everyday life, we only deal socially with other humans&#8212;only other humans are partners in the great game&#8212;and so we only <em>meet the minds</em> of intelligences ranging from village idiot to Einstein.  But what we really need to talk about Artificial Intelligence or theoretical optima of rationality, is <em>this</em> intelligence scale:</p> <p><a href="http://lesswrong.com/static/imported/2008/05/21/mindscalereal.png"><img src="81439a5e.png" title="Mindscalereal" height="63" width="400" alt="Mindscalereal" border="0"></a> [http://lesswrong.com/static/imported/2008/05/21/mindscalereal.png]</p> <p>For us humans, it seems that the scale of intelligence runs from "village idiot" at the bottom to "Einstein" at the top.  Yet the distance from "village idiot" to "Einstein" is tiny, in the space of <em>brain designs.  </em>Einstein and the village idiot both have a prefrontal cortex, a hippocampus, a cerebellum...</p> <p>Maybe Einstein has some minor genetic differences from the village idiot, engine tweaks.  But the brain-design-distance between Einstein and the village idiot is nothing remotely like the brain-design-distance between the village idiot and a chimpanzee.  A chimp couldn't tell the difference between Einstein and the village idiot, and our descendants may not see much of a difference either.</p> <p><a id="more"></a></p> <p>Carl Shulman has observed that some academics who talk about transhumanism, seem to use the following scale of intelligence:</p> <p><a href="http://lesswrong.com/static/imported/2008/05/21/mindscaleacademic.png"><img src="73bd0fcc.png" title="Mindscaleacademic" height="97" width="400" alt="Mindscaleacademic" border="0"></a> [http://lesswrong.com/static/imported/2008/05/21/mindscaleacademic.png]</p> <p>Douglas Hofstadter actually said something like this, at the 2006 Singularity Summit.  He looked at my diagram showing the "village idiot" next to "Einstein", and said, "That seems wrong to me; I think Einstein should be way off on the right."</p> <p>I was speechless.  Especially because this was <em>Douglas Hofstadter,</em> one of my childhood heroes.  It revealed a <a href="0138.html">cultural gap</a> [http://lesswrong.com/lw/kg/expecting_short_inferential_distances/] that I had never imagined existed.</p> <p>See, for me, what you would find toward the right side of the scale, was a Jupiter Brain.  Einstein did not <em>literally</em> have a brain the size of a planet.</p> <p>On the right side of the scale, you would find Deep Thought&#8212;Douglas Adams's original version, thank you, not the chessplayer.  The computer so intelligent that even before its stupendous data banks were connected, when it was switched on for the first time, it started from <em>I think therefore I am</em> and got as far as deducing the existence of rice pudding and income tax before anyone managed to shut it off.</p> <p>Toward the right side of the scale, you would find the Elders of Arisia, galactic overminds, Matrioshka brains, and the better class of God.  At the <em>extreme</em> right end of the scale, Old One and the Blight.</p> <p>Not frickin' Einstein.</p> <p>I'm sure Einstein was very smart for a human.  I'm sure a General Systems Vehicle would think that was very cute of him.</p> <p>I call this a "cultural gap" because I was introduced to the concept of a Jupiter Brain at the age of twelve.</p> <p>Now all of this, of course, is <a href="0131.html">the logical fallacy of generalization from fictional evidence</a> [http://lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/].</p> <p>But it is an example of why&#8212;logical fallacy or not&#8212;I suspect that reading science fiction does have a helpful effect on futurism.  Sometimes the alternative to a fictional acquaintance with worlds outside your own, is to have a mindset that is absolutely <a href="http://www.beloit.edu/~pubaff/mindset/2003.php">stuck in one era</a> [http://www.beloit.edu/~pubaff/mindset/2003.php]:  A world where humans exist, and have always existed, and always will exist.</p> <p>The universe is 13.7 billion years old, people!  <em>Homo sapiens sapiens</em> have only been around for a hundred thousand years or thereabouts!</p> <p>Then again, I have met some people who never read science fiction, but who do seem able to imagine outside their own world.  And there are science fiction fans who don't get it.  I wish I knew what "it" was, so I could bottle it.</p> <p>Yesterday, I wanted to talk about the <em>efficient use of evidence,</em> i.e., Einstein was cute for a human but in an absolute sense he was around as efficient as the US Department of Defense.</p> <p>So I had to talk about <a href="0358.html">a civilization that included thousands of Einsteins, thinking for decades</a> [http://lesswrong.com/lw/qk/that_alien_message/].  Because if I'd just depicted a Bayesian superintelligence in a box, looking at a webcam, people would think: "But... how does it know how to interpret a 2D picture?"  They wouldn't put <em>themselves</em> in the shoes of the mere machine, even if it was called a "Bayesian superintelligence"; they wouldn't apply even their <em>own</em> creativity to the problem of what you could extract from looking at a grid of bits.</p> <p>It would just be a ghost in a box, that happened to be called a "Bayesian superintelligence".  The ghost hasn't been told anything about how to interpret the input of a webcam; so, in their mental model, the ghost does not know.</p> <p>As for whether it's realistic to suppose that one Bayesian superintelligence can "do all that"... i.e., the stuff that occurred to me on first sitting down to the problem, writing out the story as I went along...</p> <p>Well, let me put it this way:  Remember how <a href="0347.html">Jeffreyssai</a> [http://lesswrong.com/lw/q9/the_failures_of_eld_science/] pointed out that if the experience of having an important insight doesn't take more than 5 minutes, this theoretically gives you time for 5760 insights per month?  Assuming you sleep 8 hours a day and have no important insights while sleeping, that is.</p> <p>Now humans cannot use themselves this efficiently.  But humans are not adapted for the task of scientific research.  Humans are adapted to chase deer across the savanna, throw spears into them, cook them, and then&#8212;this is probably the part that takes most of the brains&#8212;cleverly argue that they deserve to receive a larger share of the meat.</p> <p>It's amazing that Albert Einstein managed to repurpose a brain like that for the task of doing physics.  This deserves applause.  It deserves more than applause, it deserves a place in the Guinness Book of Records.  Like successfully building the fastest car ever to be made entirely out of Jello.</p> <p>How poorly did the <a href="0149.html">blind idiot god</a> [http://lesswrong.com/lw/kr/an_alien_god/] (evolution) <em>really</em> design the human brain?</p> <p>This is something that can only be grasped through much study of cognitive science, until the full horror begins to dawn upon you.</p> <p>All the biases we have discussed here should at least be a hint.</p> <p>Likewise the fact that the human brain must use its full power and concentration, with trillions of synapses firing, to multiply out two three-digit numbers without a paper and pencil.</p> <p>No more than Einstein made efficient use of his sensory data, did his brain make efficient use of his neurons firing.</p> <p>Of course I have certain ulterior motives in saying all this.  But let it also be understood that, years ago, when I set out to be a rationalist, the impossible unattainable ideal of intelligence that inspired me, was never Einstein.</p> <p>Carl Schurz said:</p> <blockquote> <p style="line-height: 12pt;"><span style="color: black;">"Ideals are like stars. You will not succeed in touching them with your hands. But, like the seafaring man on the desert of waters, you choose them as your guides and following them you will reach your destiny."</span></p> </blockquote> <p>So now you've caught a glimpse of one of my great childhood role models&#8212;my dream of an AI.  Only the dream, of course, the reality not being available.  I reached up to that dream, once upon a time.</p> <p>And this helped me to some degree, and harmed me to some degree.</p> <p>For some ideals are like dreams: they come from within us, not from outside.  Mentor of Arisia proceeded from E. E. "doc" Smith's imagination, not from any real thing.  If you imagine what a Bayesian superintelligence would say, it is only your own mind talking.  Not like a star, that you can follow from outside.  You have to guess where your ideals are, and if you guess wrong, you go astray.</p> <p>But do not limit your ideals to mere stars, to mere humans who actually existed, especially if they were born more than fifty years before you and are dead.  Each <a href="0193.html">succeeding generation</a> [http://lesswrong.com/lw/lz/guardians_of_the_truth/] has a chance to <a href="0022.html">do better</a> [http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/]. To let your ideals be composed only of humans, especially dead ones, is to <a href="0193.html">limit yourself to what has already been accomplished</a> [http://lesswrong.com/lw/lz/guardians_of_the_truth/].  You will ask yourself, "Do I dare to do this thing, which Einstein could not do?  Is this not <em>l&#232;se majest&#233;?</em>"  Well, if Einstein had sat around asking himself, "Am I allowed to do better than Newton?" he would not have gotten where he did.  This is the problem with following stars; at best, it gets you to the star.</p> <p>Your era supports you more than you realize, in unconscious assumptions, in subtly improved technology of mind.  Einstein was a nice fellow, but he talked a deal of nonsense about an impersonal God, which shows you how well he understood the art of careful thinking <a href="0009.html">at a higher level of abstraction than his own field</a> [http://lesswrong.com/lw/gv/outside_the_laboratory/].  It may seem less like sacrilege to <a href="0347.html">think that</a> [http://lesswrong.com/lw/q9/the_failures_of_eld_science/], if you have at least one imaginary galactic supermind to compare with Einstein, so that he is not the far right end of your intelligence scale.</p> <p>If you only try to do what seems humanly possible, you will ask too little of yourself.  When you imagine reaching up to some higher and inconvenient goal, all the convenient reasons why it is "not possible" leap readily to mind.</p> <p>The most important role models are dreams: they come from within ourselves.  To dream of anything less than what you conceive to be perfection, is to draw on less than the full power of the part of yourself that dreams.</p> <p> </p> <p style="text-align:right">Part of <a href="0379.html"><em>The Quantum Physics Sequence</em></a> [http://lesswrong.com/lw/r5/the_quantum_physics_sequence/]</p> <p style="text-align:right">Next post: "<a href="0366.html">Einstein's Superpowers</a> [http://lesswrong.com/lw/qs/einsteins_superpowers/]"</p> <p style="text-align:right">Previous post: "<a href="0358.html">That Alien Message</a> [http://lesswrong.com/lw/qk/that_alien_message/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq13.html">Sequence 13: Quantum Physics</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0358.html">That Alien Message</a></p></td><td><p><i>Next: </i><a href="0366.html">Einstein's Superpowers</a></p></td></tr></table><p><i>Referenced by: </i><a href="0358.html">That Alien Message</a> &#8226; <a href="0366.html">Einstein's Superpowers</a> &#8226; <a href="0371.html">Timeless Identity</a> &#8226; <a href="0379.html">The Quantum Physics Sequence</a> &#8226; <a href="0388.html">Grasping Slippery Things</a> &#8226; <a href="0395.html">The Psychological Unity of Humankind</a> &#8226; <a href="0528.html">Measuring Optimization Power</a> &#8226; <a href="0530.html">Economic Definition of Intelligence?</a> &#8226; <a href="0531.html">Intelligence in Economics</a> &#8226; <a href="0568.html">Recursive Self-Improvement</a> &#8226; <a href="0601.html">Free to Optimize</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ql/my_childhood_role_model/">My Childhood Role Model</a></p></body></html>