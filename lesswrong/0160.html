<html><head><title>Protein Reinforcement and DNA Consequentialism</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Protein Reinforcement and DNA Consequentialism</h1><p><i>Eliezer Yudkowsky, 13 November 2007 01:34AM</i></p><div><p><strong>Followup to</strong>:  <a href="0159.html">Evolutionary Psychology</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/]</p> <p>It takes <a href="0151.html">hundreds of generations</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/] for a simple beneficial mutation to promote itself to universality in a gene pool.  Thousands of generations, or even millions, to create complex interdependent machinery.</p> <p>That's some slow learning there.  Let's say you're building a squirrel, and you want the squirrel to know locations for finding nuts.  Individual nut trees don't last for the thousands of years required for natural selection.  You're going to have to learn using proteins.  You're going to have to build a brain.</p> <p><a id="more"></a></p> <p>Protein computers and sensors can learn by looking, much faster than DNA can learn by mutation and selection.  And yet (until <em>very</em> recently) the protein learning machines only learned in narrow, specific domains.  Squirrel brains learn to find nut trees, but not to build gliders - as flying squirrel DNA is slowly learning to do.  The protein computers learned faster than DNA, but much less generally.</p> <p>How the heck does a double-stranded molecule that fits inside a cell nucleus, come to embody truths that baffle a whole damn squirrel brain?</p> <p>Consider the high-falutin' abstract thinking that modern evolutionary theorists do in order to understand how adaptations increase inclusive genetic fitness.  Reciprocal altruism, evolutionarily stable strategies, deterrence, costly signaling, sexual selection - how many <em>humans</em> explicitly represent this knowledge?  Yet DNA can learn it without a protein computer.</p> <p>There's a long chain of causality whereby a male squirrel, eating a nut today, produces more offspring months later:  Chewing and swallowing food, to digesting food, to burning some calories today and turning others into fat, to burning the fat through the winter, to surviving the winter, to mating with a female, to the sperm fertilizing an egg inside the female, to the female giving birth to an offspring that shares 50% of the squirrel's genes.</p> <p>With the sole exception of humans, no protein brain can <em>imagine</em> chains of causality that long, that abstract, and crossing that many domains.  With one exception, no protein brain is even <em>capable</em> of drawing the consequential link from chewing and swallowing to inclusive reproductive fitness.</p> <p>Yet natural selection exploits links between local actions and distant reproductive benefits.  In wide generality, across domains, and through levels of abstraction that confuse some humans.  Because - of course - the basic evolutionary idiom works <em>through</em> the actual real-world consequences, avoiding the difficulty of having a brain imagine them.</p> <p>Naturally, this also misses the <em>efficiency</em> of having a brain imagine consequences.  It takes millions of years and billions of dead bodies to build complex machines this way.  And if you want to memorize the location of a nut tree, you're out of luck.</p> <p>Gradually DNA acquired the ability to build protein computers, brains, that could learn small modular facets of reality like the location of nut trees. To call these brains "limited" implies that a speed limit was tacked onto a general learning device, which isn't what happened.  It's just that the incremental successes of particular mutations tended to build out into domain-specific nut-tree-mapping programs.  (If you know how to program, you can verify for yourself that it's easier to build a nut-tree-mapper than an Artificial General Intelligence.)</p> <p>One idiom that brain-building DNA seems to have hit on, over and over, is reinforcement learning - repeating policies similar to policies previously rewarded.  If a food contains lots of calories and doesn't make you sick, then eat more foods that have similar tastes.  This doesn't require a brain that visualizes the whole chain of digestive causality.</p> <p>Reinforcement learning isn't trivial:  You've got to chop up taste space into neighborhoods of similarity, and stick a sensor in the stomach to detect calories or indigestion, and do some kind of long-term-potentiation that strengthens the eating impulse.  But it seems much easier for evolution to hit on reinforcement learning, than a brain that accurately visualizes the digestive system, let alone a brain that accurately visualizes the reproductive consequences N months later.</p> <p>(This efficiency does come at a price:  If the environment changes, making food no longer scarce and famines improbable, the organisms may go on eating food until they explode.)</p> <p>Similarly, a bird doesn't have to cognitively model the airflow over its wings.  It just has to track which wing-flapping policies cause it to lurch.</p> <p>Why not learn to like food based on reproductive success, so that you'll stop liking the taste of candy if it stops leading to reproductive success?  Why don't birds wait and see which wing-flapping policies result in <em>more eggs,</em> not just more stability?</p> <p>Because it takes too long.  Reinforcement learning still requires you to wait for the detected consequences before you learn.</p> <p>Now, if a protein brain could <em>imagine</em> the consequences, <em>accurately,</em> it wouldn't need a reinforcement sensor that waited for them to <em>actually happen</em>.</p> <p>Put a food reward in a transparent box.  Put the corresponding key, which looks unique and uniquely corresponds to that box, in another transparent box.  Put the key to <em>that</em> box in another box.  Do this with five boxes.  Mix in another sequence of five boxes that doesn't lead to a food reward.  Then offer a choice of two keys, one which starts the sequence of five boxes leading to food, one which starts the sequence leading nowhere.</p> <p>Chimpanzees can learn to do this.  (Dohl 1970.)  So consequentialist reasoning, backward chaining from goal to action, is not <em>strictly </em>limited to <em>Homo sapiens.</em></p> <p>But as far as I know, no non-primate species can pull that trick.  And working with a few transparent boxes is nothing compared to the kind of high-falutin' cross-domain reasoning you would need to causally link food to inclusive fitness.  (Never mind linking reciprocal altruism to inclusive fitness).  Reinforcement learning seems to evolve a lot more easily.</p> <p>When natural selection builds a digestible-calorie-sensor linked by reinforcement learning to taste, then the DNA itself embodies the implicit belief that calories lead to reproduction.  So the <em>long-term</em>, complicated, cross-domain, <em>distant</em> link from calories to reproduction, is learned by natural selection - it's implicit in the reinforcement learning mechanism that uses calories as a reward signal.</p> <p>Only <em>short-term </em>consequences, which the protein brains can quickly observe and easily learn from, get hooked up to protein learning.  The DNA builds a protein computer that seeks <em>calories,</em> rather than, say, chewiness.  Then the protein computer learns which tastes are caloric.  (Oversimplified, I know.  Lots of inductive hints embedded in this machinery.)</p> <p>But the DNA had better hope that its protein computer never ends up in an environment where calories are bad for it...  or where sexual pleasure stops correlating to reproduction... or where there are marketers that intelligently <a href="0017.html">reverse-engineer reward signals</a> [http://lesswrong.com/lw/h3/superstimuli_and_the_collapse_of_western/]...</p></div> <hr><p><i>Referenced by: </i><a href="0161.html">Thou Art Godshatter</a> &#8226; <a href="0183.html">Fake Fake Utility Functions</a> &#8226; <a href="0394.html">Optimization and the Singularity</a> &#8226; <a href="0518.html">Ethical Inhibitions</a> &#8226; <a href="0606.html">Emotional Involvement</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/l2/protein_reinforcement_and_dna_consequentialism/">Protein Reinforcement and DNA Consequentialism</a></p></body></html>