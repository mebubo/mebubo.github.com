<html><head><title>A Priori</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>A Priori</h1><p><i>Eliezer Yudkowsky, 08 October 2007 09:02PM</i></p><div><p><strong>See also:</strong>  <a href="0113.html">Comments</a> [http://lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/] on <a href="0113.html">"How to Convince Me That 2 + 2 = 3"</a> [http://lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/]</p> <p>Traditional Rationality is phrased as social rules, with violations interpretable as cheating: if you break the rules and no one else is doing so, you're the first to defect - making you a bad, bad person.  To Bayesians, the brain is an engine of accuracy: <a href="0123.html">if you violate the laws of rationality, the engine doesn't run</a> [http://lesswrong.com/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/], and this is equally true whether anyone else breaks the rules or not.</p> <p>Consider the problem of <a href="0111.html">Occam's Razor</a> [http://lesswrong.com/lw/jp/occams_razor/], as confronted by Traditional philosophers.  If two hypotheses fit the same observations <a href="0111.html">equally well</a> [http://lesswrong.com/lw/jp/occams_razor/], why believe the simpler one is more likely to be true?</p><a id="more"></a><p>You could argue that Occam's Razor has worked in the past, and is therefore likely to continue to work in the future.  But this, itself, appeals to a prediction from Occam's Razor.  "Occam's Razor works up to October 8th, 2007 and then stops working thereafter" is more complex, but it fits the observed evidence equally well.</p> <p>You could argue that Occam's Razor is a reasonable distribution on prior probabilities.  But what is a "reasonable" distribution?  Why not label "reasonable" a very complicated prior distribution, which makes Occam's Razor work in all observed tests so far, but generates exceptions in future cases?</p> <p>Indeed, it seems there is no way to <em>justify</em> Occam's Razor except by <em>appealing</em> to Occam's Razor, making this <em>argument</em> unlikely to <em>convince</em> any <em>judge</em> who does not already <em>accept</em> Occam's Razor.  (What's special about the words I italicized?)</p> <p>If you are a philosopher whose daily work is to write papers, criticize other people's papers, and respond to others' criticisms of your own papers, then you may look at Occam's Razor and shrug.  Here is an end to justifying, arguing and convincing.  You decide to call a <a href="0079.html">truce</a> [http://lesswrong.com/lw/it/semantic_stopsigns/] on writing papers; if your fellow philosophers do not demand justification for your un-arguable beliefs, you will not demand justification for theirs.  And as the symbol of your treaty, your white flag, you use the phrase "<a href="0113.html">a priori truth</a> [http://lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/]".</p> <p>But to a Bayesian, in this era of cognitive science and evolutionary biology and Artificial Intelligence, saying "a priori" doesn't explain why the brain-engine runs.  If the brain has an amazing "a priori truth factory" that <em>works</em> to produce accurate beliefs, it makes you wonder why a thirsty hunter-gatherer can't use the "a priori truth factory" to locate drinkable water.  It makes you wonder why eyes evolved in the first place, if there are ways to produce accurate beliefs without <a href="0107.html">looking at things</a> [http://lesswrong.com/lw/jl/what_is_evidence/].</p> <p>James R. Newman said:  "The fact that one apple added to one apple invariably gives two apples helps in the teaching of arithmetic, but has no bearing on the truth of the proposition that 1 + 1 = 2."  The Internet Encyclopedia of Philosophy <a href="http://www.iep.utm.edu/a/apriori.htm">defines</a> [http://www.iep.utm.edu/a/apriori.htm] "a priori" propositions as those knowable independently of experience.  Wikipedia <a href="http://en.wikipedia.org/wiki/A_priori_and_a_posteriori_%28philosophy%29">quotes</a> [http://en.wikipedia.org/wiki/A_priori_and_a_posteriori_%28philosophy%29] Hume:  Relations of ideas are "discoverable by the mere operation of thought, without dependence on what is anywhere existent in the universe."  You can see that 1 + 1 = 2 <em>just by thinking about it,</em> without looking at apples.</p> <p>But in this era of neurology, one ought to be aware that <em>thoughts</em> are existent in the universe; they are identical to the operation of brains.  Material brains, real in the universe, composed of quarks in a single unified mathematical physics whose laws draw no border between the inside and outside of your skull.</p> <p>When you add 1 + 1 and get 2 by thinking, these thoughts are themselves embodied in flashes of neural patterns.  In principle, we could <em>observe</em>, experientially, the exact same material events as they occurred within someone else's brain.  It would require some advances in computational neurobiology and brain-computer interfacing, but in principle, it could be done.  You could see someone else's engine operating materially, through material chains of cause and effect, to compute by "pure thought" that 1 + 1 = 2.  How is observing this pattern in <em>someone else's</em> brain any different, as a way of knowing, from observing your own brain doing the same thing?  When "pure thought" tells you that 1 + 1 = 2, "independently of any experience or observation", you are, in effect, observing your own brain as evidence.</p> <p>If this seems counterintuitive, try to see minds/brains as engines - an engine that collides the neural pattern for 1 and the neural pattern for 1 and gets the neural pattern for 2.  If this engine works at all, then it should <em>have the same output</em> if it observes (with eyes and retina) a similar brain-engine carrying out a similar collision, and copies into itself the resulting pattern.  In other words, for every form of a priori knowledge obtained by "pure thought", you are learning exactly the same thing you would learn if you saw an outside brain-engine carrying out the same pure flashes of neural activation.  The engines are equivalent, the <a href="0114.html">bottom-line outputs</a> [http://lesswrong.com/lw/js/the_bottom_line/] are equivalent, the <a href="0107.html">belief-entanglements</a> [http://lesswrong.com/lw/jl/what_is_evidence/] are the same.</p> <p>There is nothing you can know "a priori", which you could not know with equal validity by observing the chemical release of neurotransmitters within some outside brain.  What do you think you <em>are,</em> dear reader?</p> <p>This is <em>why</em> you can predict the result of adding 1 apple and 1 apple by imagining it first in your mind, or punch "3 x 4" into a calculator to predict the result of imagining 4 rows with 3 apples per row.  You and the apple exist within a <a href="0041.html">boundary-less unified physical process</a> [http://lesswrong.com/lw/hr/universal_law/], and one part may echo another.</p> <p>Are the sort of neural flashes that philosophers label "a priori beliefs", <em>arbitrary</em>?  Many AI algorithms function better with "regularization" that biases the solution space toward simpler solutions.  But the regularized algorithms are themselves more complex; they contain an extra line of code (or 1000 extra lines) compared to unregularized algorithms.  The human brain is biased toward simplicity, and we think more efficiently thereby.  If you <a href="0088.html">press the Ignore button</a> [http://lesswrong.com/lw/j2/explainworshipignore/] at this point, you're left with a complex brain that exists for no reason and works for no reason.  So don't try to tell me that "a priori" beliefs are arbitrary, because they sure aren't generated by rolling random numbers.  (What does the adjective "arbitrary" <em>mean,</em> anyway?)</p> <p>You can't <a href="0123.html">excuse</a> [http://lesswrong.com/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/] calling a proposition "a priori" by pointing out that <em>other</em> philosophers are having trouble justifying <em>their</em> propositions.  If a philosopher fails to explain something, this fact cannot supply electricity to a refrigerator, nor act as a magical factory for accurate beliefs.  There's no truce, no white flag, until you understand why the engine works.</p> <p>If you clear your mind of <em>justification,</em> of <em>argument,</em> then it seems obvious why Occam's Razor works in practice: we live in a simple world, a low-entropy universe in which there are short explanations to be found.  "But," you cry, "why is the universe itself orderly?"  This I do not know, but it is what I see as the next mystery to be <a href="0088.html">explained</a> [http://lesswrong.com/lw/j2/explainworshipignore/].  This is not the same question as "How do I argue Occam's Razor to a hypothetical debater who has not already accepted it?"</p> <p>Perhaps you cannot argue <em>anything</em> to a hypothetical debater who has not accepted Occam's Razor, just as you cannot argue anything to a rock.  A mind needs a certain amount of dynamic structure to be an argument-acceptor.  If a mind doesn't implement Modus Ponens, it can accept "A" and "A-&gt;B" all day long without ever producing "B".  How do you justify Modus Ponens to a mind that hasn't accepted it?  How do you argue a rock into becoming a mind?</p> <p>Brains evolved from non-brainy matter by natural selection; they were not justified into existence by arguing with an ideal philosophy student of perfect emptiness.  This does not make our judgments meaningless.  A brain-engine can work correctly, producing accurate beliefs, even if it was merely <em>built</em> - by human hands or cumulative stochastic selection pressures - rather than argued into existence.  But to be satisfied by this answer, one must see rationality in terms of engines, rather than arguments.</p></div> <hr><p><i>Referenced by: </i><a href="0228.html">The Allais Paradox</a> &#8226; <a href="0239.html">The "Intuitions" Behind "Utilitarianism"</a> &#8226; <a href="0242.html">Newcomb's Problem and Regret of Rationality</a> &#8226; <a href="0284.html">Mind Projection Fallacy</a> &#8226; <a href="0397.html">No Universally Compelling Arguments</a> &#8226; <a href="0406.html">Moral Complexities</a> &#8226; <a href="0410.html">Where Recursive Justification Hits Bottom</a> &#8226; <a href="0416.html">Probability is Subjectively Objective</a> &#8226; <a href="0446.html">Abstracted Idealized Dynamics</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/k2/a_priori/">A Priori</a></p></body></html>