<html><head><title>Reversed Stupidity Is Not Intelligence</title></head><body><h1>Reversed Stupidity Is Not Intelligence</h1><p><i>Eliezer Yudkowsky, 12 December 2007 10:14PM</i></p><div><blockquote> <p>  &#160;&#160; &#160;&#160; "...then our people on that time-line went to work with corrective action.  Here."<br>  &#160;&#160; &#160;&#160; He wiped the screen and then began punching combinations.  Page after page appeared, bearing accounts of people who had claimed to have seen the mysterious disks, and each report was more fantastic than the last.<br>  &#160;&#160; &#160;&#160; "The standard smother-out technique," Verkan Vall grinned.  "I only heard a little talk about the 'flying saucers,' and all of that was in joke.  In that order of culture, you can always discredit one true story by setting up ten others, palpably false, parallel to it."<br>  &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160;   &#8212;H. Beam Piper, <em>Police Operation</em></p> </blockquote> <p>Piper had a point.  Pers'nally, I don't believe there are any poorly hidden aliens infesting these parts.  But my disbelief has nothing to do with the awful embarrassing irrationality of flying saucer cults&#8212;at least, I hope not.</p> <p>You and I believe that flying saucer cults arose in the total absence of any flying saucers.  <a href="0189.html">Cults can arise around almost any idea</a> [http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/], thanks to human silliness.  This silliness operates <em>orthogonally</em> to alien intervention:  We would expect to see flying saucer cults whether or not there were flying saucers.  Even if there were poorly hidden aliens, it would not be any <em>less</em> likely for flying saucer cults to arise.  p(cults|aliens) isn't less than p(cults|~aliens), unless you suppose that poorly hidden aliens would deliberately suppress flying saucer cults.  By the <a href="http://yudkowsky.net/bayes/bayes.html">Bayesian definition of evidence</a> [http://yudkowsky.net/bayes/bayes.html], the observation "flying saucer cults exist" is not evidence <em>against</em> the existence of flying saucers.  It's not much evidence one way or the other.</p> <p>This is an application of the general principle that, as Robert Pirsig puts it, "The world's greatest fool may say the Sun is shining, but that doesn't make it dark out."</p> <p><a id="more"></a></p> <p>If you knew someone who was wrong 99.99% of the time on yes-or-no questions, you could obtain 99.99% accuracy just by reversing their answers.  They would need to do all the work of obtaining good evidence entangled with reality, and processing that evidence coherently, just to <em>anticorrelate</em> that reliably.  They would have to be superintelligent to be that stupid.</p> <p>A car with a broken engine cannot drive backward at 200 mph, even if the engine is <em>really really broken.</em></p> <p>If stupidity does not reliably anticorrelate with truth, how much less should human evil anticorrelate with truth?  The converse of the <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/] is the horns effect:  All perceived negative qualities correlate.  If Stalin is evil, then everything he says should be false.  You wouldn't want to agree with <em>Stalin,</em> would you?</p> <p>Stalin also believed that 2 + 2 = 4.  Yet if you defend any statement made by Stalin, even "2 + 2 = 4", people will see only that you are "agreeing with Stalin"; you must be on his side.</p> <p>Corollaries of this principle:</p> <ul> <li>To argue against an idea honestly, you should argue against the best arguments of the strongest advocates.  Arguing against weaker advocates proves <em>nothing,</em> because even the strongest idea will attract weak advocates.  If you want to argue against transhumanism or the intelligence explosion, you have to directly challenge the arguments of Nick Bostrom or Eliezer Yudkowsky post-2003.  The <a href="http://lesswrong.com/lw/2k/the_least_convenient_possible_world/">least convenient path</a> [http://lesswrong.com/lw/2k/the_least_convenient_possible_world/] is the only valid one.</li> <li>Exhibiting sad, pathetic lunatics, driven to madness by their apprehension of an Idea, is no evidence against that Idea.  Many New Agers have been made crazier by their personal apprehension of <a href="0379.html">quantum mechanics</a> [http://lesswrong.com/lw/r5/the_quantum_physics_sequence/].</li> <li>Someone once said, "Not all conservatives are stupid, but most stupid people are conservatives."  If you cannot place yourself in a state of mind where this statement, true or false, seems <em>completely irrelevant</em> as a critique of conservatism, you are not ready to think rationally about politics.</li> <li><a href="http://plover.net/~bonds/adhominem.html">Ad hominem</a> [http://plover.net/~bonds/adhominem.html] argument is not valid.</li> <li>You need to be able to argue against genocide without saying "Hitler wanted to exterminate the Jews."  If Hitler <em>hadn't</em> advocated genocide, would it thereby become okay?</li> <li>In Hansonian terms:  Your instinctive willingness to believe something will change along with your willingness to <em>affiliate</em> with people who are known for believing it&#8212;quite apart from whether the belief is actually <em>true.</em>  Some people may be reluctant to believe that God does not exist, not because there is evidence that God <em>does </em>exist, but rather because they are reluctant to affiliate with Richard Dawkins or those darned "strident" atheists who go around publicly saying "God does not exist".</li> <li>If your current computer stops working, you can't conclude that everything about the current system is wrong and that you need a new system without an AMD processor, an ATI video card, a Maxtor hard drive, or case fans&#8212;even though your current system has all these things and it doesn't work.  Maybe you just need a new power cord.</li> <li><a href="0546.html">If a hundred inventors fail</a> [http://lesswrong.com/lw/vs/selling_nonapples/] to build flying machines using metal and wood and canvas, it doesn't imply that what you really need is a flying machine of bone and flesh.  If a thousand projects fail to build Artificial Intelligence using electricity-based computing, this doesn't mean that electricity is the source of the problem.  Until you understand the problem, <a href="0167.html">hopeful reversals are exceedingly unlikely to hit the solution</a> [http://lesswrong.com/lw/l9/artificial_addition/].</li> </ul> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer"><em>Politics Is the Mind-Killer</em></a> [http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0191.html">Argument Screens Off Authority</a> [http://lesswrong.com/lw/lx/argument_screens_off_authority/]"</p> <p style="text-align:right">Previous post: "<a href="0187.html">The Robbers Cave Experiment</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq03.html">Sequence 03: Politics is the Mind-Killer</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0187.html">The Robbers Cave Experiment</a></p></td><td><p><i>Next: </i><a href="0191.html">Argument Screens Off Authority</a></p></td></tr></table><p><i>Referenced by: </i><a href="0187.html">The Robbers Cave Experiment</a> &#8226; <a href="0191.html">Argument Screens Off Authority</a> &#8226; <a href="0192.html">Hug the Query</a> &#8226; <a href="0193.html">Guardians of the Truth</a> &#8226; <a href="0207.html">Cultish Countercultishness</a> &#8226; <a href="0215.html">But There's Still A Chance, Right?</a> &#8226; <a href="0217.html">Absolute Authority</a> &#8226; <a href="0281.html">Dissolving the Question</a> &#8226; <a href="0300.html">Is Humanism A Religion-Substitute?</a> &#8226; <a href="0353.html">No Safe Defense, Not Even Science</a> &#8226; <a href="0377.html">Against Devil's Advocacy</a> &#8226; <a href="0393.html">Surface Analogies and Deep Causes</a> &#8226; <a href="0410.html">Where Recursive Justification Hits Bottom</a> &#8226; <a href="0413.html">The Genetic Fallacy</a> &#8226; <a href="0437.html">The Comedy of Behaviorism</a> &#8226; <a href="0481.html">My Best and Worst Mistake</a> &#8226; <a href="0549.html">Logical or Connectionist AI?</a> &#8226; <a href="0552.html">Failure By Affective Analogy</a> &#8226; <a href="0585.html">Prolegomena to a Theory of Fun</a> &#8226; <a href="0598.html">Amputation of Destiny</a> &#8226; <a href="0608.html">Serious Stories</a> &#8226; <a href="0614.html">Justified Expectation of Pleasant Surprises</a> &#8226; <a href="0682.html">Why Our Kind Can't Cooperate</a> &#8226; <a href="0793.html">Undiscriminating Skepticism</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/">Reversed Stupidity Is Not Intelligence</a></p></body></html>