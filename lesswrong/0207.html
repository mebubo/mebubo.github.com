<html><head><title>Cultish Countercultishness</title></head><body><h1>Cultish Countercultishness</h1><p><i>Eliezer Yudkowsky, 30 December 2007 12:53AM</i></p><div><p><strong>Followup to</strong>:  <a href="0189.html">Every Cause Wants To Be A Cult</a> [http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/], <a href="0205.html">Lonely Dissent</a> [http://lesswrong.com/lw/mb/lonely_dissent/]</p> <p>In the modern world, joining a cult is probably one of the worse things that can happen to you.  The best-case scenario is that you'll end up in a group of sincere but deluded people, making an honest mistake but otherwise well-behaved, and you'll spend a lot of time and money but end up with nothing to show.  Actually, that could describe any failed Silicon Valley startup.  Which is supposed to be a hell of a harrowing experience, come to think.  So yes, very scary.</p> <p>Real cults are vastly worse.  "Love bombing" as a recruitment technique, targeted at people going through a personal crisis.  Sleep deprivation.  Induced fatigue from hard labor.  Distant communes to isolate the recruit from friends and family.  Daily meetings to confess impure thoughts.  It's not unusual for cults to take <em>all</em> the recruit's money&#8212;life savings plus weekly paycheck&#8212;forcing them to depend on the cult for food and clothing.  Starvation as a punishment for disobedience.  Serious brainwashing and serious harm.</p> <p>With all that taken into account, I should probably sympathize more with people who are terribly nervous, embarking on some odd-seeming endeavor, that <em>they might be joining a cult.</em>  It should not grate on my nerves.  Which it does.</p> <p><a id="more"></a></p> <p>Point one:  "Cults" and "non-cults" aren't separated natural kinds like dogs and cats.  If you look at any <a href="http://www.prem-rawat-talk.org/forum/uploads/CultCharacteristics.htm">list of cult characteristics</a> [http://www.prem-rawat-talk.org/forum/uploads/CultCharacteristics.htm], you'll see items that could easily describe political parties and corporations&#8212;"group members encouraged to distrust outside criticism as having hidden motives", "hierarchical authoritative structure".  I've posted on group failure modes like <a href="0187.html">group polarization</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/], <a href="0180.html">happy death spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/], <a href="0182.html">uncriticality</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/], and <a href="0185.html">evaporative cooling</a> [http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/], all of which seem to feed on each other.  When these failures swirl together and meet, they combine to form a Super-Failure stupider than any of the parts, like <a href="http://www.youtube.com/watch?v=tZZv5Z2Iz_s">Voltron</a> [http://www.youtube.com/watch?v=tZZv5Z2Iz_s].  But this is not a cult <em>essence;</em> it is a cult <em>attractor.</em></p> <p>Dogs are born with dog DNA, and cats are born with cat DNA.  In the current world, there is no in-between.  (Even with genetic manipulation, it wouldn't be as simple as creating an organism with half dog genes and half cat genes.)  It's not like there's a mutually reinforcing set of dog-characteristics, which an individual cat can wander halfway into and become a semidog.</p> <p>The human mind, as it thinks about categories, seems to prefer essences to attractors.  The one wishes to say "It is a cult" or "It is not a cult", and then the task of classification is over and done.  If you observe that Socrates has ten fingers, wears clothes, and speaks fluent Greek, then you can say "Socrates is human" and from there deduce "Socrates is vulnerable to hemlock" without doing specific blood tests to confirm his mortality.  You have decided Socrates's humanness once and for all.</p> <p>But if you observe that a certain group of people seems to exhibit <a href="0187.html">ingroup-outgroup polarization</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/] and see a positive <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/] around their Favorite Thing Ever&#8212;which could be <a href="0195.html">Objectivism</a> [http://lesswrong.com/lw/m1/guardians_of_ayn_rand/], or vegetarianism, or <a href="http://thedailywtf.com/Articles/No,_We_Need_a_Neural_Network.aspx">neural networks</a> [http://thedailywtf.com/Articles/No,_We_Need_a_Neural_Network.aspx]&#8212;you cannot, <em>from the evidence gathered so far,</em> deduce whether they have achieved <a href="0182.html">uncriticality</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/].  You cannot deduce whether their main idea is true, or false, or genuinely useful but not quite as useful as they think.  <em>From the information gathered so far,</em> you cannot deduce whether they are otherwise polite, or if they will lure you into isolation and deprive you of sleep and food.  The characteristics of cultness are not all present or all absent.</p> <p>If you look at online arguments over "X is a cult", "X is not a cult", then one side goes through an online list of cult characteristics and finds one that applies and says "Therefore is a cult!"  And the defender finds a characteristic that does not apply and says "Therefore it is not a cult!"</p> <p>You cannot build up an accurate picture of a group's reasoning dynamic using this kind of essentialism.  You've got to pay attention to individual characteristics individually.</p> <p>Furthermore, <a href="0190.html">reversed stupidity is not intelligence</a> [http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/].  If you're interested in the central <em>idea,</em> not just the implementation group, then smart ideas can have stupid followers.  Lots of New Agers talk about "quantum physics" but this is no strike against quantum physics.  Of course stupid ideas can also have stupid followers.  Along with binary essentialism goes the idea that if you infer that a group is a "cult", therefore their beliefs must be false, because false beliefs are characteristic of cults, just like cats have fur.  If you're interested in the idea, then <a href="0192.html">look at the idea, not the people</a> [http://lesswrong.com/lw/ly/hug_the_query/].  Cultishness is a characteristic of <em>groups</em> more than <em>hypotheses.</em></p> <p>The second error is that when people nervously ask, "This isn't a cult, is it?" it sounds to me like they're seeking <em>reassurance of rationality.</em>  The notion of a rationalist not getting too attached to their self-image as a rationalist deserves its own post (though see <a href="http://yudkowsky.net/virtues/">this</a> [http://yudkowsky.net/virtues/], <a href="0002.html">this</a> [http://lesswrong.com/lw/go/why_truth_and/] and <a href="0198.html">this</a> [http://lesswrong.com/lw/m4/two_cult_koans/]).  But even without going into detail, surely one can see that <em>nervously seeking reassurance</em> is not the best frame of mind in which to evaluate questions of rationality.  You will not be <a href="0121.html">genuinely curious</a> [http://lesswrong.com/lw/jz/the_meditation_on_curiosity/] or think of ways to <a href="0061.html">fulfill your doubts</a> [http://lesswrong.com/lw/ib/the_proper_use_of_doubt/].  Instead, you'll find some online source which says that cults use sleep deprivation to control people, you'll notice that Your-Favorite-Group doesn't use sleep deprivation, and you'll conclude "It's not a cult.  Whew!"  If it doesn't have fur, it must not be a cat.  Very reassuring.</p> <p>But <a href="0189.html">Every Cause Wants To Be A Cult</a> [http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/], whether the cause itself is wise or foolish.  The <a href="0187.html">ingroup-outgroup dichotomy</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/] etc. are part of human nature, not a <a href="0049.html">special curse</a> [http://lesswrong.com/lw/hz/correspondence_bias/] of <a href="0050.html">mutants</a> [http://lesswrong.com/lw/i0/are_your_enemies_innately_evil/].  Rationality is the exception, not the rule.  You have to put forth a constant effort to maintain rationality against the natural slide into entropy.  If you decide "It's not a cult!" and sigh with relief, then you will not put forth a continuing effort to push back <em>ordinary</em> tendencies toward cultishness.  You'll decide the cult-essence is absent, and stop pumping against the entropy of the cult-attractor.</p> <p>If you are terribly nervous about cultishness, then you will want to deny any hint of any characteristic that resembles a cult.  But <em>any</em> group with a goal seen in a positive light, is at risk for the <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/], and will have to pump against entropy to avoid an <a href="0180.html">affective death spiral</a> [http://lesswrong.com/lw/lm/affective_death_spirals/].  This is true even for ordinary institutions like political parties&#8212;people who think that "liberal values" or "conservative values" can cure cancer, etc.  It is true for Silicon Valley startups, both failed and successful.  It is true of Mac users and of Linux users.  The <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/] doesn't become okay just because everyone does it; if everyone walks off a cliff, you wouldn't too.  The error in reasoning is to be fought, not tolerated.  But if you're too nervous about "Are you <em>sure</em> this isn't a cult?" then you will be reluctant to see <em>any</em> sign of cultishness, because that would imply you're in a cult, and <em>It's not a cult!!</em>  So you won't see the current battlefields where the <em>ordinary</em> tendencies toward cultishness are creeping forward, or being pushed back.</p> <p>The third mistake in nervously asking "This isn't a cult, is it?" is that, I strongly suspect, the <em>nervousness</em> is there for entirely the wrong reasons.</p> <p>Why is it that groups which praise their Happy Thing to the stars, encourage members to donate all their money and work in voluntary servitude, and run private compounds in which members are kept tightly secluded, are called "religions" rather than "cults" once they've been around for a few hundred years?</p> <p>Why is it that most of the people who nervously ask of cryonics, "This isn't a cult, is it?" would not be equally nervous about attending a Republican or Democrat political rally?  <a href="0187.html">Ingroup-outgroup dichotomies</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/] and <a href="0180.html">happy death spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/] can happen in political discussion, in mainstream religions, in sports fandom.  If the <em>nervousness</em> came from fear of <em>rationality errors,</em> people would ask "This isn't an <a href="0187.html">ingroup-outgroup dichotomy</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/], is it?" about Democrat or Republican political rallies, in just the same fearful tones.<em> </em></p> <p>There's a legitimate reason to be less fearful of Libertarianism than of a flying-saucer cult, because Libertarians don't have a reputation for employing sleep deprivation to convert people.  But cryonicists don't have a reputation for using sleep deprivation, either.  So why be any more worried about <a href="http://www.alcor.org/">having your head frozen after you stop breathing</a> [http://www.alcor.org/]?</p> <p>I suspect that the <em>nervousness</em> is not the fear of believing falsely, or the fear of physical harm.  It is the fear of <a href="0205.html">lonely dissent</a> [http://lesswrong.com/lw/mb/lonely_dissent/].  The nervous feeling that subjects get in <a href="0203.html">Asch's conformity experiment</a> [http://lesswrong.com/lw/m9/aschs_conformity_experiment/], when all the other subjects (actually confederates) say one after another that line C is the same size as line X, and it looks to the subject like line B is the same size as line X.  The fear of leaving the pack.</p> <p>That's why groups whose beliefs have been around long enough to seem "normal" don't inspire the same nervousness as "cults", though some mainstream religions may also take all your money and send you to a monastery.  It's why groups like political parties, that are strongly liable for rationality errors, don't inspire the same nervousness as "cults".  The word "cult" isn't being used to symbolize rationality errors, it's being used as a label for something that <em>seems weird.</em></p> <p>Not every change is an improvement, but every improvement is necessarily a change.  That which you want to do better, you have no choice but to do differently.  Common wisdom does embody a fair amount of, well, actual wisdom; yes, it makes sense to require an extra burden of proof for weirdness.  But the <em>nervousness </em>isn't that kind of deliberate, rational consideration.  It's the fear of believing something that will make your friends look at you really oddly.  And so people ask "This isn't a <em>cult,</em> is it?" in a tone that they would never use for attending a political rally, or for putting up a gigantic Christmas display.</p> <p><em>That's</em> the part that bugs me.</p> <p>It's as if, as soon as you believe anything that your ancestors did not believe, the Cult Fairy comes down from the sky and infuses you with the Essence of Cultness, and the next thing you know, you're all <a href="0198.html">wearing robes</a> [http://lesswrong.com/lw/m4/two_cult_koans/] and <a href="0196.html">chanting</a> [http://lesswrong.com/lw/m2/the_litany_against_gurus/].  As if "weird" beliefs are the <em>direct cause</em> of the problems, never mind the sleep deprivation and beatings.  The harm done by cults&#8212;the Heaven's Gate suicide and so on&#8212;just goes to show that everyone with an odd belief is crazy; the first and foremost characteristic of "cult members" is that they are Outsiders with Peculiar Ways.</p> <p>Yes, socially unusual belief puts a group at risk for <a href="0187.html">ingroup-outgroup thinking</a> [http://lesswrong.com/lw/lt/the_robbers_cave_experiment/] and <a href="0185.html">evaporative cooling</a> [http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/] and other problems.  But the unusualness is a risk factor, not a disease in itself.  Same thing with having a goal that you think is worth accomplishing.  Whether or not the belief is true, having a nice goal always puts you at risk of the <a href="0180.html">happy death spiral</a> [http://lesswrong.com/lw/lm/affective_death_spirals/].  But that makes lofty goals a risk factor, not a disease.  Some goals are <a href="0181.html">genuinely worth pursuing</a> [http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/].</p> <p>On the other hand, I see no legitimate reason for sleep deprivation or threatening dissenters with beating, <a href="0182.html">full stop</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/].  When a group does this, then whether you call it "cult" or "not-cult", you have <a href="0192.html">directly answered</a> [http://lesswrong.com/lw/ly/hug_the_query/] the pragmatic question of whether to join.</p> <p>Problem four:  The fear of lonely dissent is something that <em>cults themselves</em> exploit.  Being afraid of your friends looking at you disapprovingly is <em>exactly the effect that real cults use to convert and keep members&#8212;</em>surrounding converts with wall-to-wall agreement among cult believers.</p> <p>The fear of strange ideas, the impulse to <a href="0203.html">conformity</a> [http://lesswrong.com/lw/m9/aschs_conformity_experiment/], has no doubt warned many potential victims away from flying-saucer cults.  When you're out, it keeps you out.  But when you're <em>in,</em> it keeps you <em>in.</em>  Conformity just glues you to wherever you are, whether that's a good place or a bad place.</p> <p>The one wishes there was some way they could be <em>sure </em>that they weren't in a "cult".  Some definite, crushing rejoinder to people who looked at them funny.  Some way they could know once and for all that they were doing the right thing, without these constant doubts.  I believe that's called "need for closure".  And&#8212;of course&#8212;cults exploit that, too.</p> <p>Hence the phrase, "Cultish countercultishness."</p> <p>Living with doubt is not a virtue&#8212;the <a href="0061.html">purpose of every doubt is to annihilate itself</a> [http://lesswrong.com/lw/ib/the_proper_use_of_doubt/] in success or failure, and a doubt that just hangs around, accomplishes nothing.  But sometimes a doubt does take a while to annihilate itself.  Living with a stack of currently unresolved doubts is an unavoidable fact of life for rationalists.  Doubt shouldn't be scary.  Otherwise you're going to have to choose between living one heck of a hunted life, or one heck of a stupid one.</p> <p>If you really, genuinely can't figure out whether a group is a "cult", then you'll just have to choose under conditions of uncertainty.  That's what decision theory is all about.</p> <p>Problem five:  Lack of strategic thinking.</p> <p>I know people who are cautious around <a href="http://intelligence.org/AIRisk.pdf">Singularitarianism</a> [http://intelligence.org/AIRisk.pdf], and they're <em>also</em> cautious around political parties and mainstream religions.  <em>Cautious,</em> not nervous or defensive.  These people can see at a glance that Singularitarianism is obviously not a full-blown cult with sleep deprivation etc.  But they worry that Singularitarianism will <em>become</em> a cult, because of risk factors like turning the concept of a powerful AI into a <a href="0182.html">Super Happy Agent</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/] (an agent defined primarily by agreeing with any nice thing said about it).  Just because something isn't a cult now, doesn't mean it won't become a cult in the future.  Cultishness is an attractor, not an essence.</p> <p>Does <em>this</em> kind of caution annoy me?  Hell no.  I spend a lot of time worrying about that scenario myself.  I try to place my Go stones in advance to block movement in that direction.  Hence, for example, the series of posts on cultish failures of reasoning.</p> <p>People who talk about "rationality" also have an added risk factor.  Giving people advice about how to think is an inherently dangerous business.  But it is a <em>risk factor,</em> not a <em>disease.</em></p> <p>Both of my favorite Causes are at-risk for cultishness.  Yet somehow, I get asked "Are you sure this isn't a cult?" a lot more often when I talk about powerful AIs, than when I talk about probability theory and cognitive science.  I don't know if one risk factor is higher than the other, but I know which one <em>sounds weirder</em>...</p> <p>Problem #6 with asking "This isn't a cult, is it?"...</p> <p>Just the question itself places me in a very annoying sort of Catch-22.  An actual Evil Guru would surely use the one's nervousness against them, and design a plausible elaborate argument explaining Why This Is Not A Cult, and the one would be eager to accept it.  Sometimes I get the impression that this is what people <em>want</em> me to do!  Whenever I try to write about cultishness and how to avoid it, I keep feeling like I'm giving in to that flawed desire&#8212;that I am, in the end, providing people with <em>reassurance.</em>  Even when I tell people that a constant fight against entropy is required.</p> <p>It feels like I'm making myself a first dissenter in Asch's conformity experiment, telling people, "Yes, line X really is the same as line B, it's okay for you to say so too."  They shouldn't need to ask!  Or, even worse, it feels like I'm presenting an elaborate argument for Why This Is Not A Cult.  It's a <em>wrong question.</em></p> <p>Just look at the group's reasoning processes for yourself, and decide for yourself whether it's something you want to be part of, once you get rid of the fear of weirdness.  It is your own responsibility to stop yourself from thinking cultishly, no matter which group you currently happen to be operating in.</p> <p>Once someone asks "This isn't a cult, is it?" then no matter how I answer, I always feel like I'm defending something.  I do not like this feeling.  It is not the function of a <a href="0198.html">Bayesian Master</a> [http://lesswrong.com/lw/m4/two_cult_koans/] to give reassurance, nor of rationalists to defend.</p> <p>Cults feed on groupthink, nervousness, desire for reassurance.  You cannot make nervousness go away by wishing, and false self-confidence is even worse.  But so long as someone needs reassurance&#8212;even reassurance about being a rationalist&#8212;that will always be a flaw in their armor.  A skillful swordsman focuses on the <a href="0192.html">target</a> [http://lesswrong.com/lw/ly/hug_the_query/], rather than glancing away to see if anyone might be laughing.  When you know what you're trying to do and why, you'll know whether you're getting it done or not, and whether a group is helping you or hindering you.</p> <p>(PS:  If the one comes to you and says, "Are you <em>sure</em> this isn't a cult?", don't try to explain all these concepts in one breath.  You're <a href="0138.html">underestimating inferential distances</a> [http://lesswrong.com/lw/kg/expecting_short_inferential_distances/].  The one will say, "Aha, so you're <em>admitting</em> you're a cult!" or "Wait, you're saying I shouldn't worry about joining cults?" or "So... the fear of cults is cultish?  That sounds awfully cultish to me."  So the last annoyance factor&#8212;#7 if you're keeping count&#8212;is that all of this is such a long story to explain.)</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor"><em>Death Spirals and the Cult Attractor</em></a> [http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0093.html">Anchoring and Adjustment</a> [http://lesswrong.com/lw/j7/anchoring_and_adjustment/]" (start of next subsequence)</p> <p style="text-align:right">Previous post: "<a href="0205.html">Lonely Dissent</a> [http://lesswrong.com/lw/mb/lonely_dissent/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq04.html">Sequence 04: Death Spirals and the Cult Attractor</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0205.html">Lonely Dissent</a></p></td><td><p><i>Next: </i><a href="seq05.html">Sequence 05: Seeing with Fresh Eyes</a></p></td></tr></table><p><i>Referenced by: </i><a href="0093.html">Anchoring and Adjustment</a> &#8226; <a href="0205.html">Lonely Dissent</a> &#8226; <a href="0252.html">Disguised Queries</a> &#8226; <a href="0253.html">Neural Categories</a> &#8226; <a href="0302.html">To Spread Science, Keep It Secret</a> &#8226; <a href="0306.html">Heat vs. Motion</a> &#8226; <a href="0353.html">No Safe Defense, Not Even Science</a> &#8226; <a href="0451.html">When Anthropomorphism Became Stupid</a> &#8226; <a href="0509.html">Crisis of Faith</a> &#8226; <a href="0573.html">Is That Your True Rejection?</a> &#8226; <a href="0677.html">Schools Proliferating Without Evidence</a> &#8226; <a href="0682.html">Why Our Kind Can't Cooperate</a> &#8226; <a href="0684.html">You're Calling *Who* A Cult Leader?</a> &#8226; <a href="0704.html">Whining-Based Communities</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/md/cultish_countercultishness/">Cultish Countercultishness</a></p></body></html>