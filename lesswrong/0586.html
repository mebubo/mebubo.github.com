<html><head><title>High Challenge</title></head><body><h1>High Challenge</h1><p><i>Eliezer Yudkowsky, 19 December 2008 12:51AM</i></p><div><p><strong>Followup to</strong>:  <a href="0169.html">Not for the Sake of Happiness (Alone)</a> [http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/], <a href="0422.html">Existential Angst Factory</a> [http://lesswrong.com/lw/sc/existential_angst_factory/]</p> <p>There's a class of prophecy that runs:  "In the Future, machines will do all the work.  Everything will be automated.  Even labor of the sort we now consider 'intellectual', like engineering, will be done by machines.  We can sit back and own the capital.  You'll never have to lift a finger, ever again."</p> <p>But then won't people be bored?</p> <p>No; they can play computer games&#8212;not like <em>our</em> games, of course, but much more advanced and entertaining.</p> <p>Yet wait!  If you buy a modern computer game, you'll find that it contains some tasks that are&#8212;there's no kind word for this&#8212;<em>effortful.</em>  (I would even say "difficult", with the understanding that we're talking about something that takes 10 minutes, not 10 years.)</p> <p>So in the future, we'll have programs that <em>help </em>you play the game&#8212;taking over if you get stuck on the game, or just bored; or so that you can play games that would otherwise be too advanced for you.</p> <p>But isn't there some wasted effort, here?  Why have one programmer working to make the game harder, and another programmer to working to make the game easier?  Why not just make the game easier to <em>start with?</em>  Since you play the game to get gold and experience points, making the game easier will let you get more gold per unit time: the game will become more fun.</p> <p>So this is the ultimate end of the prophecy of technological progress&#8212;just staring at a screen that says "YOU WIN", forever.</p> <p>And maybe we'll build a robot that does <em>that,</em> too.</p> <p>Then what?</p> <p><a id="more"></a></p> <p>The world of machines that do <em>all</em> the work&#8212;well, I don't want to say it's "analogous to the Christian Heaven" because it isn't <a href="0477.html">supernatural</a> [http://lesswrong.com/lw/tv/excluding_the_supernatural/]; it's something that could in principle be realized.  Religious analogies are far too easily tossed around as accusations...  But, without implying any other similarities, I'll say that it seems analogous in the sense that eternal laziness "<a href="0585.html">sounds like good news</a> [http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/]" to your present self who still has to work.</p> <p>And as for playing games, as a substitute&#8212;what <em>is</em> a computer game except synthetic work?  Isn't there a wasted step here?  (And computer games in their present form, considered as work, have various aspects that reduce stress and increase engagement; but they also carry costs in the form of artificiality and isolation.)</p> <p>I sometimes think that futuristic ideals phrased in terms of "getting rid of work" would be better reformulated as "removing low-quality work to make way for high-quality work".</p> <p>There's a broad class of goals that aren't suitable as the long-term meaning of life, because you can actually achieve them, and then you're done.</p> <p>To look at it another way, if we're looking for a suitable long-run meaning of life, we should look for goals that are good to <em>pursue</em> and not just good to <em>satisfy.</em></p> <p>Or to phrase that somewhat less paradoxically:  We should look for valuations that are over 4D states, rather than 3D states.  Valuable ongoing processes, rather than "make the universe have property P and then you're done".</p> <p>Timothy Ferris is again worth quoting:  To find happiness, "the question you should be asking isn't 'What do I want?' or 'What are my goals?' but 'What would excite me?'"</p> <p>You might say that for a long-run meaning of life, we need games that are fun to <em>play</em> and not just to <em>win.</em></p> <p>Mind you&#8212;sometimes you <em>do</em> <a href="0242.html">want to win</a> [http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/].  There are legitimate goals where winning is everything.  If you're talking, say, about curing cancer, then the suffering experienced by even a single cancer patient outweighs any fun that you might have in solving their problems.  If you work at creating a cancer cure for twenty years through your own efforts, learning new knowledge and new skill, making friends and allies&#8212;and then some alien superintelligence offers you a cancer cure on a silver platter for thirty bucks&#8212;then you shut up and take it.</p> <p>But "curing cancer" is a problem of the 3D-predicate sort: you want the no-cancer predicate to go from False in the present to True in the future.  The importance of this destination far outweighs the journey; you don't want to <em>go</em> there, you just want to <em>be</em> there.  There are many <em>legitimate</em> goals of this sort, but they are not suitable as long-run fun.  "Cure cancer!" is a worthwhile activity for us to pursue here and now, but it is not a plausible future goal of galactic civilizations.</p> <p>Why should this "valuable ongoing process" be a process of <em>trying to do things</em>&#8212;why not a process of passive experiencing, like the Buddhist Heaven?</p> <p>I confess I'm not entirely sure how to set up a "passively experiencing" mind.  The human brain was <em>designed</em> to perform various sorts of internal work that add up to an active intelligence; even if you lie down on your bed and exert no particular effort to think, the thoughts that go on through your mind are activities of brain areas that are designed to, you know, <em>solve problems.</em></p> <p>How much of the human brain could you eliminate, <em>apart </em>from the pleasure centers, and still keep the subjective experience of pleasure?</p> <p>I'm not going to touch that one.  I'll stick with the much simpler answer of "I wouldn't actually <em>prefer </em>to be a passive experiencer."  If I <em>wanted</em> Nirvana, I might try to figure out how to achieve that impossibility.  But once you strip away Buddha telling me that Nirvana is the end-all of existence, Nirvana seems rather more like "sounds like good news in the moment of first being told" or "ideological belief in desire" rather than, y'know, something I'd actually <em>want.</em><em><br></em></p> <p>The reason I have a mind at all, is that natural selection built me to <em>do</em> things&#8212;to solve certain kinds of problems.</p> <p>"Because it's human nature" is not an <a href="0410.html">explicit</a> [http://lesswrong.com/lw/s0/where_recursive_justification_hits_bottom/] justification for anything.  There is human nature, which is what we are; and there is humane nature, which is what, being human, we wish we were.</p> <p>But I don't <em>want</em> to change my nature toward a more passive object&#8212;which <em>is </em>a justification.  A happy blob is <em>not </em>what, being human, I wish to become.</p> <p><a href="0169.html">I earlier argued that many values require both subjective happiness and the external objects of that happiness</a> [http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/].  That you can legitimately have a utility function that says, "It matters to me whether or not the person I love is a real human being or just a highly realistic nonsentient chatbot, <em>even if I don't know,</em> because that-which-I-value is not my own state of mind, but the external reality."  So that you need both the experience of love, and the real lover.</p> <p>You can similarly have valuable activities that require both real challenge and real effort.</p> <p>Racing along a track, it matters that the other racers are real, and that you have a real chance to win or lose.  (We're not talking about <a href="0374.html">physical determinism</a> [http://lesswrong.com/lw/r0/thou_art_physics/] here, but whether some external optimization process explicitly chose for you to win the race.)</p> <p>And it matters that you're racing with your own skill at running and your own willpower, not just pressing a button that says "Win".  (Though, since you never designed your own leg muscles, you <em>are</em> racing using strength that isn't yours.  A race between robot cars is a purer contest of their designers.  There is plenty of room to improve on the human condition.)</p> <p>And it matters that you, a sentient being, are experiencing it.  (Rather than some nonsentient process carrying out a skeleton imitation of the race, trillions of times per second.)</p> <p>There must be the true effort, the true victory, and the true experience&#8212;the journey, the destination and the traveler.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0587.html">Complex Novelty</a> [http://lesswrong.com/lw/wx/complex_novelty/]"</p> <p style="text-align:right">Previous post: "<a href="0585.html">Prolegomena to a Theory of Fun</a> [http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0585.html">Prolegomena to a Theory of Fun</a></p></td><td><p><i>Next: </i><a href="0587.html">Complex Novelty</a></p></td></tr></table><p><i>Referenced by: </i><a href="0585.html">Prolegomena to a Theory of Fun</a> &#8226; <a href="0587.html">Complex Novelty</a> &#8226; <a href="0592.html">Harmful Options</a> &#8226; <a href="0598.html">Amputation of Destiny</a> &#8226; <a href="0601.html">Free to Optimize</a> &#8226; <a href="0606.html">Emotional Involvement</a> &#8226; <a href="0608.html">Serious Stories</a> &#8226; <a href="0610.html">Continuous Improvement</a> &#8226; <a href="0614.html">Justified Expectation of Pleasant Surprises</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0626.html">31 Laws of Fun</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ww/high_challenge/">High Challenge</a></p></body></html>