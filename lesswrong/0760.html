<html><head><title>Outlawing Anthropics: An Updateless Dilemma</title></head><body><h1>Outlawing Anthropics: An Updateless Dilemma</h1><p><i>Eliezer Yudkowsky, 08 September 2009 06:31PM</i></p><div><p>Let us start with a (non-quantum) logical coinflip - say, look at the heretofore-unknown-to-us-personally 256th binary digit of pi, where the choice of binary digit is itself intended not to be random.</p> <p>If the result of this logical coinflip is 1 (aka "heads"), we'll create 18 of you in green rooms and 2 of you in red rooms, and if the result is "tails" (0), we'll create 2 of you in green rooms and 18 of you in red rooms.</p> <p>After going to sleep at the start of the experiment, you wake up in a green room.</p> <p>With what degree of credence do you believe - what is your posterior probability - that the logical coin came up "heads"?</p> <p>There are exactly two tenable answers that I can see, "50%" and "90%".</p> <p>Suppose you reply 90%.</p> <p>And suppose you also happen to be "altruistic" enough to care about what happens to all the copies of yourself.  (If your current system cares about yourself and your future, but doesn't care about very similar xerox-siblings, then you will tend to self-modify to have <em>future</em> copies of yourself care about each other, as this maximizes your <em>expectation </em>of pleasant experience over <em>future</em> selves.)</p> <p>Then I attempt to force a reflective inconsistency in your decision system, as follows:</p> <p>I inform you that, after I look at the unknown binary digit of pi, I will ask all the copies of you in green rooms whether to pay $1 to every version of you in a green room and steal $3 from every version of you in a red room.  If they all reply "Yes", I will do so.<a id="more"></a></p> <p>(It will be understood, of course, that $1 represents 1 utilon, with actual monetary amounts rescaled as necessary to make this happen.  Very little rescaling should be necessary.)</p> <p>(<a href="http://wiki.lesswrong.com/wiki/Timeless_decision_theory">Timeless decision agents</a> [http://wiki.lesswrong.com/wiki/Timeless_decision_theory] reply as if controlling all similar decision processes, including all copies of themselves.  Classical causal decision agents, to reply "Yes" as a group, will need to somehow work out that other copies of themselves reply "Yes", and then reply "Yes" themselves.  We can try to help out the causal decision agents on their coordination problem by supplying rules such as "If conflicting answers are delivered, everyone loses $50".  If causal decision agents can win on the problem "If everyone says 'Yes' you all get $10, if everyone says 'No' you all lose $5, if there are conflicting answers you all lose $50" then they can presumably handle this.  If not, then ultimately, I decline to be responsible for <a href="http://wiki.lesswrong.com/wiki/Timeless_decision_theory">the stupidity of causal decision agents</a> [http://wiki.lesswrong.com/wiki/Timeless_decision_theory].)</p> <p>Suppose that you wake up in a green room.  You reason, "With 90% probability, there are 18 of me in green rooms and 2 of me in red rooms; with 10% probability, there are 2 of me in green rooms and 18 of me in red rooms.  Since I'm altruistic enough to at least care about my xerox-siblings, I calculate the expected utility of replying 'Yes' as (90% * ((18 * +$1) + (2 * -$3))) + (10% * ((18 * -$3) + (2 * +$1))) = +$5.60."  You reply yes.</p> <p>However, before the experiment, you calculate the general utility of the conditional strategy "Reply 'Yes' to the question if you wake up in a green room" as (50% * ((18 * +$1) + (2 * -$3))) + (50% * ((18 * -$3) + (2 * +$1))) = -$20.  You want your future selves to reply 'No' under these conditions.</p> <p>This is a dynamic inconsistency - different answers at different times - which argues that decision systems which update on anthropic evidence will self-modify <a href="http://wiki.lesswrong.com/wiki/Updateless_decision_theory">not to update probabilities</a> [http://wiki.lesswrong.com/wiki/Updateless_decision_theory] on anthropic evidence.</p> <p>I originally thought, on first formulating this problem, that it had to do with <em>double-counting </em>the <em>utilons </em>gained by your variable numbers of green friends, and the <em>probability </em>of being one of your green friends.</p> <p>However, the problem also works if we care about paperclips.  No selfishness, no altruism, just paperclips.</p> <p>Let the dilemma be, "I will ask all people who wake up in green rooms if they are willing to take the bet 'Create 1 paperclip if the logical coinflip came up heads, destroy 3 paperclips if the logical coinflip came up tails'.  (Should they disagree on their answers, I will destroy 5 paperclips.)"  Then a paperclip maximizer, before the experiment, wants the paperclip maximizers who wake up in green rooms to refuse the bet.  But a conscious paperclip maximizer who updates on anthropic evidence, who wakes up in a green room, will want to take the bet, with expected utility ((90% * +1 paperclip) + (10% * -3 paperclips)) = +0.6 paperclips.</p> <p>This argues that, in general, decision systems - whether they start out selfish, or start out caring about paperclips - will not want their future versions to update on anthropic "evidence".</p> <p>Well, that's not too disturbing, is it?  I mean, the whole anthropic thing seemed very confused to begin with - full of notions about "consciousness" and "reality" and "identity" and "reference classes" and other poorly defined terms.  Just throw out anthropic reasoning, and you won't have to bother.</p> <p>When I explained this problem to Marcello, he said, "Well, <a href="0597.html">we don't want to build conscious AIs</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/], so of course we don't want them to use anthropic reasoning", which is a fascinating sort of reply.  And I responded, "But when you have a problem this confusing, and you find yourself wanting to build an AI that just doesn't use anthropic reasoning to begin with, maybe that implies that the correct resolution involves <em>us</em> not using anthropic reasoning either."</p> <p>So we can just throw out anthropic reasoning, and relax, and <a href="0757.html">conclude that we are Boltzmann brains</a> [http://lesswrong.com/lw/17d/forcing_anthropics_boltzmann_brains/].  QED.</p> <p>In general, I find the sort of argument given here - that a certain type of decision system is not reflectively consistent - to be pretty damned compelling.  But I also find the Boltzmann conclusion to be, ahem, more than ordinarily unpalatable.</p> <p>In personal conversation, Nick Bostrom suggested that a division-of-responsibility principle might cancel out the anthropic update - i.e., the paperclip maximizer would have to reason, "If the logical coin came up heads then I am 1/18th responsible for adding +1 paperclip, if the logical coin came up tails then I am 1/2 responsible for destroying 3 paperclips."  I confess that my initial reaction to this suggestion was "Ewwww", but I'm not exactly comfortable concluding I'm a Boltzmann brain, either.</p> <p>EDIT:  On further reflection, I also wouldn't want to build an AI that concluded it was a Boltzmann brain!  Is there a form of inference which rejects this conclusion without relying on any reasoning about subjectivity?</p> <p>EDIT2:  Psy-Kosh has <a href="http://lesswrong.com/lw/17c/outlawing_anthropics_an_updateless_dilemma/13e1">converted this into a non-anthropic problem</a> [http://lesswrong.com/lw/17c/outlawing_anthropics_an_updateless_dilemma/13e1]!</p></div> <hr><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/17c/outlawing_anthropics_an_updateless_dilemma/">Outlawing Anthropics: An Updateless Dilemma</a></p></body></html>