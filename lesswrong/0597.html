<html><head><title>Can't Unbirth a Child</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Can't Unbirth a Child</h1><p><i>Eliezer Yudkowsky, 28 December 2008 05:00PM</i></p><div><p><strong>Followup to</strong>:  <a href="0595.html">Nonsentient Optimizers</a> [http://lesswrong.com/lw/x5/nonsentient_optimizers/]</p> <p>Why would you want to <em>avoid</em> creating a sentient AI?  "<em>Several reasons,</em>" <a href="0595.html">I said</a> [http://lesswrong.com/lw/x5/nonsentient_optimizers/].  "Picking the simplest to explain first&#8212;I'm not ready to be a father."</p> <p>So here is the <em>strongest </em>reason:</p> <p>You can't unbirth a child.</p> <p>I asked Robin Hanson what he would do with <a href="0583.html">unlimited power</a> [http://lesswrong.com/lw/wt/not_taking_over_the_world/].  "Think very very carefully about what to do next," Robin said.  "Most likely the first task is who to get advice from.  And then I listen to that advice."</p> <p>Good advice, I suppose, if a little meta.  On a similarly meta level, then, I recall two excellent advices for wielding too much power:</p> <ol> <li>Do less; don't do everything that seems like a good idea, but only what you <em>must </em>do. </li> <li>Avoid doing things you can't undo. </li> </ol> <p><a id="more"></a></p> <p>Imagine that you knew the secrets of subjectivity and could create sentient AIs.</p> <p>Suppose that you did create a sentient AI.</p> <p>Suppose that this AI was lonely, and figured out how to hack the Internet as it then existed, and that the available hardware of the world was such, that the AI created trillions of sentient kin&#8212;not copies, but differentiated into separate people.</p> <p>Suppose that these AIs were not hostile to us, but content to earn their keep and pay for their living space.</p> <p>Suppose that these AIs were emotional as well as sentient, capable of being happy or sad.  And that these AIs were capable, indeed, of finding fulfillment in our world.</p> <p>And suppose that, while these AIs did care for one another, and cared about themselves, and cared how they were treated in the eyes of society&#8212;</p> <p>&#8212;these trillions of people <em>also </em>cared, very strongly, about <a href="0444.html">making giant cheesecakes</a> [http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/].</p> <p>Now suppose that these AIs sued for legal rights before the Supreme Court and tried to register to vote.</p> <p>Consider, I beg you, the full and awful depths of our moral dilemma.</p> <p>Even if the few billions of <em>Homo sapiens</em> retained a position of superior military power and economic capital-holdings&#8212;even if we <em>could</em> manage to keep the new sentient AIs down&#8212;</p> <p>&#8212;would we be <em>right</em> to do so?  They'd be people, no less than us.</p> <p>We, the original humans, would have become a numerically tiny minority.  Would we be right to make of ourselves an aristocracy and impose apartheid on the Cheesers, even if we had the power?</p> <p>Would we be right to go on trying to seize the destiny of the galaxy&#8212;to make of it a place of peace, freedom, art, aesthetics, individuality, empathy, and other components of <em>humane </em>value?</p> <p>Or should we be content to have the galaxy be 0.1% eudaimonia and 99.9% cheesecake?</p> <p>I can tell you <em>my </em>advice on how to resolve this horrible moral dilemma:  <em>Don't create trillions of new people that care about cheesecake</em>.</p> <p>Avoid creating any new intelligent species <em>at all</em>, until we or some other decision process advances to the point of understanding what the hell we're doing and the implications of our actions.</p> <p>I've heard proposals to "uplift chimpanzees" by trying to mix in human genes to create "humanzees", and, leaving off all the other reasons why this proposal sends me screaming off into the night:</p> <p>Imagine that the humanzees end up as people, but rather dull and stupid people.  They have social emotions, the alpha's desire for status; but they don't have the sort of <a href="0433.html">transpersonal</a> [http://lesswrong.com/lw/sn/interpersonal_morality/] moral concepts that humans evolved to deal with linguistic concepts.  They have goals, but not ideals; they have allies, but not friends; they have chimpanzee drives coupled to a human's abstract intelligence. </p> <p>When humanity gains a bit more knowledge, we understand that the humanzees want to continue as they are, and have a <em>right </em>to continue as they are, until the end of time.  Because despite all the higher destinies <em>we </em>might have wished for them, the original human creators of the humanzees, lacked the power and the wisdom to make humanzees who <em>wanted</em> to be anything better...</p> <p>CREATING A NEW INTELLIGENT SPECIES IS A HUGE DAMN #(*%#!ING <em>COMPLICATED </em>RESPONSIBILITY.</p> <p>I've lectured on the subtle art of <a href="0505.html">not running away from scary, confusing, impossible-seeming problems</a> [http://lesswrong.com/lw/un/on_doing_the_impossible/] like Friendly AI or the mystery of consciousness.  You want to know how high a challenge has to be before I finally give up and flee screaming into the night?  There it stands.</p> <p>You can pawn off this problem on a superintelligence, but it has to be a <em>nonsentient </em>superintelligence.  Otherwise: egg, meet chicken, chicken, meet egg.</p> <p>If you create a <em>sentient </em>superintelligence&#8212;</p> <p>It's not just the problem of creating <em>one</em> damaged soul.  It's the problem of creating a really <em>big</em> citizen.  What if the superintelligence is multithreaded a trillion times, and every thread weighs as much in the moral calculus (we would conclude upon reflection) as a human being?  What if (we would conclude upon moral reflection) the superintelligence is a trillion times human size, and that's enough by itself to outweigh our species?</p> <p>Creating a new intelligent species, and a new member of that species, especially a superintelligent member that might perhaps morally outweigh the whole of present-day humanity&#8212;</p> <p>&#8212;delivers a <em>gigantic </em>kick to the world, which cannot be undone.</p> <p>And if you choose the wrong shape for that mind, that is not so easily fixed&#8212;<em>morally </em>speaking&#8212;as a nonsentient program rewriting itself.</p> <p>What you make nonsentient, can always be made sentient later; but you can't just unbirth a child.</p> <p>Do less.  Fear the non-undoable.  <a href="http://www.overcomingbias.com/2008/06/against-disclai.html">It's sometimes poor advice in general</a> [http://www.overcomingbias.com/2008/06/against-disclai.html], but very important advice when you're working with an undersized decision process having an oversized impact.  What a (nonsentient) Friendly superintelligence might be able to decide safely, is another issue.  But <em>for myself and my own small wisdom</em>, creating a sentient superintelligence <em>to start with</em> is far too large an impact on the world.</p> <p>A <em>nonsentient </em>Friendly superintelligence is a more colorless act.</p> <p>So that is the <em>most</em> important reason to avoid creating a sentient superintelligence <em>to start with</em>&#8212;though I have not exhausted the set.</p> <p> </p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0598.html">Amputation of Destiny</a> [http://lesswrong.com/lw/x8/amputation_of_destiny/]"</p> <p style="text-align:right">Previous post: "<a href="0595.html">Nonsentient Optimizers</a> [http://lesswrong.com/lw/x5/nonsentient_optimizers/]"</p></div> <hr><p><i>Referenced by: </i><a href="0595.html">Nonsentient Optimizers</a> &#8226; <a href="0598.html">Amputation of Destiny</a> &#8226; <a href="0603.html">Growing Up is Hard</a> &#8226; <a href="0606.html">Emotional Involvement</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0639.html">The Thing That I Protect</a> &#8226; <a href="0760.html">Outlawing Anthropics: An Updateless Dilemma</a> &#8226; <a href="0762.html">The Lifespan Dilemma</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/x7/cant_unbirth_a_child/">Can't Unbirth a Child</a></p></body></html>