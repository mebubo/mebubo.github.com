<html><head><title>Dreams of AI Design</title></head><body><h1>Dreams of AI Design</h1><p><i>Eliezer Yudkowsky, 26 August 2008 11:28PM</i></p><div><p><strong>Followup to</strong>:  <a href="0439.html">Anthropomorphic Optimism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/], <a href="0460.html">Three Fallacies of Teleology</a> [http://lesswrong.com/lw/te/three_fallacies_of_teleology/]</p> <p>After spending a decade or two living inside a mind, you might think you knew a bit about how minds work, right?  That's what quite a few <a href="http://www.mail-archive.com/agi@v2.listbox.com/">AGI wannabes</a> [http://www.mail-archive.com/agi@v2.listbox.com/] (people who think they've got what it takes to program an Artificial General Intelligence) seem to have concluded.  This, unfortunately, is wrong.</p> <p>Artificial Intelligence is fundamentally about reducing the mental to the non-mental.</p> <p>You might want to contemplate that sentence for a while.  It's important.</p> <p>Living inside a human mind doesn't teach you the art of <a href="0289.html">reductionism</a> [http://lesswrong.com/lw/on/reductionism/], because nearly all of the work is carried out beneath your sight, by the opaque black boxes of the brain.  So far beneath your sight that there is no introspective sense that the black box is there - no internal sensory event marking that the work has been delegated.</p> <p>Did Aristotle realize that when he talked about the <em>telos,</em> the <a href="0460.html">final cause</a> [http://lesswrong.com/lw/te/three_fallacies_of_teleology/] of events, that he was delegating predictive labor to his brain's complicated planning mechanisms - asking, "What would this object do, if it could make plans?"  I rather doubt it.  Aristotle thought the brain was an organ for cooling the blood - which he did think was important:  Humans, thanks to their larger brains, were more calm and contemplative.</p> <p>So there's an AI design for you!  We just need to cool down the computer a lot, so it will be more calm and contemplative, and won't rush headlong into doing stupid things like modern computers.</p><a id="more"></a><p>That's an example of <a href="0291.html">fake reductionism</a> [http://lesswrong.com/lw/op/fake_reductionism/].  "Humans are more contemplative because their blood is cooler", I mean.  It doesn't resolve the black box of the word <em>contemplative</em>.  You can't predict what a <em>contemplative</em> thing does using a complicated model with internal moving parts composed of merely material, merely causal elements - positive and negative voltages on a transistor being the canonical example of a merely material and causal element of a model.  All you can do is <em>imagine yourself</em> being contemplative, to get an idea of what a <em>contemplative</em> agent does.</p> <p>Which is to say that you can <em>only</em> reason about "contemplative-ness" by <a href="0437.html">empathic inference</a> [http://lesswrong.com/lw/sr/the_comedy_of_behaviorism/] - using your own brain as a black box with the contemplativeness <a href="0435.html">lever</a> [http://lesswrong.com/lw/sp/detached_lever_fallacy/] pulled, to predict the output of another black box.</p> <p>You can imagine another agent being <em>contemplative,</em> but again that's an act of <a href="0437.html">empathic inference</a> [http://lesswrong.com/lw/sr/the_comedy_of_behaviorism/] - the way this imaginative act works is by adjusting your own brain to run in contemplativeness-mode, not by modeling the other brain neuron by neuron.  Yes, that may be more efficient, but it doesn't let you build a "contemplative" mind from scratch.</p> <p>You can say that "cold blood causes contemplativeness" and then you just have <a href="0078.html">fake causality</a> [http://lesswrong.com/lw/is/fake_causality/]:  You've drawn a little arrow from a box reading "cold blood" to a box reading "contemplativeness", but you haven't looked <em>inside</em> the box - you're still generating your predictions using empathy.</p> <p>You can say that "lots of little neurons, which are all strictly electrical and chemical with no ontologically basic contemplativeness in them, combine into a <a href="0083.html">complex</a> [http://lesswrong.com/lw/ix/say_not_complexity/] network that <a href="0081.html">emergently</a> [http://lesswrong.com/lw/iv/the_futility_of_emergence/] exhibits contemplativeness".  And that is <em>still</em> a fake reduction and you <em>still</em> haven't looked inside the black box.  You still can't say what a "contemplative" thing will do, using a <em>non-empathic</em> model.  You just took a box labeled "lotsa neurons", and drew an arrow labeled "emergence" to a black box containing your remembered sensation of contemplativeness, which, when you imagine it, tells your brain to empathize with the box by contemplating.</p> <p>So what do <em>real</em> reductions look like?</p> <p>Like the relationship between the <em>feeling</em> of evidence-ness, of justification-ness, and E. T. Jaynes's "Probability Theory: The Logic of Science".  You can go around in circles all day, saying how the nature of <em>evidence</em>, is that it <em>justifies</em> some <em>proposition,</em> by <em>meaning</em> that it's <em>more likely</em> to be <em>true,</em> but all of these just invoke your brain's internal feelings of evidence-ness, justifies-ness, likeliness.  That part is easy - the going around in circles part.  The part where you go from there to Bayes's Theorem is <em>hard.</em></p> <p>And the fundamental mental ability that lets someone <em>learn</em> Artificial Intelligence is the ability to tell the <em>difference.</em>  So that you know you <em>aren't done yet, nor even really started, </em>when you say, "Evidence is when an observation justifies a belief".    But atoms are not evidential, justifying, meaningful, likely, propositional, or true, they are just atoms.  Only things like "P(H|E)/P(~H|E) = P(E|H)/P(E|~H) * P(H)/P(~H)" count as substantial progress.  (And that's only the first step of the reduction: what are these E and H objects, if not mysterious black boxes?  Where do your hypotheses come from?  From your <em>creativity?</em>  And what's a hypothesis, when no atom is a hypothesis?)</p> <p>Another excellent example of genuine reduction can be found in Judea Pearl's <em>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</em>  You could go around all day in circles talk about how a <em>cause</em> is something that <em>makes</em> something else happen, and until you understood the nature of conditional independence, you would be helpless to make an AI that reasons about <a href="0365.html">causation</a> [http://lesswrong.com/lw/qr/timeless_causality/].  Because you wouldn't understand <em>what</em> was happening, when <em>your brain mysteriously decided</em>, that if you learned your burglar alarm went off, but you then learned that a small earthquake took place, you would retract your initial conclusion that your house had been burglarized.</p> <p>If you want an AI that plays chess, you can go around in circles indefinitely talking about how you want the AI to make <em>good</em> moves, which are moves that can be <em>expected to win the game,</em> which are moves that are <em>prudent strategies for defeating the opponent,</em> etcetera; and while <em>you</em> may then have some idea of which moves you <a href="0439.html">want</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/] the AI to make, it's all for naught until you come up with the notion of a mini-max search tree.</p> <p>But <em>until</em> you know about search trees, <em>until</em> you know about conditional independence, <em>until</em> you know about Bayes's Theorem, then it may still <em>seem</em> to you that you have a perfectly good understanding of where good moves and nonmonotonic reasoning and evaluation of evidence come from.  It may seem, for example, that they come from cooling the blood.</p> <p>And indeed I know many people who believe that <em>intelligence</em> is the product of <em>commonsense knowledge</em> or  <em>massive parallelism </em>or <em>creative destruction</em> or <em>intuitive rather than rational reasoning</em>, or whatever.  But all these are only dreams, which do not give you any way to say what intelligence is, or what an intelligence will do next, except by pointing at a human.  And when the one goes to build their wondrous AI, they only build a system of <a href="0435.html">detached levers</a> [http://lesswrong.com/lw/sp/detached_lever_fallacy/], "knowledge" consisting of LISP tokens labeled <tt>apple</tt> and the like; or perhaps they build a "massively parallel neural net, just like the human brain".  And are shocked - shocked! - when nothing much happens.</p> <p>AI designs made of human parts are only dreams; they can exist in the imagination, but not translate into transistors.  This applies specifically to "AI designs" that look like boxes with arrows between them and meaningful-sounding labels on the boxes.  (For a truly epic example thereof, see any <a href="http://mind.sourceforge.net/diagrams.html">Mentifex Diagram</a> [http://mind.sourceforge.net/diagrams.html].) </p> <p>Later I will say more upon this subject, but I can go ahead and tell you one of the guiding principles:  If you meet someone who says that their AI will do XYZ <em>just like humans</em>, do not give them any venture capital.  Say to them rather:  "I'm sorry, I've never seen a human brain, or any other intelligence, and I have no reason as yet to believe that any such thing can exist.  Now please explain to me <em>what</em> your AI does, and <em>why</em> you believe it will do it, without pointing to humans as an example."  Planes would fly just as well, given a fixed design, if birds had never existed; they are not kept aloft by <a href="0393.html">analogies</a> [http://lesswrong.com/lw/rj/surface_analogies_and_deep_causes/].</p> <p>So now you perceive, I hope, why, if you wanted to teach someone to do <em>fundamental</em> work on strong AI - bearing in mind that this is demonstrably a very <em>difficult</em> art, which is not learned by a supermajority of students who are just taught existing reductions such as search trees - then you might go on for some length about such matters as <a href="0289.html">the fine art of reductionism</a> [http://lesswrong.com/lw/on/reductionism/], about <a href="0260.html">playing rationalist's Taboo</a> [http://lesswrong.com/lw/nu/taboo_your_words/] to excise problematic <a href="0279.html">words</a> [http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/] and <a href="0261.html">replace them with their referents</a> [http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/], about <a href="0439.html">anthropomorphism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/], and, of course, about <a href="0144.html">early stopping</a> [http://lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/] on <a href="0080.html">mysterious answers to mysterious questions</a> [http://lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/].</p></div> <hr><p><i>Referenced by: </i><a href="0477.html">Excluding the Supernatural</a> &#8226; <a href="0491.html">My Naturalistic Awakening</a> &#8226; <a href="0494.html">Above-Average AI Scientists</a> &#8226; <a href="0519.html">Ethical Injunctions</a> &#8226; <a href="0530.html">Economic Definition of Intelligence?</a> &#8226; <a href="0551.html">Failure By Analogy</a> &#8226; <a href="0552.html">Failure By Affective Analogy</a> &#8226; <a href="0574.html">Artificial Mysterious Intelligence</a> &#8226; <a href="0594.html">Nonperson Predicates</a> &#8226; <a href="0595.html">Nonsentient Optimizers</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/tf/dreams_of_ai_design/">Dreams of AI Design</a></p></body></html>