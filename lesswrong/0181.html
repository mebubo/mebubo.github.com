<html><head><title>Resist the Happy Death Spiral</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Resist the Happy Death Spiral</h1><p><i>Eliezer Yudkowsky, 04 December 2007 01:15AM</i></p><div><p><strong>Followup to</strong>:  <a href="0180.html">Affective Death Spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/]</p> <p>Once upon a time, there was a man who was convinced that he possessed a Great Idea.  Indeed, as the man thought upon the Great Idea more and more, he realized that it was not just <em>a</em> great idea, but <em>the most wonderful idea ever.</em> The Great Idea would unravel the mysteries of the universe, supersede the authority of the corrupt and error-ridden Establishment, confer nigh-magical powers upon its wielders, feed the hungry, heal the sick, make the whole world a better place, etc. etc. etc.</p> <p>The man was Francis Bacon, his Great Idea was the scientific method, and he was the only crackpot in all history to claim that level of benefit to humanity and turn out to be completely right.</p> <p>(Bacon didn't singlehandedly invent science, of course, but he did contribute, and may have been the first to realize the power.)</p> <p>That's the problem with deciding that you'll never admire anything that much:  Some ideas really <em>are</em> that good.  Though no one has <em>fulfilled</em> claims more audacious than Bacon's; at least, not yet.</p> <p>But then how can we resist the <a href="0180.html">happy death spiral</a> [http://lesswrong.com/lw/lm/affective_death_spirals/] with respect to Science itself?  The <a href="0180.html">happy death spiral</a> [http://lesswrong.com/lw/lm/affective_death_spirals/] starts when you believe something is <em>so</em> wonderful that the <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/] leads you to find <em>more </em>and <em>more</em> nice things to say about it, making you see it as <em>even more</em> wonderful, and so on, spiraling up into the abyss.  What if Science is <em>in fact</em> so beneficial that we cannot acknowledge its true glory and retain our sanity?  Sounds like a nice thing to say, doesn't it?  <em>Oh no it's starting ruuunnnnn...</em></p> <p><a id="more"></a></p> <p>If you retrieve the <a href="0128.html">standard</a> [http://lesswrong.com/lw/k6/the_outside_the_box_box/] <a href="0127.html">cached</a> [http://lesswrong.com/lw/k5/cached_thoughts/] <a href="0130.html">deep wisdom</a> [http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/] for <em>don't go overboard on admiring science,</em> you will find thoughts like "Science gave us air conditioning, but it also made the hydrogen bomb" or "Science can tell us about stars and biology, but it <a href="0058.html">can never prove or disprove</a> [http://lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/] the <a href="0054.html">dragon in my garage</a> [http://lesswrong.com/lw/i4/belief_in_belief/]."  But the people who <em>originated</em> such thoughts were <em>not</em> trying to resist a happy death spiral.  They weren't worrying about their own admiration of science spinning out of control.  Probably they didn't like something science had to say about their pet beliefs, and sought ways to undermine its authority.</p> <p>The <em>standard </em>negative things to say about science, aren't likely to appeal to someone who genuinely feels the exultation of science&#8212;that's not the intended audience.  So we'll have to search for other negative things to say instead.</p> <p>But if you look selectively for something negative to say about science&#8212;even in an attempt to resist a happy death spiral&#8212;do you not automatically convict yourself of <a href="0116.html">rationalization</a> [http://lesswrong.com/lw/ju/rationalization/]? Why would you pay attention to your own thoughts, if you knew you were trying to <a href="0100.html">manipulate yourself</a> [http://lesswrong.com/lw/je/doublethink_choosing_to_be_biased/]?</p> <p>I am generally skeptical of people who claim that one bias can be used to counteract another.  It sounds to me like an automobile mechanic who says that the motor is broken on your right windshield wiper, but instead of fixing it, they'll just break your left windshield wiper to balance things out.  This is the sort of cleverness that leads to shooting yourself in the foot.  Whatever the solution, it ought to involve believing true things, rather than believing you believe things that you believe are false.</p> <p>Can you prevent the happy death spiral by restricting your admiration of Science to a narrow domain?  Part of the happy death spiral is seeing the Great Idea everywhere&#8212;thinking about how Communism could cure cancer if it was only given a chance.  Probably the single most reliable sign of a cult guru is that the guru claims expertise, not in one area, not even in a cluster of related areas, but in <em>everything.</em>  The guru knows what cult members should eat, wear, do for a living; who they should have sex with; which art they should look at; which music they should listen to...</p> <p>Unfortunately for this plan, most people fail miserably when they try to describe the neat little box that science has to stay inside.  The usual trick, "Hey, science won't cure cancer" isn't going to fly.  "Science has nothing to say about a parent's love for their child"&#8212;sorry, that's simply <a href="0159.html">false</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/].  If you try to sever science from e.g. parental love, you aren't just denying cognitive science and evolutionary psychology.  You're also denying Martine Rothblatt's founding of United Therapeutics to seek a cure for her daughter's pulmonary hypertension.  (Successfully, I might add.)  Science is legitimately related, one way or another, to just about every important facet of human existence.</p> <p>All right, so what's an example of a <em>false</em> nice claim you could make about science?</p> <p>In my humble opinion, one false claim is that science is so wonderful that <a href="http://intelligence.org/blog/2007/10/21/should-ethicists-be-inside-or-outside-a-profession/">scientists shouldn't even try to take ethical responsibility for their work</a> [http://intelligence.org/blog/2007/10/21/should-ethicists-be-inside-or-outside-a-profession/], it will automatically end well.  This claim, to me, seems to misunderstand the nature of the process whereby science benefits humanity.  Scientists are human, they have prosocial concerns just like most other other people, and this is at least <em>part</em> of why science ends up doing more good than evil.</p> <p>But that point is, evidently, not beyond dispute.  So here's a simpler false nice claim:  "A cancer patient can be cured just by publishing enough journal papers."  Or, "Sociopaths could become fully normal, if they just committed themselves to never believing anything without replicated experimental evidence with p&lt;0.05."</p> <p>The way to avoid believing such statements isn't an affective cap, deciding that science is only slightly nice.  Nor searching for reasons to believe that publishing journal papers <em>causes</em> cancer.  Nor believing that science has nothing to say about cancer one way or the other.</p> <p>Rather, if you know with enough <a href="0062.html">specificity</a> [http://lesswrong.com/lw/ic/the_virtue_of_narrowness/] how science works, then you know that, while it may be possible for "science to cure cancer", a cancer patient writing journal papers isn't going to experience a miraculous remission.  That <em>specific</em> proposed chain of cause and effect is not going to work out.</p> <p>The happy death spiral is only an emotional problem because of a perceptual problem, the <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/], which makes us more likely to accept future positive claims once we've accepted an initial positive claim.  We can't get rid of this effect just by wishing; it will probably always influence us a little.  But we can manage to slow down, stop, consider each additional nice claim as an additional <a href="0106.html">burdensome detail</a> [http://lesswrong.com/lw/jk/burdensome_details/], and focus on the specific points of the claim apart from its positiveness.</p> <p>What if a specific nice claim "can't be disproven" but there are arguments "both for and against" it?  Actually these are words to be wary of in general, because often this is what people say when they're <a href="0070.html">rehearsing the evidence</a> [http://lesswrong.com/lw/ik/one_argument_against_an_army/] or <a href="0120.html">avoiding the real weak points</a> [http://lesswrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/].  Given the danger of the happy death spiral, it makes sense to try to avoid being happy about <em>unsettled</em> claims&#8212;to avoid making them into a source of yet more positive affect about something you liked already.</p> <p>The happy death spiral is only a <em>big</em> emotional problem because of the overly positive feedback, the ability for the process to go critical.  You may not be able to eliminate the halo effect entirely, but you can apply enough critical reasoning to keep the halos subcritical&#8212;make sure that the resonance dies out rather than exploding.</p> <p>You might even say that the whole problem starts with people not bothering to critically examine <a href="0106.html">every additional burdensome detail</a> [http://lesswrong.com/lw/jk/burdensome_details/]&#8212;demanding <a href="0109.html">sufficient</a> [http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/] evidence to compensate for <a href="0111.html">complexity</a> [http://lesswrong.com/lw/jp/occams_razor/], <a href="0115.html">searching</a> [http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/] for flaws as well as support, invoking <a href="0121.html">curiosity</a> [http://lesswrong.com/lw/jz/the_meditation_on_curiosity/]&#8212;once they've accepted some core premise.  Without the <a href="0104.html">conjunction fallacy</a> [http://lesswrong.com/lw/ji/conjunction_fallacy/], there might still be a <a href="0177.html">halo effect</a> [http://lesswrong.com/lw/lj/the_halo_effect/], but there wouldn't be a <a href="0180.html">happy death spiral</a> [http://lesswrong.com/lw/lm/affective_death_spirals/].</p> <p>Even on the nicest Nice Thingies in the known universe, a perfect rationalist who demanded exactly the necessary evidence for every additional (positive) claim, would experience no affective resonance. You can't do this, but you can stay close enough to rational to keep your happiness from spiraling out of control.</p> <p>The really dangerous cases are the ones where <em>any criticism of any positive claim about the Great Thingy feels bad or is socially unacceptable.</em>  <a href="0010.html">Arguments are soldiers, any positive claim is a soldier on our side, stabbing your soldiers in the back is treason.</a> [http://lesswrong.com/lw/gw/politics_is_the_mindkiller/]  Then the chain reaction goes <em>super</em>critical.  More on this tomorrow.</p> <p><strong>Addendum:</strong>  Stuart Armstrong gives closely related <a href="0180.html">advice:</a> [http://lesswrong.com/lw/lm/affective_death_spirals/gp5]</p> <blockquote> <p>Cut up your Great Thingy into smaller independent ideas, <em>and treat them as independent.</em></p> <p>For instance a marxist would cut up Marx's Great Thingy into a theory of value of labour, a theory of the political relations between classes, a theory of wages, a theory on the ultimate political state of mankind. Then each of them should be assessed independently, and the truth or falsity of one should not halo on the others. If we can do that, we should be safe from the spiral, as each theory is too narrow to start a spiral on its own.</p> </blockquote> <p>This, metaphorically, is like keeping subcritical masses of plutonium from coming together.  Three Great Ideas are far less likely to drive you mad than one Great Idea.  Armstrong's advice also helps promote specificity:  As soon as someone says, "Publishing enough papers can cure your cancer," you ask, "Is that a benefit of the experimental method, and if so, at which stage of the experimental process is the cancer cured?  Or is it a benefit of science as a social process, and if so, does it rely on individual scientists wanting to cure cancer, or can they be self-interested?"  Hopefully this leads you away from the good or bad feeling, and toward noticing the confusion and lack of support.</p> <p><strong>Addendum 2:</strong>  To summarize, you <em>do</em> avoid a Happy Death Spiral by (1) splitting the Great Idea into parts (2) treating every additional detail as burdensome (3) thinking about the specifics of the causal chain instead of the good or bad feelings (4) not rehearsing evidence (5) not adding happiness from claims that "you can't <em>prove</em> are wrong"; but <em>not</em> by (6) refusing to admire anything too much (7) conducting a biased search for negative points until you feel unhappy again (8) forcibly shoving an idea into a safe box.</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor"><em>Death Spirals and the Cult Attractor</em></a> [http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0182.html">Uncritical Supercriticality</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/]"</p> <p style="text-align:right">Previous post: "<a href="0180.html">Affective Death Spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq04.html">Sequence 04: Death Spirals and the Cult Attractor</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0180.html">Affective Death Spirals</a></p></td><td><p><i>Next: </i><a href="0182.html">Uncritical Supercriticality</a></p></td></tr></table><p><i>Referenced by: </i><a href="0180.html">Affective Death Spirals</a> &#8226; <a href="0182.html">Uncritical Supercriticality</a> &#8226; <a href="0183.html">Fake Fake Utility Functions</a> &#8226; <a href="0189.html">Every Cause Wants To Be A Cult</a> &#8226; <a href="0195.html">Guardians of Ayn Rand</a> &#8226; <a href="0207.html">Cultish Countercultishness</a> &#8226; <a href="0391.html">Heading Toward Morality</a> &#8226; <a href="0509.html">Crisis of Faith</a> &#8226; <a href="0639.html">The Thing That I Protect</a> &#8226; <a href="0708.html">That Crisis thing seems pretty useful</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/">Resist the Happy Death Spiral</a></p></body></html>