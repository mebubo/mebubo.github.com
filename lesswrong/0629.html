<html><head><title>Value is Fragile</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Value is Fragile</h1><p><i>Eliezer Yudkowsky, 29 January 2009 08:46AM</i></p><div><p><strong>Followup to</strong>:  <a href="0624.html">The Fun Theory Sequence</a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/], <a href="0183.html">Fake Fake Utility Functions</a> [http://lesswrong.com/lw/lp/fake_fake_utility_functions/], <a href="0443.html">Joy in the Merely Good</a> [http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/], <a href="0171.html">The Hidden Complexity of Wishes</a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/], <a href="0420.html">The Gift We Give To Tomorrow</a> [http://lesswrong.com/lw/sa/the_gift_we_give_to_tomorrow/], <a href="0397.html">No Universally Compelling Arguments</a> [http://lesswrong.com/lw/rn/no_universally_compelling_arguments/], <a href="0439.html">Anthropomorphic Optimism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/], <a href="0459.html">Magical Categories</a> [http://lesswrong.com/lw/td/magical_categories/], ...</p> <p>If I had to pick a single statement that <em>relies</em> on more Overcoming Bias content I've written than any other, that statement would be:</p> <p><em>Any Future <strong>not </strong>shaped by a goal system with detailed reliable inheritance from human morals and metamorals, will contain almost nothing of worth.</em></p> <p>"Well," says the one, "maybe according to your provincial <em>human</em> values, <em>you </em>wouldn't like it.  But I can easily imagine a galactic civilization full of agents who are nothing like <em>you</em>, yet find great value and interest in their <em>own</em> goals.  And that's fine by me.  I'm not so bigoted as you are.  Let the Future go its own way, without trying to bind it forever to the laughably primitive prejudices of a pack of four-limbed Squishy Things -"</p> <p>My friend, I have <em>no problem</em> with the thought of a galactic civilization vastly unlike our own... full of strange beings who look nothing like me even in their own imaginations... pursuing pleasures and experiences I can't begin to empathize with... trading in a marketplace of unimaginable goods... allying to pursue incomprehensible objectives... people whose life-stories I could never understand.</p> <p>That's what the Future looks like if things go <em>right.</em></p> <p>If the chain of inheritance from human (meta)morals is broken, the Future does <em>not </em>look like this.  It does <em>not</em> end up magically, delightfully incomprehensible.</p> <p>With very high probability, it ends up looking <em>dull.</em>  Pointless.  Something whose loss you wouldn't mourn.</p> <p>Seeing this as obvious, is what requires that immense amount of background explanation.</p> <p><a id="more"></a></p> <p>And I'm not going to iterate through <em>all </em>the points and winding pathways of argument here, because that would take us back through 75% of my <em>Overcoming Bias</em> posts.  Except to remark on how <em>many</em> different things must be known to constrain the final answer.</p> <p>Consider the <a href="0617.html">incredibly important human value of "boredom"</a> [http://lesswrong.com/lw/xr/in_praise_of_boredom/] - our desire not to do "the same thing" over and over and over again.  You can imagine a mind that contained <em>almost</em> the whole specification of human value, almost all the morals and metamorals, but left out <em>just this one thing</em> -</p> <p>- and so it spent until the end of time, and until the farthest reaches of its light cone, replaying a single highly optimized experience, over and over and over again.</p> <p>Or imagine a mind that contained almost the whole specification of which sort of feelings humans most enjoy - but not the idea that those feelings had important <em>external referents.</em>  So that the mind just went around <em>feeling</em> like it had made an important discovery, <em>feeling</em> it had found the perfect lover, <em>feeling</em> it had helped a friend, but not actually <em>doing</em> any of those things - having become its own experience machine.  And if the mind pursued those feelings <em>and their referents,</em> it would be a good future and true; but because this <em>one dimension</em> of value was left out, the future became something dull.  Boring and repetitive, because although this mind <em>felt</em> that it was encountering experiences of incredible novelty, this feeling was in no wise true.</p> <p>Or the converse problem - an agent that contains all the aspects of human value, <em>except </em>the valuation of subjective experience.  So that the result is a nonsentient optimizer that goes around making genuine discoveries, but the discoveries are not savored and enjoyed, because there is no one there to do so.  This, I admit, I don't quite know to be possible.  Consciousness does still confuse me to some extent.  But a universe with no one to bear witness to it, might as well not be.</p> <p>Value isn't just complicated, it's <em>fragile.</em>  There is <em>more than one dimension</em> of human value, where <em>if just that one thing is lost</em>, the Future becomes null.  A <em>single</em> blow and <em>all </em>value shatters.  Not every <em>single </em>blow will shatter <em>all </em>value - but more than one possible "single blow" will do so.</p> <p>And then there are the long defenses of this proposition, which relies on 75% of my <em>Overcoming Bias</em> posts, so that it would be more than one day's work to summarize all of it.  Maybe some other week.  There's so many branches I've seen that discussion tree go down.</p> <p>After all - a mind <em>shouldn't</em> just go around having the same experience over and over and over again.  Surely no superintelligence would be so <a href="0444.html">grossly mistaken</a> [http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/] about the <a href="0443.html">correct action</a> [http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/]?</p> <p>Why would any supermind <a href="0439.html">want</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/] something so <a href="0284.html">inherently</a> [http://lesswrong.com/lw/oi/mind_projection_fallacy/] <a href="0444.html">worthless</a> [http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/] as the feeling of discovery without any real discoveries?  Even if that were its utility function, wouldn't it just <a href="0387.html">notice that its utility function was wrong</a> [http://lesswrong.com/lw/rd/passing_the_recursive_buck/], and rewrite it?  It's got <a href="0386.html">free will</a> [http://lesswrong.com/lw/rc/the_ultimate_source/], right?</p> <p>Surely, at least <em>boredom</em> has to be a <a href="0397.html">universal</a> [http://lesswrong.com/lw/rn/no_universally_compelling_arguments/] value.  It evolved in humans because it's valuable, right?  So any mind that doesn't share our dislike of repetition, will fail to thrive in the universe and be eliminated...</p> <p>If you are familiar with the difference between <a href="0162.html">instrumental values and terminal values,</a> [http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/] <em>and</em> familiar with the <a href="0151.html">stupidity</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/] of <a href="0149.html">natural selection</a> [http://lesswrong.com/lw/kr/an_alien_god/], <em>and </em>you understand how this stupidity manifests in the difference between <a href="0158.html">executing adaptations versus maximizing fitness</a> [http://lesswrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/], <em>and</em> you know this turned <a href="0159.html">instrumental subgoals of reproduction into decontextualized unconditional emotions</a> [http://lesswrong.com/lw/l1/evolutionary_psychology/]...</p> <p>...<em>and</em> you're familiar with how the tradeoff between exploration and exploitation works in Artificial Intelligence...</p> <p>...then you might be able to see that the human form of boredom that demands a steady trickle of novelty for its own sake, isn't a grand universal, but just a particular algorithm that evolution coughed out into us.  And you might be able to see how the vast majority of possible expected utility maximizers, would only engage in just so much efficient exploration, and spend most of its time exploiting the best alternative found so far, over and over and over.</p> <p>That's a lot of background knowledge, though.</p> <p>And so on and so on and so on through 75% of my posts on <em>Overcoming Bias</em>, and many chains of fallacy and counter-explanation.  Some week I may try to write up the whole diagram.  But for now I'm going to assume that you've read the arguments, and just deliver the conclusion:</p> <p>We can't relax our grip on the future - let go of the steering wheel - and still end up with anything of value.</p> <p>And those who think we <em>can </em>-</p> <p>- they're trying to be cosmopolitan.  I understand that.  I read those same science fiction books as a kid:  The provincial villains who enslave aliens for the crime of not looking just like humans.  The provincial villains who enslave helpless AIs in durance vile on the assumption that silicon can't be sentient.  And the cosmopolitan heroes who understand that <em>minds don't have to be just like us to be embraced as valuable</em> -</p> <p>I read those books.  I once believed them.  But the beauty that jumps out of one box, is not jumping out of <em>all </em>boxes.  (This being the moral of the sequence on <a href="0540.html">Lawful Creativity</a> [http://lesswrong.com/lw/vm/lawful_creativity/].)  If you leave behind all order, what is left is not the perfect answer, what is left is perfect noise.  Sometimes you have to abandon an old design rule to build a better mousetrap, but that's not the same as giving up all design rules and collecting wood shavings into a heap, with every pattern of wood as good as any other.  The old rule is always abandoned at the behest of some higher rule, some higher criterion of value that governs.</p> <p>If you loose the grip of human morals and metamorals - the result is not mysterious and alien and beautiful by the standards of human value.  It is moral noise, a universe tiled with paperclips.  To change away from human morals <em>in the direction of improvement rather than entropy,</em> requires a criterion of improvement; and that criterion would be physically represented in our brains, and our brains alone.</p> <p>Relax the grip of human value upon the universe, and it will end up <em>seriously </em>valueless.  Not, strange and alien and wonderful, shocking and terrifying and beautiful beyond all human imagination.  Just, tiled with paperclips.</p> <p>It's only some <em>humans</em>, you see, who have this idea of embracing manifold varieties of mind - of wanting the Future to be something greater than the past - of being not bound to our past selves - of trying to change and move forward.</p> <p>A paperclip maximizer just chooses whichever action leads to the greatest number of paperclips.</p> <p>No free lunch.  You want a wonderful and mysterious universe?  That's <em>your </em>value.  <em>You </em>work to create that value.  Let that value exert its force through you who represents it, let it make decisions in you to shape the future.  And maybe you shall indeed obtain a wonderful and mysterious universe.</p> <p>No free lunch.  Valuable things appear because a goal system that values them takes action to create them.  Paperclips don't materialize from nowhere for a paperclip maximizer.  And a wonderfully alien and mysterious Future will not materialize from nowhere for us humans, if our values that prefer it are physically obliterated - or even <em>disturbed</em> in the wrong dimension.  Then there is nothing left in the universe that works to make the universe valuable.</p> <p>You <em>do</em> have values, even when you're trying to be "cosmopolitan", trying to display a properly virtuous appreciation of alien minds.  Your values are then <a href="0456.html">faded further into the invisible background</a> [http://lesswrong.com/lw/ta/invisible_frameworks/] - they are less <em>obviously</em> human.  Your brain probably <a href="0439.html">won't even generate an alternative so awful</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/] that it would wake you up, make you say "No!  Something went wrong!" even at your most cosmopolitan.  E.g. "a nonsentient optimizer absorbs all matter in its future light cone and tiles the universe with paperclips".  You'll just imagine strange alien worlds to appreciate.</p> <p>Trying to be "cosmopolitan" - to be <em>a citizen of the cosmos</em> - just strips off a <em>surface veneer</em> of goals that seem <em>obviously</em> "human".</p> <p>But if you wouldn't like the Future tiled over with paperclips, and you would prefer a civilization of...</p> <p>...sentient beings...</p> <p>...with enjoyable experiences...</p> <p>...that aren't the <em>same </em>experience over and over again...</p> <p>...and are bound to something besides just being a sequence of internal pleasurable feelings...</p> <p>...learning, discovering, freely choosing...</p> <p>...well, I've just been through <a href="0624.html">the posts on Fun Theory</a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/] that went into <em>some </em>of the <a href="0171.html">hidden details</a> [http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/] on those <a href="0459.html">short English words</a> [http://lesswrong.com/lw/td/magical_categories/].</p> <p>Values that you might praise as <em>cosmopolitan</em> or <em>universal </em>or <em>fundamental </em>or <em>obvious common sense,</em> are represented in your brain just as much as those values that you might dismiss as <em>merely human.</em>  Those values come of the long history of humanity, and the <a href="0420.html">morally miraculous stupidity of evolution that created us</a> [http://lesswrong.com/lw/sa/the_gift_we_give_to_tomorrow/].  (And once I <em>finally </em>came to that realization, I felt less ashamed of values that seemed 'provincial' - but that's another matter.)</p> <p>These values do <em>not</em> emerge in all possible minds.  They will <em>not</em> appear from nowhere to rebuke and revoke the utility function of an expected paperclip maximizer.</p> <p>Touch too hard in the wrong dimension, and the physical representation of those values will shatter - and <em>not come back</em>, for there will be nothing left to <em>want </em>to bring it back.</p> <p>And the <em>referent </em>of those values - a worthwhile universe - would no longer have any physical reason to come into being.</p> <p>Let go of the steering wheel, and the Future crashes.</p></div> <hr><p><i>Referenced by: </i><a href="0639.html">The Thing That I Protect</a> &#8226; <a href="0674.html">Raising the Sanity Waterline</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/y3/value_is_fragile/">Value is Fragile</a></p></body></html>