<html><head><title>Purchase Fuzzies and Utilons Separately</title></head><body><h1>Purchase Fuzzies and Utilons Separately</h1><p><i>Eliezer Yudkowsky, 01 April 2009 09:51AM</i></p><div><p><strong>Previously in series</strong>:  <a href="0694.html">Money: The Unit of Caring</a> [http://lesswrong.com/lw/65/money_the_unit_of_caring/]</p> <p>Yesterday:</p> <blockquote> <p>There <em>is</em> this very, very old puzzle/observation in economics about the lawyer who spends an hour volunteering at the soup kitchen, instead of working an extra hour and donating the money to hire someone...</p> <p>If the lawyer needs to work an hour at the soup kitchen to keep himself motivated and remind himself why he's doing what he's doing, <em>that's fine.</em>  But he should <em>also</em> be donating some of the hours he worked at the office, because that is the power of professional specialization and it is how grownups really get things done.  One might consider the check as buying the right to volunteer at the soup kitchen, or validating the time spent at the soup kitchen.</p> </blockquote> <p>I hold open doors for little old ladies.  I can't actually remember the last time this happened literally (though I'm sure it has, sometime in the last year or so).  But within the last month, say, I was out on a walk and discovered a station wagon parked in a driveway with its trunk completely open, giving full access to the car's interior.  I looked in to see if there were packages being taken out, but this was not so.  I looked around to see if anyone was doing anything with the car.  And finally I went up to the house and knocked, then rang the bell.  And yes, the trunk had been accidentally left open.</p> <p>Under other circumstances, this would be a simple act of altruism, which might signify true concern for another's welfare, or fear of guilt for inaction, or a desire to signal trustworthiness to oneself or others, or finding altruism pleasurable.  I think that these are all perfectly legitimate motives, by the way; I might give bonus points for the first, but I wouldn't deduct any penalty points for the others.  Just so long as people get helped.</p> <p>But in my own case, since I already work in the nonprofit sector, the further question arises as to whether I could have better employed the same sixty seconds in a more <em>specialized</em> way, to bring greater benefit to others.  That is: can I really defend this as the <em>best</em> use of my time, given the other things I claim to believe?<a id="more"></a></p> <p>The obvious defense&#8212;or perhaps, obvious rationalization&#8212;is that an act of altruism like this one acts as an <a href="http://en.wikipedia.org/wiki/Ego_depletion">willpower restorer</a> [http://en.wikipedia.org/wiki/Ego_depletion], much more efficiently than, say, listening to music.  I also mistrust my ability to be an altruist <em>only</em> in theory; I suspect that if I walk past problems, my altruism will start to fade.  I've never pushed that far enough to test it; it doesn't seem worth the risk.</p> <p>But if that's the defense, then my act can't be defended as a good deed, can it?  For these are self-directed benefits that I list.</p> <p>Well&#8212;who said that I <em>was</em> defending the act as a selfless good deed?  It's a <em>selfish</em> good deed.  If it restores my willpower, or if it keeps me altruistic, then there are indirect other-directed benefits from that (or so I believe).  You could, of course, reply that you don't trust selfish acts that are supposed to be other-benefiting as an "ulterior motive"; but then I could just as easily respond that, by the same principle, you should just look directly at the original good deed rather than <em>its</em> supposed ulterior motive.</p> <p>Can I get away with that?  That is, can I really get away with calling it a "selfish good deed", and still derive willpower restoration therefrom, rather than feeling guilt about it being selfish?  Apparently I can.  I'm surprised it works out that way, but it does.  So long as I knock to tell them about the open trunk, and so long as the one says "Thank you!", my brain feels like it's done its wonderful good deed for the day.</p> <p>Your mileage may vary, of course.  The problem with trying to work out an art of willpower restoration is that different things seem to work for different people.  (<a href="0565.html">That is</a> [http://lesswrong.com/lw/wb/chaotic_inversion/]:  We're probing around on the level of surface phenomena without understanding the deeper rules that would also predict the variations.)</p> <p>But if you find that you are like me in this aspect&#8212;that selfish good deeds still work&#8212;then I recommend that you <em>purchase warm fuzzies and utilons separately.</em>  Not at the same time.  Trying to do both at the same time just means that neither ends up done well.  If status matters to you, purchase status separately too!</p> <p>If I had to give advice to some new-minted billionaire entering the realm of charity, my advice would go something like this:</p> <ul> <li>To purchase warm fuzzies, find some hard-working but poverty-stricken woman who's about to drop out of state college after her husband's hours were cut back, and personally, but anonymously, give her a cashier's check for $10,000.  Repeat as desired.</li> <li>To purchase status among your friends, donate $100,000 to the current sexiest X-Prize, or whatever other charity seems to offer the most stylishness for the least price.  Make a big deal out of it, show up for their press events, and brag about it for the next five years.</li> <li>Then&#8212;with absolute cold-blooded calculation&#8212;without <a href="0046.html">scope insensitivity</a> [http://lesswrong.com/lw/hw/scope_insensitivity/] or <a href="http://en.wikipedia.org/wiki/Ambiguity_aversion">ambiguity aversion</a> [http://en.wikipedia.org/wiki/Ambiguity_aversion]&#8212;without concern for status or warm fuzzies&#8212;figuring out some common scheme for converting outcomes to utilons, and trying to express uncertainty in percentage probabilitiess&#8212;find the charity that offers the greatest expected utilons per dollar.  Donate up to however much money you wanted to give to charity, until their marginal efficiency drops below that of the next charity on the list.</li> </ul> <p>I would furthermore advise the billionaire that what they spend on utilons should be at least, say, 20 times what they spend on warm fuzzies&#8212;5% overhead on keeping yourself altruistic seems reasonable, and I, your dispassionate judge, would have no trouble <em>validating </em>the warm fuzzies against a multiplier that large.  Save that the original, fuzzy act really should be helpful rather than actively harmful.</p> <p>(Purchasing <em>status</em> seems to me essentially unrelated to altruism.  If giving money to the X-Prize gets you more awe from your friends than an equivalently priced speedboat, then there's really no reason to buy the speedboat.  Just put the money under the "impressing friends" column, and be aware that this is not the "altruism" column.)</p> <p>But the main lesson is that all three of these things&#8212;warm fuzzies, status, and expected utilons&#8212;can be bought <em>far</em> more efficiently when you buy <em>separately</em>, optimizing for only one thing at a time.  Writing a check for $10,000,000 to a breast-cancer charity&#8212;while far more laudable than spending the same $10,000,000 on, I don't know, parties or something&#8212;won't give you the concentrated euphoria of being present in person when you turn a single human's life around, probably not anywhere <em>close</em>.  It won't give you as much to talk about at parties as donating to something sexy like an X-Prize&#8212;maybe a short nod from the other rich.  And if you threw away all concern for warm fuzzies and status, there are probably at least a <em>thousand</em> underserved existing charities that could produce <em>orders of magnitude</em> more utilons with ten million dollars.  Trying to optimize for all three criteria in one go only ensures that none of them end up optimized very well&#8212;just vague pushes along all three dimensions.</p> <p>Of course, if you're not a millionaire or even a billionaire&#8212;then you can't be quite as <em>efficient</em> about things, can't so easily purchase in bulk.  But I would still say&#8212;for warm fuzzies, find a relatively <em>cheap </em>charity with bright, vivid, ideally in-person and direct beneficiaries.  Volunteer at a soup kitchen.  Or just get your warm fuzzies from holding open doors for little old ladies.  Let that be <em>validated</em> by your other efforts to purchase utilons, but don't <em>confuse</em> it with purchasing utilons.  Status is probably cheaper to purchase by buying nice clothes.</p> <p>And when it comes to purchasing expected utilons&#8212;then, of course, <a href="0233.html">shut up and multiply</a> [http://lesswrong.com/lw/n3/circular_altruism/].</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="0726.html"><em>The Craft and the Community</em></a> [http://lesswrong.com/lw/cz/the_craft_and_the_community/]</p> <p style="text-align:right">Next post: "<a href="0697.html">Selecting Rationalist Groups</a> [http://lesswrong.com/lw/77/selecting_rationalist_groups/]"</p> <p style="text-align:right">Previous post: "<a href="0694.html">Money: The Unit of Caring</a> [http://lesswrong.com/lw/65/money_the_unit_of_caring/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq16.html">Sequence 16: The Craft and the Community</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0694.html">Money: The Unit of Caring</a></p></td><td><p><i>Next: </i><a href="0697.html">Selecting Rationalist Groups</a></p></td></tr></table><p><i>Referenced by: </i><a href="0694.html">Money: The Unit of Caring</a> &#8226; <a href="0697.html">Selecting Rationalist Groups</a> &#8226; <a href="0726.html">The Craft and the Community</a> &#8226; <a href="0727.html">The End (of Sequences)</a> &#8226; <a href="0812.html">Firewalling the Optimal from the Rational</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/">Purchase Fuzzies and Utilons Separately</a></p></body></html>