<html><head><title>Allais Malaise</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Allais Malaise</h1><p><i>Eliezer Yudkowsky, 21 January 2008 12:40AM</i></p><div><p><strong>Continuation of</strong>:  <a href="0228.html">The Allais Paradox</a> [http://lesswrong.com/lw/my/the_allais_paradox/], <a href="0229.html">Zut Allais!</a> [http://lesswrong.com/lw/mz/zut_allais/]</p> <p>Judging by the comments on Zut Allais, I failed to emphasize the points that needed emphasis.</p> <p><strong>The problem with the Allais Paradox is the incoherent <em>pattern</em> 1A &gt; 1B, 2B &gt; 2A.  </strong>If you need $24,000 for a lifesaving operation and an extra $3,000 won't help that much, then you choose 1A &gt; 1B <strong>and</strong> 2A &gt; 2B.  If you have a million dollars in the bank account and your utility curve doesn't change much with an extra $25,000 or so, then you should choose 1B &gt; 1A <strong>and</strong> 2B &gt; 2A.  <strong>Neither the individual choice 1A &gt; 1B, nor the individual choice 2B &gt; 2A, are of themselves irrational.</strong>  It's the <strong>combination</strong> that's the problem.</p> <p><strong>Expected utility is not expected dollars.</strong>  In the case above, the utility-distance from $24,000 to $27,000 is a tiny fraction of the distance from $21,000 to $24,000.  So, as stated, you should choose 1A &gt; 1B and 2A &gt; 2B, a quite coherent combination.  <strong>The Allais Paradox has nothing to do with believing that every added dollar is equally useful.</strong>  That idea has been rejected since the dawn of decision theory.</p> <p><strong>If satisfying your intuitions is more important to you than money, do whatever the heck you want.</strong>  Drop the money over Niagara falls.  Blow it all on expensive champagne.  Set fire to your hair.  Whatever.  <strong>If the largest utility you care about is the utility of feeling good about your decision, then any decision that feels good is the right one.</strong>  If you say that different trajectories to the same outcome "matter emotionally", then you're attaching an inherent utility to conforming to the brain's native method of optimization, whether or not it actually optimizes.  Heck, <strong>running around in circles from preference reversals</strong> could feel really good too.  <strong>But if you care enough about the stakes that winning is <em>more important</em> than your brain's good feelings about an intuition-conforming strategy, <em>then</em> use decision theory.</strong></p><a id="more"></a><p><strong>If you suppose the problem is different from the one presented</strong> - that the gambles are untrustworthy and that, after this mistrust is taken into account, the payoff probabilities are not as described - then, obviously, <strong>you can make the answer anything you want.</strong></p> <p> Let's say you're dying of thirst, you only have $1.00, and you have to choose between a vending machine that dispenses a drink with certainty for $0.90, versus spending $0.75 on a vending machine that dispenses a drink with 99% probability.  Here, the 1% chance of dying is worth more to you than $0.15, so you would pay the extra fifteen cents.  You would also pay the extra fifteen cents if the two vending machines dispensed drinks with 75% probability and 74% probability respectively.  <strong>The 1% probability is worth the same amount whether or not it's the last increment towards certainty.</strong>  This pattern of decisions is perfectly coherent.  <strong>Don't confuse being rational with being shortsighted or greedy.</strong></p> <p><em>Added:</em>  A 50% probability of $30K and a 50% probability of $20K, is not the same as a 50% probability of $26K and a 50% probability of $24K.  If your utility is logarithmic in money (the standard assumption) then you will definitely prefer the latter to the former:  0.5 log(30) + 0.5 log(20)  &lt;  0.5 log(26) + 0.5 log(24).  <strong>You take the expectation of the utility of the money, not the utility of the expectation of the money.</strong></p></div> <hr><p><i>Referenced by: </i><a href="0239.html">The "Intuitions" Behind "Utilitarianism"</a> &#8226; <a href="0254.html">How An Algorithm Feels From Inside</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/n1/allais_malaise/">Allais Malaise</a></p></body></html>