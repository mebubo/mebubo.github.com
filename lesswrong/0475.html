<html><head><title>Points of Departure</title></head><body><h1>Points of Departure</h1><p><i>Eliezer Yudkowsky, 09 September 2008 09:18PM</i></p><div><p><strong>Followup to</strong>:  <a href="0439.html">Anthropomorphic Optimism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/]</p> <p>If you've watched Hollywood sci-fi involving supposed robots, androids, or AIs, then you've seen AIs that are depicted as "emotionless".  In the olden days this was done by having the AI speak in a monotone pitch - while perfectly stressing the syllables, of course.  (I could similarly go on about how AIs that disastrously misinterpret their mission instructions, never seem to need help parsing spoken English.)  You can also show that an AI is "emotionless" by having it notice an emotion with a blatant somatic effect, like tears or laughter, and ask what it means (though of course the AI never asks about sweat or coughing).</p> <p>If you watch <em>enough</em> Hollywood sci-fi, you'll run into all of the following situations occurring with supposedly "emotionless" AIs:</p> <ol><li>An AI that malfunctions or otherwise turns evil, instantly acquires all of the negative human emotions - it hates, it wants revenge, and feels the need to make self-justifying speeches.</li> <li>Conversely, an AI that turns to the Light Side, <em>gradually</em> acquires a <em>full</em> complement of human emotions.</li> <li>An "emotionless" AI suddenly exhibits human emotion when under <em>exceptional</em> stress; e.g. an AI that displays no reaction to thousands of deaths, suddenly showing remorse upon killing its creator.</li> <li>An AI begins to exhibit signs of human emotion, and <em>refuses to admit it.</em></li></ol> <p>Now, why might a Hollywood scriptwriter make those <em>particular</em> mistakes?</p><a id="more"></a><p>These mistakes seem to me to bear the signature of modeling an Artificial Intelligence as an <em>emotionally repressed human.</em></p> <p>At least, I <em>can't seem to think of any other simple hypothesis that explains the behaviors</em> 1-4 above.  The AI that turns evil has lost its negative-emotion-suppressor, so the negative emotions suddenly switch on.  The AI that turns from mechanical agent to good agent, gradually loses the emotion-suppressor keeping it mechanical, so the good emotions rise to the surface.  Under <em>exceptional </em>stress, <em>of course</em> the emotional repression that keeps the AI "mechanical" will <em>immediately</em> break down and let the emotions out.  But if the stress isn't so exceptional, the firmly repressed AI will deny any hint of the emotions leaking out - that conflicts with the AI's self-image of itself as being emotionless.</p> <p>It's not that the Hollywood scriptwriters are explicitly reasoning "An AI will be like an emotionally repressed human", of course; but rather that when they imagine an "emotionless AI", this is the intuitive model that forms in the background - a Standard mind (which is to say a human mind) plus an extra Emotion Suppressor.</p> <p>Which all goes to illustrate yet another fallacy of anthropomorphism - treating humans as your <em>point of departure</em>, modeling a mind as a human plus a set of differences.</p> <p>This is a logical fallacy because it warps <a href="0111.html">Occam's Razor</a> [http://lesswrong.com/lw/jp/occams_razor/].  A mind that <em>entirely lacks</em> chunks of brainware to implement "hate" or "kindness", is simpler - in a computational complexity sense - than a mind that has "hate" plus a "hate-suppressor", or "kindness" plus a "kindness-repressor".  But if you start out with a human mind, then adding an activity-suppressor is a smaller alteration than deleting the whole chunk of brain.</p> <p>It's also easier for human scriptwriters to imagine themselves repressing an emotion, pushing it back, crushing it down, then it is for them to imagine once deleting an emotion and it never coming back.  The former is a mode that human minds can operate in; the latter would take neurosurgery.</p> <p>But that's just a kind of anthropomorphism previously covered - the plain old ordinary fallacy of using your brain as a black box to predict something that doesn't work like it does.  Here, I want to talk about the formally different fallacy of measuring <em>simplicity</em> in terms of the <em>shortest diff</em> from "normality", i.e., what your brain says a "mind" does in the absence of specific instruction otherwise, i.e., humanness.  Even if you can grasp that something doesn't have to work <em>just</em> like a human, thinking of it as a human+diff will distort your intuitions of simplicity - your Occam-sense.</p></div> <hr><p><i>Referenced by: </i><a href="0491.html">My Naturalistic Awakening</a> &#8226; <a href="0494.html">Above-Average AI Scientists</a> &#8226; <a href="0530.html">Economic Definition of Intelligence?</a> &#8226; <a href="0576.html">Disjunctions, Antipredictions, Etc.</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/tt/points_of_departure/">Points of Departure</a></p></body></html>