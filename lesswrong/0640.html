<html><head><title>...And Say No More Of It</title></head><body><h1>...And Say No More Of It</h1><p><i>Eliezer Yudkowsky, 09 February 2009 12:15AM</i></p><div><p><strong>Followup to</strong>:  <a href="0639.html">The Thing That I Protect</a> [http://lesswrong.com/lw/yd/the_thing_that_i_protect/]</p><p>Anything done with an ulterior motive has to be done with a pure heart.  You cannot serve your ulterior motive, without faithfully prosecuting your <em>overt </em>purpose as a thing in its own right, that has its own integrity.  If, for example, you're writing about rationality with the intention of recruiting people to your utilitarian Cause, then you cannot talk too much about your Cause, or you will fail to successfully write about rationality.</p><p>This doesn't mean that you never say <em>anything</em> about your Cause, but there's a balance to be struck.  "A fanatic is someone who can't change his mind and won't change the subject."</p><p>In previous months, I've pushed this balance too far toward talking about Singularity-related things.  And this was for (first-order) selfish reasons on my part; I was finally GETTING STUFF SAID that had been building up painfully in my brain for FRICKIN' YEARS.  And so I just kept writing, because it was finally coming <em>out</em>.  For those of you who have not the slightest interest, I'm sorry to have polluted your blog with that.</p><p>When Less Wrong starts up, it will, by my own request, impose a two-month moratorium on discussion of "<span style="text-decoration: underline;"></span><a href="http://yudkowsky.net/singularity/ai-risk">Friendly AI</a> [http://yudkowsky.net/singularity/ai-risk]" and other <a href="http://yudkowsky.net/singularity/schools">Singularity/intelligence explosion</a> [http://yudkowsky.net/singularity/schools]-related topics.</p><p>There's a number of reasons for this.  One of them is simply to restore the balance.  Another is to make sure that a forum intended to have a more general audience, doesn't narrow itself down and disappear.</p><p>But more importantly - there are certain subjects which tend to drive people crazy, <em>even if</em> there's truth behind them.  <a href="0379.html">Quantum mechanics</a> [http://lesswrong.com/lw/r5/the_quantum_physics_sequence/] would be the paradigmatic example; you don't <em>have</em> to go funny in the head but a lot of people <em>do.</em>  Likewise Godel's Theorem, consciousness, Artificial Intelligence -</p><p>The concept of "Friendly AI" can be <em>poisonous </em>in certain ways.  True or false, it carries risks to mental health.  And <em>not</em> just the obvious liabilities of praising a<span style="text-decoration: underline;"> </span><a href="0182.html">Happy Thing</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/].  Something stranger and subtler that <em>drains </em>enthusiasm.</p><a id="more"></a> <p>If there were no such problem as Friendly AI, I would probably be devoting more or less my entire life to cultivating human rationality; I would have already been doing it for years.</p><p>And though I could be mistaken - I'm guessing that I would have been <em>much further along</em> by now.</p><p>Partially, of course, because it's easier to tell people things that they're already prepared to hear.  "Rationality" doesn't command <span style="font-style: italic;"></span><em>universal </em>respect, but it commands wide respect and recognition.  There is already the New Atheist movement, and the Bayesian revolution; there are already currents flowing in that direction.</p><p>One has to be wary, in life, of substituting easy problems for hard problems.  This is a form of <a href="0505.html">running away</a> [http://lesswrong.com/lw/un/on_doing_the_impossible/].  "Life is what happens to you while you are making other plans", and it takes a very strong and non-distractable focus to avoid that...</p><p>But I'd been working on <em>directly </em>launching a Singularity movement for years, and it just wasn't getting traction.  At <a href="0011.html">some point</a> [http://lesswrong.com/lw/gx/just_lose_hope_already/] you also have to say, "This isn't working the way I'm doing it," and try something different.</p><p>There are many ulterior motives behind my participation in Overcoming Bias / Less Wrong.  One of the simpler ones is the idea of "First, produce rationalists - people who can shut up and multiply - and then, try to recruit <em>some</em> of them."  <em>Not </em>all.  You do have to care about the rationalist community for its own sake.  You have to be willing <em>not </em>to recruit all the rationalists you create.  The first rule of acting with ulterior motives is that it must be done with a pure heart, faithfully serving the overt purpose.</p><p>But more importantly - the whole thing only works if the strange intractability of the <em>direct </em>approach - the mysterious slowness of trying to build an organization <em>directly </em>around the Singularity - does not <em>contaminate</em> the new rationalist movement.</p><p>There's an old saw about the lawyer who works in a soup kitchen for an hour in order to <a href="0046.html">purchase moral satisfaction</a> [http://lesswrong.com/lw/hw/scope_insensitivity/], rather than work the same hour at the law firm and donate the money to hire 5 people to work at the soup kitchen.  Personal involvement isn't just pleasurable, it keeps people involved; the lawyer is more likely to donate real money to the soup kitchen later.  Research problems don't have a lot of opportunity for outsiders to get personally involved, including FAI research.  (This is why scientific research isn't usually supported by individuals, I suspect; instead scientists fight over the division of money that has been block-allocated by governments and foundations.  I should write about this later.)</p><p>If it were the Cause of human rationality - if that had always been the purpose I'd been pursuing - then there would have been all <em>sorts</em> of things people could have done to personally help out, to keep their spirits high and encourage them to stay involved.  Writing letters to the editor, trying to get heuristics and biases taught in organizations and in classrooms; holding events, handing out flyers; starting a magazine, increasing the number of subscribers; students handing out copies of the "<a href="http://yudkowsky.net/rational/virtues">Twelve Virtues of Rationality</a> [http://yudkowsky.net/rational/virtues]" at campus events...</p><p>It might not be too late to start going down that road - <em>but only if</em> the "Friendly AI" meme doesn't take over and suck out the life and motivation.</p><p>In a purely utilitarian sense - the sort of thinking that would lead a lawyer to actually work that extra hour at the law firm and donate the money - someone who thinks that handing out flyers is important to the Cause of human rationality, should be strictly <em>less</em> enthusiastic than someone who thinks that handing out flyers for human rationality has directly rationality-related benefits <em>and</em> might help a Friendly AI project.  It's a strictly added benefit; it should result in strictly more enthusiasm...</p><p>But in practice - it's as though the idea of "Friendly AI" exerts an attraction that sucks the emotional energy <em>out of its own subgoals.</em></p><p>You only press the "Run" button after you finish coding and teaching a Friendly AI; which happens after the theory has been worked out; which happens after theorists have been recruited; which happens after (a) mathematically smart people have comprehended cognitive naturalism on a deep gut level and (b) a regular flow of funding exists to support these professional specialists; which first requires that the whole project get sufficient traction; for which handing out flyers may be involved...</p><p>But something about the fascination of finally building the AI, seems to make all the <em>mere preliminaries</em> pale in emotional appeal.  Or maybe it's that the actual researching takes on an <a href="0366.html">aura of the sacred magisterium</a> [http://lesswrong.com/lw/qs/einsteins_superpowers/], and then it's impossible to scrape up enthusiasm for any work outside the sacred magisterium.</p><p>If you're handing out flyers for the Cause <em>of human rationality</em>... it's not about a faraway final goal that makes the mere <em>work </em>seem all too mundane by comparison, and there isn't a sacred magisterium that you're not part of.</p><p>And this is only a brief gloss on the mental health risks of "Friendly AI"; there are others I haven't even touched on, though the others are relatively more obvious.  Import <a href="0183.html">morality</a> [http://lesswrong.com/lw/lp/fake_fake_utility_functions/].crazy, import <a href="0430.html">metaethics</a> [http://lesswrong.com/lw/sk/changing_your_metaethics/].crazy, import <a href="0368.html">AI</a> [http://lesswrong.com/lw/qu/a_premature_word_on_ai/].crazy, import <a href="0189.html">Noble Cause</a> [http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/].crazy, import <a href="0182.html">Happy Agent</a> [http://lesswrong.com/lw/lo/uncritical_supercriticality/].crazy, import <a href="http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html">Futurism</a> [http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html].crazy, etcetera.</p><p>But it boils down to this:  From my perspective, my participation in Overcoming Bias / Less Wrong has many different ulterior motives, and many different helpful potentials, many potentially useful paths leading out of it.  But the <em>meme</em> of Friendly AI potentially <em>poisons </em>many of those paths, if it interacts in the wrong way; and so the ability to shut up about the Cause is more than usually important, here.  Not shut up <em>entirely </em>- but the rationality part of it needs to have its own integrity.  Part of protecting that integrity is to <em>not</em> inject comments about "Friendly AI" into any post that isn't directly about "Friendly AI".</p><p>I would like to see "Friendly AI" be <em>a</em> rationalist Cause sometimes discussed on Less Wrong, alongside other rationalist Causes whose members likewise hang out there for companionship and skill acquisition.  This is as much as is necessary to recruit a <em>fraction</em> of the rationalists created.  Anything more would poison the community, I think.  Trying to find hooks to steer every arguably-related conversation toward your own Cause is <em>not</em> virtuous, it is dangerously and destructively greedy.  <em>All</em> Causes represented on LW will have to bear this in mind, on pain of their clever conversational hooks being downvoted to oblivion.</p><p>And when Less Wrong starts up, its integrity will be protected in a simpler way: shut up about the Singularity entirely for two months.</p><p>...and that's it.</p><p>Back to rationality.</p><p>WHEW.</p><p>(This would be a great time to announce that Less Wrong is ready to go, but they're still working on it.  Possibly later this week, possibly not.)</p></div> <hr><p><i>Referenced by: </i><a href="0725.html">Less Meta</a> &#8226; <a href="0727.html">The End (of Sequences)</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ye/and_say_no_more_of_it/">...And Say No More Of It</a></p></body></html>