<html><head><title>Entropy, and Short Codes</title></head><body><h1>Entropy, and Short Codes</h1><p><i>Eliezer Yudkowsky, 23 February 2008 03:16AM</i></p><div><p><strong>Followup to</strong>:  <a href="0266.html">Where to Draw the Boundary?</a> [http://lesswrong.com/lw/o0/where_to_draw_the_boundary/]</p> <p>Suppose you have a system X that's equally likely to be in any of 8 possible states:</p> <blockquote> <p>{X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>, X<sub>4</sub>, X<sub>5</sub>, X<sub>6</sub>, X<sub>7</sub>, X<sub>8</sub>.}</p> </blockquote> <p>There's an extraordinarily ubiquitous quantity&#8212;in physics, mathematics, and even biology&#8212;called <em>entropy</em>; and the entropy of X is 3 bits.  This means that, on average, we'll have to ask 3 yes-or-no questions to find out X's value.  For example, someone could tell us X's value using this code:</p> <blockquote> <table border="0"> <tbody> <tr> <td>X<sub>1</sub>: 001</td> <td>   X<sub>2</sub>: 010</td> <td>   X<sub>3</sub>: 011</td> <td>   X<sub>4</sub>: 100</td> </tr> <tr> <td>X<sub>5</sub>: 101</td> <td>   X<sub>6</sub>: 110</td> <td>   X<sub>7</sub>: 111</td> <td>   X<sub>8</sub>: 000</td> </tr> </tbody> </table> </blockquote> <p>So if I asked "Is the first symbol 1?" and heard "yes", then asked "Is the second symbol 1?" and heard "no", then asked "Is the third symbol 1?" and heard "no", I would know that X was in state 4.</p> <p>Now suppose that the system Y has four possible states with the following probabilities:</p> <blockquote> <table border="0"> <tbody> <tr> <td>Y<sub>1</sub>: 1/2 (50%)</td> <td>    Y<sub>2</sub>: 1/4 (25%)</td> <td>    Y<sub>3</sub>: 1/8 (12.5%)</td> <td>    Y<sub>4</sub>: 1/8 (12.5%)</td> </tr> </tbody> </table> </blockquote> <p>Then the entropy of Y would be 1.75 bits, meaning that we can find out its value by asking 1.75 yes-or-no questions.</p> <p><a id="more"></a></p> <p>What does it mean to talk about asking one and three-fourths of a question?  Imagine that we designate the states of Y using the following code:</p> <blockquote> <table border="0"> <tbody> <tr> <td>Y<sub>1</sub>: 1</td> <td>    Y<sub>2</sub>: 01</td> <td>    Y<sub>3</sub>: 001</td> <td>    Y<sub>4</sub>: 000</td> </tr> </tbody> </table> </blockquote> <p>First you ask, "Is the first symbol 1?"  If the answer is "yes", you're done:  Y is in state 1.  This happens half the time, so 50% of the time, it takes 1 yes-or-no question to find out Y's state.</p> <p>Suppose that instead the answer is "No".  Then you ask, "Is the second symbol 1?"  If the answer is "yes", you're done:  Y is in state 2.  Y is in state 2 with probability 1/4, and each time Y is in state 2 we discover this fact using two yes-or-no questions, so 25% of the time it takes 2 questions to discover Y's state.</p> <p>If the answer is "No" twice in a row, you ask "Is the third symbol 1?"  If "yes", you're done and Y is in state 3; if "no", you're done and Y is in state 4.  The 1/8 of the time that Y is in state 3, it takes three questions; and the 1/8 of the time that Y is in state 4, it takes three questions.</p> <blockquote> <p>(1/2 * 1) + (1/4 * 2) + (1/8 * 3) + (1/8 * 3)<br>= 0.5 + 0.5 + 0.375 + 0.375<br>= 1.75.</p> </blockquote> <p>The general formula for the entropy of a system S is the sum, over all S<sub>i</sub>, of -p(S<sub>i</sub>)*log<sub>2</sub>(p(S<sub>i</sub>)).</p> <p>For example, the log (base 2) of 1/8 is -3.  So -(1/8 * -3) = 0.375 is the contribution of state S<sub>4</sub> to the total entropy:  1/8 of the time, we have to ask 3 questions.</p> <p>You can't always devise a perfect code for a system, but if you have to tell someone the state of arbitrarily many copies of S in a single message, you can get arbitrarily close to a perfect code.  (Google "arithmetic coding" for a simple method.)</p> <p>Now, you might ask:  "Why not use the code 10 for Y<sub>4</sub>, instead of 000?  Wouldn't that let us transmit messages more quickly?"</p> <p>But if you use the code 10 for Y<sub>4</sub><sub> </sub>, then when someone answers "Yes" to the question "Is the first symbol 1?", you won't know yet whether the system state is Y<sub>1</sub> (1) or Y<sub>4</sub> (10).  In fact, if you change the code this way, the whole system falls apart&#8212;because if you hear "1001", you don't know if it means "Y<sub>4</sub>, followed by Y<sub>2</sub>" or "Y<sub>1</sub>, followed by Y<sub>3</sub>."</p> <p>The moral is that <em>short words are a conserved resource.</em></p> <p>The key to creating a good code&#8212;a code that transmits messages as compactly as possible&#8212;is to reserve short words for things that you'll need to say frequently, and use longer words for things that you won't need to say as often.<br> <br> When you take this art to its limit, the length of the message you need to describe something, corresponds exactly or almost exactly to its probability.  This is the Minimum Description Length or Minimum Message Length formalization of <a href="0111.html">Occam's Razor</a> [http://lesswrong.com/lw/jp/occams_razor/].</p> <p>And so even the <em>labels</em> that we use for words are not quite arbitrary.  The sounds that we attach to our concepts can be better or worse, wiser or more foolish.  Even apart from considerations of <a href="0257.html">common usage</a> [http://lesswrong.com/lw/nr/the_argument_from_common_usage/]!</p> <p>I say all this, because the idea that "You can X any way you like" is a huge obstacle to learning how to X wisely.  "It's a free country; I have <a href="http://www.overcomingbias.com/2006/12/you_are_never_e.html">a right to my own opinion</a> [http://www.overcomingbias.com/2006/12/you_are_never_e.html]" obstructs the art of finding truth.  "I can define a word any way I like" obstructs the art of <a href="0266.html">carving reality at its joints</a> [http://lesswrong.com/lw/o0/where_to_draw_the_boundary/].  And even the sensible-sounding "The labels we attach to words are arbitrary" obstructs awareness of compactness.  Prosody too, for that matter&#8212;Tolkien once observed what a beautiful sound the phrase "cellar door" makes; that is the kind of awareness it takes to use language like Tolkien.</p> <p>The length of words also plays a nontrivial role in the cognitive science of language:</p> <p>Consider the phrases "recliner", "chair", and "furniture".  Recliner is a more specific category than chair; furniture is a more general category than chair.  But the vast majority of chairs have a common use&#8212;you use the same sort of motor actions to sit down in them, and you sit down in them for the same sort of purpose (to take your weight off your feet while you eat, or read, or type, or rest).  Recliners do not depart from this theme.  "Furniture", on the other hand, includes things like beds and tables which have different uses, and call up different motor functions, from chairs.</p> <p>In the terminology of cognitive psychology, "chair" is a <em>basic-level category.</em></p> <p>People have a tendency to talk, and presumably think, at the basic level of categorization&#8212;to draw the boundary around "chairs", rather than around the more specific category "recliner", or the more general category "furniture".  People are more likely to say "You can sit in that chair" than "You can sit in that recliner" or "You can sit in that furniture".</p> <p>And it is no coincidence that the word for "chair" contains fewer syllables than either "recliner" or "furniture".  Basic-level categories, in general, tend to have short names; and nouns with short names tend to refer to basic-level categories.  Not a perfect rule, of course, but a definite tendency.  Frequent use goes along with short words; short words go along with frequent use.</p> <p>Or as Douglas Hofstadter put it, there's a reason why the English language uses "the" to mean "the" and "antidisestablishmentarianism" to mean "antidisestablishmentarianism" instead of antidisestablishmentarianism other way around.</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words"><em>A Human's Guide to Words</em></a> [http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words]</p> <p style="text-align:right">Next post: "<a href="0268.html">Mutual Information, and Density in Thingspace</a> [http://lesswrong.com/lw/o2/mutual_information_and_density_in_thingspace]"</p> <p style="text-align:right">Previous post: "<a href="0266.html">Where to Draw the Boundary?</a> [http://lesswrong.com/lw/o0/where_to_draw_the_boundary/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq11.html">Sequence 11: A Human's Guide to Words</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0266.html">Where to Draw the Boundary?</a></p></td><td><p><i>Next: </i><a href="0268.html">Mutual Information, and Density in Thingspace</a></p></td></tr></table><p><i>Referenced by: </i><a href="0266.html">Where to Draw the Boundary?</a> &#8226; <a href="0268.html">Mutual Information, and Density in Thingspace</a> &#8226; <a href="0269.html">Superexponential Conceptspace, and Simple Words</a> &#8226; <a href="0271.html">The Second Law of Thermodynamics, and Engines of Cognition</a> &#8226; <a href="0274.html">Conditional Independence, and Naive Bayes</a> &#8226; <a href="0279.html">37 Ways That Words Can Be Wrong</a> &#8226; <a href="0528.html">Measuring Optimization Power</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/o1/entropy_and_short_codes/">Entropy, and Short Codes</a></p></body></html>