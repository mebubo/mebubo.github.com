<html><head><title>Not Taking Over the World</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Not Taking Over the World</h1><p><i>Eliezer Yudkowsky, 15 December 2008 10:18PM</i></p><div><p><strong>Followup to</strong>:  <a href="0579.html">What I Think, If Not Why<br></a> [http://lesswrong.com/lw/wp/what_i_think_if_not_why/]</p> <p>My esteemed co-blogger Robin Hanson<span style="text-decoration: underline;"> </span><a href="0579.html">accuses</a> [http://lesswrong.com/lw/wp/what_i_think_if_not_why/] me of <em>trying to take over the world</em>.</p> <p>Why, oh why must I be so misunderstood?</p> <p>(Well, it's not like I don't <em>enjoy </em>certain misunderstandings.  Ah, I remember the first time someone seriously and not in a joking way accused me of trying to take over the world.  On that day I felt like a true mad scientist, though I lacked a castle and hunchbacked assistant.)</p> <p>But if you're working from the premise of a <a href="0569.html">hard takeoff</a> [http://lesswrong.com/lw/wf/hard_takeoff/] - an Artificial Intelligence that self-improves at an extremely rapid rate - and you suppose such <a href="0506.html">extra-ordinary</a> [http://lesswrong.com/lw/uo/make_an_extraordinary_effort/] depth of insight and precision of craftsmanship that you can <em>actually </em>specify the AI's <a href="0459.html">goal system</a> [http://lesswrong.com/lw/td/magical_categories/] instead of <em>automatically </em>failing -</p> <p>- then it takes some <a href="http://intelligence.org/upload/CEV.html">work</a> [http://intelligence.org/upload/CEV.html] to come up with a way <em>not </em>to take over the world.<a href="http://intelligence.org/upload/CEV.html"></a> [http://intelligence.org/upload/CEV.html]</p> <p>Robin <a href="http://www.overcomingbias.com/2008/12/two-visions-of.html">talks up</a> [http://www.overcomingbias.com/2008/12/two-visions-of.html] the <a href="http://www.overcomingbias.com/2008/12/types-of-distru.html">drama</a> [http://www.overcomingbias.com/2008/12/types-of-distru.html] inherent in the <a href="http://yudkowsky.net/singularity/schools">intelligence explosion</a> [http://yudkowsky.net/singularity/schools], presumably because he feels that this is a primary source of bias.  But I've got to say that Robin's dramatic story, does <em>not </em>sound like <a href="0496.html">the story I tell of myself</a> [http://lesswrong.com/lw/ue/the_magnitude_of_his_own_folly/].  There, the drama comes from tampering with such <em>extreme </em>forces that <em>every single idea you invent is wrong</em>.  The standardized Final Apocalyptic Battle of Good Vs. Evil would be trivial by comparison; then all you have to do is put forth a <a href="0506.html">desperate effort.</a> [http://lesswrong.com/lw/uo/make_an_extraordinary_effort/]  Facing an <a href="0507.html">adult problem</a> [http://lesswrong.com/lw/up/shut_up_and_do_the_impossible/] in a <a href="0502.html">neutral universe</a> [http://lesswrong.com/lw/uk/beyond_the_reach_of_god/] isn't so straightforward.  Your enemy is yourself, who will <em>automatically </em>destroy the world, or just fail to accomplish anything, unless you can defeat you.  - That is the drama I crafted into the story I tell myself, for I too would disdain anything so <a href="0127.html">cliched</a> [http://lesswrong.com/lw/k5/cached_thoughts/] as Armageddon.</p> <p>So, Robin, I'll ask you something of a probing question.  Let's say that someone walks up to you and grants you unlimited power.</p> <p>What do you do with it, so as to <em>not </em>take over the world?</p> <p><a id="more"></a></p> <p>Do you say, "I will do nothing - I take the null action"?</p> <p>But then you have instantly become a malevolent God, as <a href="http://img216.imageshack.us/img216/820/winningathiestsqp4.jpg/">Epicurus</a> [http://img216.imageshack.us/img216/820/winningathiestsqp4.jpg/] said:</p> <p style="margin-left: 40px;">Is God willing to prevent evil, but not able?  Then he is not omnipotent.<br>Is he able, but not willing?  Then he is malevolent.<br>Is both able, and willing?  Then whence cometh evil?<br>Is he neither able nor willing?  Then why call him God.</p> <p>Peter Norvig said, "Refusing to act is like refusing to allow time to pass."  The null action is also a choice.  So have you not, in refusing to act, established all sick people as sick, established all poor people as poor, ordained all in despair to continue in despair, and condemned the dying to death?  Will you not be, until the end of time, responsible for every sin committed?</p> <p>Well, yes and no.  If someone says, "I don't trust myself not to destroy the world, therefore I take the null action," then I would tend to sigh and say, "If that is so, then you did the right thing."  Afterward, murderers will still be responsible for their murders, and altruists will still be creditable for the help they give.</p> <p>And to say that you used your power to <em>take over the world</em> by <em>doing nothing to it,</em> seems to stretch the ordinary meaning of the phrase.</p> <p>But it wouldn't be the <em>best </em>thing you could do with unlimited power, either.</p> <p>With "unlimited power" you have no need to crush your enemies.  You have no moral defense if you treat your enemies with less than the utmost consideration.</p> <p>With "unlimited power" you cannot plead the necessity of monitoring or restraining others so that they do not rebel against you.  If you do such a thing, you are simply a tyrant who enjoys power, and not a defender of the people.</p> <p>Unlimited power removes a lot of moral defenses, really.  You can't say "But I had to."  You can't say "Well, I wanted to help, but I couldn't."  The only excuse for not helping is if you <em>shouldn't,</em> which is harder to establish.</p> <p>And let us also suppose that this power is wieldable without side effects or configuration constraints; it is wielded with <em>unlimited precision</em>.</p> <p>For example, you can't take refuge in saying anything like:  "Well, I built this AI, but <a href="0397.html">any intelligence</a> [http://lesswrong.com/lw/rn/no_universally_compelling_arguments/] will pursue its own interests, so now the AI will just be a Ricardian trading partner with humanity as it pursues its own goals."  Say, the programming team has cracked the "hard problem of conscious experience" in sufficient depth that they can <em>guarantee </em>that the AI they create is <em>not sentient</em> - not a repository of pleasure, or pain, or subjective experience, or any interest-in-self - and hence, the AI is only a means to an end, and not an end in itself.</p> <p>And you cannot take refuge in saying, "In invoking this power, the reins of destiny have passed out of my hands, and humanity has passed on the torch."  Sorry, you haven't created a new person yet - not unless you <em>deliberately </em>invoke the unlimited power to do so - and then you can't take refuge in the <em>necessity</em> of it as a side effect; you must establish that it is the right thing to do.</p> <p>The AI is not <em>necessarily</em> a trading partner.  You could make it a nonsentient device that just gave you things, <em>if </em>you thought that were wiser.</p> <p>You cannot say, "The law, in protecting the rights of all, must necessarily protect the right of Fred the Deranged to spend all day giving himself electrical shocks."  The power is wielded with unlimited precision; you <em>could,</em> if you wished, protect the rights of everyone except Fred.</p> <p>You cannot take refuge in the <em>necessity</em> of anything - that is the meaning of unlimited power.</p> <p>We will even suppose (for it removes yet more excuses, and hence reveals more of your morality) that you are not limited by the laws of physics as we know them.  You are bound to deal only in finite numbers, but not otherwise bounded.  This is so that we can see the true constraints of your morality, apart from your being able to plead constraint by the environment.</p> <p>In my <a href="0480.html">reckless youth</a> [http://lesswrong.com/lw/ty/my_childhood_death_spiral/], I used to think that it might be a good idea to flash-upgrade to the highest possible level of intelligence you could manage on available hardware.  <a href="0480.html">Being smart was good</a> [http://lesswrong.com/lw/ty/my_childhood_death_spiral/], so being smarter was better, and being as smart as possible as quickly as possible was best - right?</p> <p>But when I imagined having <em>infinite</em> computing power available, I realized that no matter how large a mind you made yourself, you could just go on making yourself larger and larger and larger.  So that wasn't an answer to the purpose of life.  And only then did it occur to me to ask after <em>eudaimonic rates of intelligence increase,</em> rather than just assuming you wanted to immediately be as smart as possible.</p> <p>Considering the infinite case moved me to change the way I considered the finite case.  Before, I was <em>running away from the question</em> by saying "More!"  But considering an <em>unlimited</em> amount of ice cream forced me to confront the issue of what to <em>do</em> with <em>any </em>of it.</p> <p>Similarly with population:  If you invoke the unlimited power to create a quadrillion people, then why not a quintillion?  If 3^^^3, why not <a href="0135.html">3^^^^3</a> [http://lesswrong.com/lw/kd/pascals_mugging_tiny_probabilities_of_vast/]?  So you can't take refuge in saying, "I will create more people - that is the difficult thing, and to accomplish it is the main challenge."  What is <em>individually</em> a life worth living?</p> <p>You can say, "It's not my place to decide; I leave it up to others" but then you are responsible for the consequences of that decision as well.  You should say, at least, how this differs from the null act.</p> <p>So, Robin, reveal to us your character:  What would you do with <em>unlimited </em>power?</p></div> <hr><p><i>Referenced by: </i><a href="0584.html">Visualizing Eutopia</a> &#8226; <a href="0597.html">Can't Unbirth a Child</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/wt/not_taking_over_the_world/">Not Taking Over the World</a></p></body></html>