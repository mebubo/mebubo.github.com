<html><head><title>Free to Optimize</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Free to Optimize</h1><p><i>Eliezer Yudkowsky, 02 January 2009 01:41AM</i></p><div><p><em>Stare decisis</em> is the legal principle which binds courts to follow precedent, retrace the footsteps of other judges' decisions.  As someone previously condemned to an Orthodox Jewish education, where I <a href="0022.html">gritted my teeth at the idea that medieval rabbis would always be wiser than modern rabbis</a> [http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/], I <em>completely missed</em> the rationale for <em>stare decisis</em>.  I thought it was about respect for the past.</p> <p>But shouldn't we presume that, in the presence of science, judges closer to the future will know more&#8212;have new facts at their fingertips&#8212;which enable them to make better decisions?  Imagine if engineers respected the decisions of past engineers, not as a source of good suggestions, but as a binding precedent!&#8212;That was my original reaction.  The standard rationale behind <em>stare decisis</em> came as a shock of revelation to me; it considerably increased my respect for the whole legal system.</p> <p>This rationale is <em>jurisprudence constante:</em>  The legal system must above all be <em>predictable,</em> so that people can execute contracts or choose behaviors knowing the legal implications.</p> <p>Judges are not necessarily there to <em>optimize,</em> like an engineer.  The purpose of law is not to make the world perfect.  The law is there to provide a <em>predictable environment</em> in which people can optimize their <em>own</em>futures.</p> <p>I was amazed at how a principle that at first glance seemed so completely Luddite, could have such an Enlightenment rationale.  It was a "<a href="0540.html">shock of creativity</a> [http://lesswrong.com/lw/vm/lawful_creativity/]"&#8212;a solution that ranked high in my preference ordering and low in my search ordering, a solution that violated my previous surface generalizations.  "Respect the past <em>just </em>because it's the past" would not have easily occurred to me as a good solution for <em>anything</em>.</p> <p>There's a <a href="http://books.google.com/books?id=inmTyPPdR5oC&amp;pg=RA1-PA123&amp;lpg=RA1-PA123&amp;dq=%22caretaking+organisms%22&amp;source=bl&amp;ots=JynSbtNF4H&amp;sig=k-2NzLYV3jwoB8bNF8JM2HdNB2Q&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result#PRA1-PA123,M1">peer commentary</a> [http://books.google.com/books?id=inmTyPPdR5oC&amp;pg=RA1-PA123&amp;lpg=RA1-PA123&amp;dq=%22caretaking+organisms%22&amp;source=bl&amp;ots=JynSbtNF4H&amp;sig=k-2NzLYV3jwoB8bNF8JM2HdNB2Q&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result#PRA1-PA123,M1] in <em>Evolutionary Origins of Morality</em> which notes in passing that "other things being equal, organisms will choose to reward themselves over being rewarded by caretaking organisms".  It's cited as the Premack principle, but the <a href="http://en.wikipedia.org/wiki/Premack's_principle">actual Premack principle</a> [http://en.wikipedia.org/wiki/Premack's_principle] looks to be something quite different, so I don't know if this is a bogus result, a misremembered citation, or a nonobvious derivation.  If true, it's definitely interesting from a <a href="0585.html">fun-theoretic</a> [http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/] perspective.</p> <p>Optimization is <a href="0529.html">the ability to squeeze the future</a> [http://lesswrong.com/lw/vb/efficient_crossdomain_optimization/] into <a href="0527.html">regions high in your preference ordering</a> [http://lesswrong.com/lw/v9/aiming_at_the_target/].  <a href="0589.html">Living by my own strength</a> [http://lesswrong.com/lw/wz/living_by_your_own_strength/], means squeezing my own future&#8212;not perfectly, but still being able to grasp <em>some</em> of the relation between my actions and their consequences.  This is <a href="0532.html">the strength of a human</a> [http://lesswrong.com/lw/ve/mundane_magic/].</p> <p>If I'm being <em>helped,</em> then some other agent is also squeezing my future&#8212;optimizing me&#8212;in the same rough direction that I try to squeeze myself.  This is "help".</p> <p>A human helper is unlikely to steer every part of my future that I could have steered myself.  They're not likely to have already exploited every connection between action and outcome that I can myself understand.  They won't be able to squeeze the future <em>that tightly;</em> there will be slack left over, that I can squeeze for myself.</p> <p>We have little experience with being "caretaken" across any <a href="0359.html">substantial gap in intelligence</a> [http://lesswrong.com/lw/ql/my_childhood_role_model/]; the closest thing that human experience provides us with is the idiom of parents and children.  Human parents are still human; they may be smart<em>er</em> than their children, but they can't predict the future or manipulate the kids in any fine-grained way.</p> <p>Even so, it's an empirical observation that some human parents <em>do</em>help their children so much that their children don't become strong.  It's not that there's <em>nothing</em> left for their children to do, but with a hundred million dollars in a trust fund, they don't <em>need</em> to do much&#8212;their remaining motivations aren't strong enough.  Something like that depends on genes, not just environment &#8212;not every overhelped child shrivels&#8212;but conversely it depends on environment too, not just genes.</p> <p>So, in considering the kind of "help" that can flow from relatively stronger agents to relatively weaker agents, we have two potential problems to track:</p> <ol> <li>Help so strong that it optimizes away the links between the desirable outcome and your own choices.</li> <li>Help that is <em>believed</em>to be so reliable, that it takes off the psychological pressure to use your own strength.</li> </ol> <p>Since (2) revolves around <em>belief</em>, could you just lie about how reliable the help was?  Pretend that you're not going to help when things get bad&#8212;but then if things do get bad, you help anyway?  That trick didn't work too well for Alan Greenspan and Ben Bernanke.</p> <p>A superintelligence might be able to pull off a better deception.  But in terms of moral theory and eudaimonia&#8212;we <em>are</em>allowed to have <a href="0169.html">preferences over external states of affairs, not just psychological states</a> [http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/].  This applies to "I want to <em>really</em> steer my own life, not just believe that I do", just as it applies to "I want to have a love affair with a fellow sentient, not just a puppet that I am deceived into thinking sentient".  So if we can state firmly from a value standpoint that we don't want to be fooled this way, then <em>building</em>an agent which respects that preference is a mere matter of Friendly AI.</p> <p>Modify people so that they <em>don't</em> relax when they believe they'll be helped?  I usually try to think of how to modify environments before I imagine modifying any people.  It's not that I want to stay the same person forever; but the issues are rather more fraught, and one might wish to take it slowly, at some eudaimonic rate of personal improvement.</p> <p>(1), though, is the most interesting issue from a philosophicalish standpoint.  It impinges on <a href="0386.html">the confusion named "free will"</a> [http://lesswrong.com/lw/rc/the_ultimate_source/].  Of which I have already untangled; see the posts referenced at top, if you're recently joining <em>OB</em>.</p> <p>Let's say that I'm an ultrapowerful AI, and I use my knowledge of your mind and your environment to forecast that, if left to your own devices, you will make $999,750.  But this does not satisfice me; it so happens that I want you to make at least $1,000,000.  So I hand you $250, and then you go on to make $999,750 as you ordinarily would have.</p> <p>How much of your own strength have you just lived by?</p> <p>The first view would say, "I made 99.975% of the money; the AI only helped 0.025% worth."</p> <p>The second view would say, "Suppose I had entirely slacked off and done nothing.  Then the AI would have handed me $1,000,000.  So my attempt to <em>steer my own future</em> was an illusion; my future was already determined to contain $1,000,000."</p> <p>Someone might reply, "Physics is deterministic, so your future is already determined no matter what you or the AI does&#8212;"</p> <p>But the second view interrupts and says, "No, you're not confusing me that easily.  <a href="0374.html">I am within physics</a> [http://lesswrong.com/lw/r0/thou_art_physics/], so in order for my future to be determined by me, it must be determined by physics.  The Past does not reach around the Present and determine the Future before the Present gets a chance&#8212;that is <a href="0375.html">mixing up a timeful view with a timeless one</a> [http://lesswrong.com/lw/r1/timeless_control/].  But if there's an AI that really <em>does</em> look over the alternatives before I do, and really <em>does</em> choose the outcome before I get a chance, then I'm really <em>not</em> steering my own future.  The future is no longer <em>counterfactually dependent</em> on my decisions."</p> <p>At which point the first view butts in and says, "But of course the future is counterfactually dependent on your actions.  The AI gives you $250 and then leaves.  As a physical fact, if you didn't work hard, you would end up with only $250 instead of $1,000,000."</p> <p>To which the second view replies, "I <a href="0242.html">one-box on Newcomb's Problem</a> [http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/], so my counterfactual reads 'if my decision were to not work hard, the AI would have given me $1,000,000 instead of $250'."</p> <p>"So you're saying," says the first view, heavy with sarcasm, "that if the AI had wanted me to make at least $1,000,000 and it had ensured this through the general policy of handing me $1,000,000 flat on a silver platter, leaving me to earn $999,750 through my own actions, for a total of $1,999,750&#8212;that this AI would have interfered <em>less</em>with my life than the one who just gave me $250."</p> <p>The second view thinks for a second and says "Yeah, actually.  Because then there's a stronger counterfactual dependency of the final outcome on your own decisions.  Every dollar you earned was a real added dollar.  The second AI helped you more, but it constrained your destiny less."</p> <p>"But if the AI had done exactly the same thing, because it <em>wanted</em>me to make exactly $1,999,750&#8212;"</p> <p>The second view nods.</p> <p>"That sounds a bit scary," the first view says, "for reasons which have nothing to do with the usual furious debates over Newcomb's Problem.  You're making your utility function path-dependent on the detailed cognition of the Friendly AI trying to help you!  You'd be okay with it if the AI only <em>could</em> give you $250.  You'd be okay if the AI had decided to give you $250 through a decision process that had <em>predicted the final outcome in less detail,</em> even though you acknowledge that in principle your decisions may already be highly deterministic.  How is a poor Friendly AI supposed to help you, when your utility function is dependent, not just on the outcome, not just on the Friendly AI's actions, but dependent on <em>differences of the exact algorithm</em> the Friendly AI uses to arrive at <em>the same decision?</em>  Isn't your whole rationale of one-boxing on Newcomb's Problem that you only care about what works?"</p> <p>"Well, that's a good point," says the second view.  "But <a href="0586.html">sometimes we only care about what works, and yet sometimes we <em>do</em> care about the journey as well as the destination</a> [http://lesswrong.com/lw/ww/high_challenge/].  If I was trying to cure cancer, I wouldn't care how I cured cancer, or whether I or the AI cured cancer, just so long as it ended up cured.  This <em>isn't</em> that kind of problem.  This is the problem of the eudaimonic journey&#8212;it's the reason I care in the first place whether I get a million dollars through my own efforts or by having an outside AI hand it to me on a silver platter.  <a href="0242.html">My utility function is not up for grabs</a> [http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/].  If I desire not to be optimized too hard by an outside agent, the agent needs to respect that preference even if it depends on the details of how the outside agent arrives at its decisions.  Though it's also worth noting that decisions <em>are</em>produced by algorithms&#8212; if the AI <em>hadn't</em> been using the algorithm of doing just what it took to bring me up to $1,000,000, it probably <em>wouldn't</em> have handed me exactly $250."</p> <p>The desire <em>not to be optimized too hard by an outside agent</em> is one of the structurally nontrivial aspects of human morality.</p> <p>But I can think of <em>a</em> solution, which unless it contains some terrible flaw not obvious to me, sets a lower bound on the goodness of a solution: any alternative solution adopted, ought to be at least this good or better.</p> <p>If there is anything in the world that resembles a god, people will try to <a href="http://miscellanea.wellingtongrey.net/2008/12/01/prayer-vs-hard-work/">pray</a> [http://miscellanea.wellingtongrey.net/2008/12/01/prayer-vs-hard-work/] to it.  It's human nature to such an extent that people will pray even if there aren't any gods&#8212;so you can imagine what would happen if there were!  But people don't pray to gravity to ignore their airplanes, because it is understood how gravity works, and it is understood that gravity doesn't adapt itself to the needs of individuals.  Instead they understand gravity and try to turn it to their own purposes.</p> <p>So one possible way of helping&#8212;which may or may not be the best way of helping&#8212;would be the gift of a world that works on <em>improved rules,</em> where the rules are stable and understandable enough that people can manipulate them and optimize their own futures together.  A nicer place to live, but free of meddling gods beyond that.  I have yet to think of a form of help that is less poisonous to human beings&#8212;but I am only human.</p> <p><strong>Added:</strong>  Note that <em>modern</em> legal systems score a low Fail on this dimension&#8212;no single human mind can even <em>know</em> all the regulations any more, let alone optimize for them.  Maybe a professional lawyer who did nothing else could memorize all the regulations applicable to them personally, but I doubt it.  As Albert Einstein observed, any fool can make things more complicated; what takes intelligence is moving in the opposite direction.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0592.html">Harmful Options</a> [http://lesswrong.com/lw/x2/harmful_options/]"</p> <p style="text-align:right">Previous post: "<a href="0589.html">Living By Your Own Strength</a> [http://lesswrong.com/lw/wz/living_by_your_own_strength/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0589.html">Living By Your Own Strength</a></p></td><td><p><i>Next: </i><a href="0592.html">Harmful Options</a></p></td></tr></table><p><i>Referenced by: </i><a href="0589.html">Living By Your Own Strength</a> &#8226; <a href="0592.html">Harmful Options</a> &#8226; <a href="0619.html">Interpersonal Entanglement</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0626.html">31 Laws of Fun</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/xb/free_to_optimize/">Free to Optimize</a></p></body></html>