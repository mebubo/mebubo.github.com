<html><head><title>The Thing That I Protect</title></head><body><h1>The Thing That I Protect</h1><p><i>Eliezer Yudkowsky, 07 February 2009 07:18PM</i></p><div><p><strong>Followup to</strong>:  <a href="0241.html">Something to Protect</a> [http://lesswrong.com/lw/nb/something_to_protect/], <a href="0629.html">Value is Fragile</a> [http://lesswrong.com/lw/y3/value_is_fragile/]</p><p>"<a href="0241.html">Something to Protect</a> [http://lesswrong.com/lw/nb/something_to_protect/]" discursed on the idea of wielding rationality in the service of something other than "rationality".  Not just that rationalists ought to pick out a Noble Cause as a hobby to keep them busy; but rather, that rationality itself is generated by having something that you care about more than your current ritual of cognition.</p><p>So what is it, then, that <em>I</em> protect?</p><p>I quite deliberately did not discuss that in "<a href="0241.html">Something to Protect</a> [http://lesswrong.com/lw/nb/something_to_protect/]", leaving it only as a hanging implication.  In the unlikely event that we ever run into aliens, I don't expect their version of Bayes's Theorem to be mathematically different from ours, even if they generated it in the course of protecting different and incompatible values.  Among humans, the idiom of having "something to protect" is not bound to any one cause, and therefore, to mention my own cause in that post would have harmed its integrity.  Causes are dangerous things, whatever their true importance; I have <a href="0181.html">written</a> [http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/] <a href="0189.html">somewhat</a> [http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/] on this, and will write more about it.</p><p>But still - what is it, then, the thing that I protect?</p><p>Friendly AI?  No - a thousand times no - a thousand times <a href="0480.html">not anymore</a> [http://lesswrong.com/lw/ty/my_childhood_death_spiral/].  It's not thinking <em>of the AI</em> that gives me strength to <a href="0628.html">carry on even in the face of inconvenience</a> [http://lesswrong.com/lw/y2/rationality_quotes_25/]. </p><a id="more"></a><p>I would be a strange and dangerous AI wannabe if <em>that</em> were my cause - the image in my mind of a perfected being, an existence greater than humankind.  Maybe someday I'll be able to imagine such a child and try to build one, but for now <a href="0597.html">I'm too young to be a father</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/].</p><p>Those of you who've been following along recent discussions, particularly "<a href="0629.html">Value is Fragile</a> [http://lesswrong.com/lw/y3/value_is_fragile/]", might have noticed something else that I might, perhaps, hold precious.  Smart agents want to protect the physical representation of their utility function for almost the same reason that male organisms are built to be protective of their testicles.  From the standpoint of the <a href="0149.html">alien god</a> [http://lesswrong.com/lw/kr/an_alien_god/], natural selection, losing the germline - the gene-carrier that propagates the pattern into the next generation - means losing almost everything that <em>natural selection</em> cares about.  Unless you already have children to protect, can protect relatives, etcetera - few are the absolute and unqualified statements that can be made in evolutionary biology - but still, if you happen to be a male human, you will find yourself rather protective of your testicles; that one, centralized vulnerability is why a kick in the testicles hurts more than being hit on the head.</p><p>To lose the <em>pattern </em>of human value - which, for now, is physically embodied <em>only </em>in the human brains that care about those values - would be to lose the Future itself; <span style="text-decoration: underline;">if there's no agent with those values, there's nothing to</span><a href="0629.html"> shape a valuable Future</a> [http://lesswrong.com/lw/y3/value_is_fragile/].</p><p>And this pattern, this one most vulnerable and precious pattern, is indeed at risk to be distorted or destroyed.&#160;&#160;<a href="0603.html">Growing up is a hard problem either way</a> [http://lesswrong.com/lw/xd/growing_up_is_hard/], whether you try to <a href="0604.html">edit existing brains</a> [http://lesswrong.com/lw/xe/changing_emotions/], or build <em>de novo</em> Artificial Intelligence that <a href="0457.html">mirrors</a> [http://lesswrong.com/lw/tb/mirrors_and_paintings/] human values.  If something more powerful than humans, and not sharing human values, comes into existence - whether by de novo AI gone wrong, or augmented humans gone wrong - then we can expect to <a href="0358.html">lose</a> [http://lesswrong.com/lw/qk/that_alien_message/], <a href="0569.html">hard</a> [http://lesswrong.com/lw/wf/hard_takeoff/].  And value is fragile; <a href="0629.html">losing just one dimension of human value can destroy nearly all of the utility we expect from the future</a> [http://lesswrong.com/lw/y3/value_is_fragile/].</p><p>So is <em>that</em>, then, the thing that I protect?</p><p>If it were - then what inspired me when times got tough would be, say, thinking of people being nice to each other.  Or thinking of people laughing, and contemplating how humor probably exists among only an infinitesimal fraction of evolved intelligent species and their descendants.  I would marvel at the power of sympathy to make us feel what others feel -</p><p>But that's not quite it either.</p><p>I once attended a small gathering whose theme was "This I Believe".  You could interpret that phrase in a number of ways; I chose "What do you believe that most other people don't believe which makes a corresponding difference in your behavior?"  And it seemed to me that most of how I behaved differently from other people boiled down to two unusual beliefs.  The first belief could be summarized as "<a href="0540.html">intelligence is a manifestation of order rather than chaos</a> [http://lesswrong.com/lw/vm/lawful_creativity/]"; this accounts both for my attempts to master rationality, and my attempt to wield the power of AI.</p><p>And the second unusual belief could be summarized as:  "Humanity's future can be a WHOLE LOT better than its past."</p><p>Not desperately darwinian robots surging out to eat as much of the cosmos as possible, mostly ignoring their own internal values to try and grab as many stars as possible, with most of the remaining matter going into making paperclips.</p><p>Not some bittersweet ending where you and I fade away on Earth while the inscrutable robots ride off into the unknowable sunset, having grown beyond such merely human values as love or sympathy.</p><p><em>Screw</em> bittersweet.  To <em>hell </em>with that melancholy-tinged crap.  Why leave <em>anyone </em>behind?  Why surrender a single thing that's precious?</p><p>(And the compromise-futures are all fake anyway; at this difficulty level, <a href="0503.html">you steer precisely or you crash</a> [http://lesswrong.com/lw/ul/my_bayesian_enlightenment/].)</p><p><a href="0626.html">The pattern of fun is also lawful</a> [http://lesswrong.com/lw/y0/31_laws_of_fun/].  And, though I do not know all the law - I do think that written in humanity's value-patterns is the implicit potential of a <em>happy </em>future.  A seriously goddamn FUN future.  A genuinely GOOD outcome.  <em>Not </em>something you'd accept with a sigh of resignation for nothing better being possible.  Something that would make you go "WOOHOO!"</p><p>In the <a href="0624.html">sequence on Fun Theory</a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/], I have given you, I hope, some small reason to believe that such a possibility might be consistently describable, if only it could be made real.  How to read that potential out of humans and project it into reality... might or might not be as simple as "<a href="0457.html">superpose our extrapolated reflected equilibria</a> [http://lesswrong.com/lw/tb/mirrors_and_paintings/]".  But that's one way of looking at what I'm <em>trying</em> to do - to reach the potential of the GOOD outcome, not the melancholy bittersweet compromise.  Why settle for less?</p><p>To really have something to protect, it has to be able to bring tears to your eyes.  That, generally, requires something concrete to visualize - not just abstract laws.  Reading the Laws of Fun doesn't bring tears to my eyes.  I can visualize a possibility or two that makes sense to me, but I don't know if it would make sense to others the same way.</p><p>What does bring tears to my eyes?  Imagining a future where humanity has its act together.  Imagining children who grow up never knowing our world, who don't even understand it.  Imagining the rescue of those now in sorrow, the end of nightmares great and small.  Seeing in reality the real sorrows that happen now, so many of which are <a href="0580.html">unnecessary</a> [http://lesswrong.com/lw/wq/you_only_live_twice/] even now.  Seeing in reality the signs of progress toward a humanity that's at least <em>trying </em>to get its act together and become something more - even if the signs are mostly just symbolic: a space shuttle launch, a march that protests a war.</p><p>(And of course these are not the only things that move me.  Not everything that moves me has to be a Cause.  When I'm listening to e.g. Bach's <em>Jesu: Joy of Man's Desiring,</em> I don't think about how every extant copy might be vaporized if things go wrong.  That may be true, but it's not the point.  It would be as bad as refusing to listen to that melody because it was once inspired by belief in the supernatural.)</p><p>To really have something to protect, you have to be able to <em>protect </em>it, not just <em>value </em>it.  My battleground for that better Future is, indeed, the fragile pattern of value.  Not to keep it in stasis, but to keep it <a href="0540.html">improving under its own criteria rather than randomly losing information</a> [http://lesswrong.com/lw/vm/lawful_creativity/].  And then to project that through more powerful <a href="0394.html">optimization</a> [http://lesswrong.com/lw/rk/optimization_and_the_singularity/], to materialize the valuable future.  Without surrendering a single thing that's precious, because <a href="0629.html">losing a single dimension of value could lose it all</a> [http://lesswrong.com/lw/y3/value_is_fragile/].</p><p>There's no easy way to do this, whether by <em>de novo</em> AI or by editing brains.  But with a de novo AI, cleanly and correctly designed, I think it should at least be <em>possible</em> to get it truly right and win completely.  It seems, for all its danger, the safest and easiest and shortest way (yes, the alternatives really are <em>that bad</em>).  And so that is my project.</p><p>That, then, is the service in which I wield rationality.  To protect the Future, on the battleground of the physical representation of value.  And my weapon, if I can master it, is the ultimate hidden technique of Bayescraft - to explicitly and fully know the structure of rationality, to such an extent that you can shape the pure form outside yourself - what some call "Artificial General Intelligence" and I call "Friendly AI".  Which is, itself, a major unsolved research problem, and so it calls into play the more informal methods of merely human rationality.  That is the purpose of my art and the wellspring of my art.</p><p>That's pretty much all I wanted to say here about this Singularity business...</p><p>...except for one last thing; so after tomorrow, I plan to go back to posting about plain old rationality on Monday.</p><p></p><p></p></div> <hr><p><i>Referenced by: </i><a href="0640.html">...And Say No More Of It</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/yd/the_thing_that_i_protect/">The Thing That I Protect</a></p></body></html>