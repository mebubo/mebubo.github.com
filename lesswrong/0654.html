<html><head><title>Fairness vs. Goodness</title></head><body><h1>Fairness vs. Goodness</h1><p><i>Eliezer Yudkowsky, 22 February 2009 08:22PM</i></p><div><p>It seems that back when the Prisoner's Dilemma was still being worked out, Merrill Flood and Melvin Drescher tried <a href="http://econ161.berkeley.edu/Economists/prisoners_dilemma.html">a 100-fold iterative PD on two smart but unprepared subjects</a> [http://econ161.berkeley.edu/Economists/prisoners_dilemma.html], Armen Alchian of UCLA and John D. Williams of RAND.</p><p>The kicker being that the payoff matrix was <em>asymmetrical,</em> with dual cooperation awarding JW twice as many points as AA:</p> <table cellpadding="5" cellspacing="2" align="center" border="1"><tbody><tr><td>(AA, JW)</td><td>JW: D</td><td>JW: C</td></tr><tr><td>AA: D</td><td><span style="color: #c00000; font-family: Trebuchet MS;"><strong>(0, 0.5)</strong></span></td><td>(1, -1)</td></tr><tr><td>AA: C</td><td>(-1, 2)</td><td><span style="color: #407f00; font-family: Trebuchet MS;"><strong>(0.5, 1)</strong></span></td></tr></tbody></table><p>The resulting 100 iterations, with a log of comments written by both players, make for <a href="http://econ161.berkeley.edu/Economists/prisoners_dilemma.html">fascinating reading</a> [http://econ161.berkeley.edu/Economists/prisoners_dilemma.html].</p><p>JW spots the possibilities of cooperation right away, while AA is slower to catch on.</p><p>But once AA does catch on to the possibilities of cooperation, AA goes on throwing in an occasional D... because AA thinks the natural meeting point for cooperation is a <em>fair</em> outcome, where both players get around the same number of total points.</p><p>JW goes on trying to enforce (C, C) - the option that maximizes total utility for both players - by punishing AA's attempts at defection.  JW's log shows comments like "He's crazy.  I'll teach him the hard way."</p><p>Meanwhile, AA's log shows comments such as "He won't share.  He'll punish me for trying!" </p><a id="more"></a><p>I confess that my own sympathies lie with JW, and I don't <em>think </em>I would have played AA's game in AA's shoes.  This would seem to indicate that I'm more of a utilitarian than a fair-i-tarian.  Life doesn't always hand you <em>fair</em> games, and the best we can do for each other is play them positive-sum.</p><p>Though I might have been <em>somewhat </em>more sympathetic to AA, if the (C, C) outcome had actually <em>lost</em> him points, and only (D, C) had made it possible for him to gain them back.  For example, this is also a Prisoner's Dilemma:</p><p></p><table cellpadding="5" cellspacing="2" align="center" border="1"><tbody><tr><td>(AA, JW)</td><td>JW: D</td><td>JW: C</td></tr><tr><td>AA: D</td><td><span style="color: #c00000; font-family: Trebuchet MS;"><strong>(-2, 2)</strong></span></td><td>(2, 0)</td></tr><tr><td>AA: C</td><td>(-5, 6)</td><td><span style="color: #407f00; font-family: Trebuchet MS;"><strong>(-1, 4)</strong></span></td></tr></tbody></table><p>Theoretically, of course, utility functions are invariant up to affine transformation, so a utility's absolute sign is not meaningful.  But this is not always a good metaphor for real life.</p><p>Of course what <em>we </em>want in this case, societally speaking, is for JW to slip AA a bribe under the table.  That way we can maximize social utility while letting AA go on making a profit.  But if AA starts out with a negative number in (C, C), how much do we want AA to demand in bribes - from our global, societal perspective?</p><p>The whole affair makes for an interesting reminder of the different worldviews that people invent for themselves - seeming so natural and <em>uniquely obvious</em> from the inside - to make themselves the heroes of their own stories.</p></div> <hr><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ys/fairness_vs_goodness/">Fairness vs. Goodness</a></p></body></html>