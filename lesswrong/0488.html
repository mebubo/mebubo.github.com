<html><head><title>Horrible LHC Inconsistency</title></head><body><h1>Horrible LHC Inconsistency</h1><p><i>Eliezer Yudkowsky, 22 September 2008 03:12AM</i></p><div><p><strong>Followup to</strong>: <a href="0426.html">When (Not) To Use Probabilities</a> [http://lesswrong.com/lw/sg/when_not_to_use_probabilities/], <a href="0487.html">How Many LHC Failures Is Too Many?</a> [http://lesswrong.com/lw/u5/how_many_lhc_failures_is_too_many/]</p> <p>While trying to answer my own question on "How Many LHC Failures Is Too Many?" I realized that I'm <em>horrendously</em> inconsistent with respect to my stated beliefs about disaster risks from the Large Hadron Collider.</p> <p>First, I thought that stating a "one-in-a-million" probability for the Large Hadron Collider destroying the world was too high, in the sense that I would much rather run the Large Hadron Collider than press a button with a known 1/1,000,000 probability of destroying the world.</p> <p>But if you asked me<a href="0426.html"> whether I could make one million statements</a> [http://lesswrong.com/lw/sg/when_not_to_use_probabilities/] of authority equal to "The Large Hadron Collider will not destroy the world", and be wrong, on average, around once, then I would have to say no.</p> <p><a href="0426.html">Unknown</a> [http://lesswrong.com/lw/sg/when_not_to_use_probabilities/] pointed out that this turns me into a money pump.  Given a portfolio of a million existential risks to which I had assigned a "less than one in a million probability", I would rather press the button on the fixed-probability device than run a random risk from this portfolio; but would rather take any particular risk in this portfolio than press the button.</p> <p>Then, I considered the question of <a href="0487.html">how many mysterious failures at the LHC</a> [http://lesswrong.com/lw/u5/how_many_lhc_failures_is_too_many/] it would take to make me question whether it might destroy the world/universe somehow, and what this revealed about my prior probability.</p> <p>If the failure probability had a known 50% probability of occurring from natural causes, like a quantum coin or some such... then I suspect that if I <em>actually</em> saw that coin come up heads 20 times in a row, I would feel a strong impulse to bet on it coming up heads the next time around.  (And that's taking into account my uncertainty about whether the anthropic principle really works that way.)</p> <p>Even having noticed this triple inconsistency, I'm not sure in which direction to resolve it!</p> <p>(But I still maintain my resolve that the LHC is not worth expending political capital, financial capital, or our time to <a href="0424.html">shut down</a> [http://lesswrong.com/lw/se/should_we_ban_physics/]; compared with using the same capital to worry about superhuman intelligence or nanotechnology.)</p></div> <hr><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/u6/horrible_lhc_inconsistency/">Horrible LHC Inconsistency</a></p></body></html>