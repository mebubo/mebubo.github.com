<html><head><title>Moore's Paradox</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Moore's Paradox</h1><p><i>Eliezer Yudkowsky, 08 March 2009 02:27AM</i></p><div><p><strong>Followup to</strong>:  <a href="0667.html">Belief in Self-Deception</a> [http://lesswrong.com/lw/s/belief_in_selfdeception/]</p> <p><a href="http://en.wikipedia.org/wiki/Moore%27s_paradox">Moore's Paradox</a> [http://en.wikipedia.org/wiki/Moore%27s_paradox] is the standard term for saying "It's raining outside but I don't believe that it is."  HT to <a href="http://www.metafilter.com/79752/More-Right-was-too-political#2477702">painquale on MetaFilter.</a> [http://www.metafilter.com/79752/More-Right-was-too-political#2477702]</p> <p>I think I understand Moore's Paradox a bit better now, after reading some of the comments on Less Wrong.  <a href="0666.html">Jimrandomh</a> [http://lesswrong.com/lw/r/no_really_ive_deceived_myself/#ga] suggests:</p> <blockquote> <p>Many people cannot distinguish between levels of indirection. To them, "I believe X" and "X" are the same thing, and therefore, reasons why it is beneficial to believe X are also reasons why X is true.</p> </blockquote> <p>I don't think this is correct&#8212;relatively young children can understand the concept of having a false belief, which requires separate mental buckets for the map and the territory.  But it points in the direction of a similar idea:</p> <p>Many people may not consciously distinguish between <em>believing </em>something and <em>endorsing</em> it.</p> <p>After all&#8212;"I believe in democracy" means, colloquially, that you endorse the concept of democracy, not that you believe democracy exists.  The word "belief", then, has more than one meaning.  We could be looking at a <a href="0262.html">confused word</a> [http://lesswrong.com/lw/nw/fallacies_of_compression/] that causes confused thinking (or maybe it just reflects pre-existing confusion).</p> <p>So: in the <a href="0667.html">original example</a> [http://lesswrong.com/lw/s/belief_in_selfdeception/], "I believe people are nicer than they are", she came up with some reasons why it would be good to believe people are nice&#8212;health benefits and such&#8212;and since she now had some warm affect on "believing people are nice", she introspected on this warm affect and concluded, "I believe people are nice".  That is, she mistook the <em>positive affect</em> attached to the quoted belief, as signaling <em>her belief in the proposition.</em>  At the same time, the world itself seemed like people weren't so nice.  So she said, "I believe people are nicer than they are."<em><a id="more"></a></em></p> <p>And that verges on being an honest mistake&#8212;sort of&#8212;since people are not taught explicitly how to know when they believe something.  As in the parable of <a href="0054.html">the dragon in the garage</a> [http://lesswrong.com/lw/i4/belief_in_belief/]; the one who says "There is a dragon in my garage&#8212;but it's invisible", does not recognize his <em>anticipation</em> of seeing no dragon, as indicating that he possesses an (accurate) model with no dragon in it.</p> <p>It's not as if people are <em>trained </em>to recognize when they believe something.  It's not like they're ever taught in high school:  "What it feels like to actually believe something&#8212;to have that statement in your belief pool&#8212;is that it just seems like the way the world <em>is.</em>  You should recognize this feeling, which is actual (unquoted) belief, and distinguish it from having good feelings about a belief that you recognize as a belief (which means that it's in quote marks)."</p> <p>This goes a long way toward making this real-life case of Moore's Paradox <a href="0667.html">seem less alien</a> [http://lesswrong.com/lw/s/belief_in_selfdeception/], and providing another mechanism whereby people can be <a href="http://lesswrong.com/lw/1d/simultaneously_right_and_wrong/">simultaneously right and wrong</a> [http://lesswrong.com/lw/1d/simultaneously_right_and_wrong/].</p> <p>Likewise <a href="0666.html">Kurige</a> [http://lesswrong.com/lw/r/no_really_ive_deceived_myself/#gk] who wrote:</p> <blockquote> <p>I believe that there is a God&#8212;and that He has instilled a sense of right and wrong in us by which we are able to evaluate the world around us.  I also believe a sense of morality has been evolutionarily programmed into us&#8212;a sense of morality that is most likely a result of the formation of meta-political coalitions in Bonobo communities a very, very long time ago.  These two beliefs are not contradictory, but the complexity lies in reconciling the two.</p> </blockquote> <p>I suspect, Kurige, you have decided that you have <em>reasons to endorse</em> the quoted belief that God has instilled a sense of right and wrong in us.  And also that you have reasons to endorse the verdict of science.  They both seem like good communities to join, right?  There are benefits to both sets of beliefs?  You introspect and find that you feel good about both beliefs?</p> <p>But you did <em>not </em>say:</p> <p>"God instilled a sense of right and wrong in us, and also a sense of morality has been evolutionarily programmed into us.  The two states of reality are not inconsistent, but the complexity lies in reconciling the two."</p> <p>If you're reading this, Kurige, you should very quickly say the above out loud, so you can notice that it seems at least slightly harder to swallow&#8212;notice the <em>subjective difference</em>&#8212;before you go to the trouble of rerationalizing.</p> <p>This is the subjective difference between having reasons to endorse two different beliefs, and your mental model of a single world, a single way-things-are.</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Doublethink"><em>Against Doublethink</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Doublethink] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0671.html">Don't Believe You'll Self-Deceive</a> [http://lesswrong.com/lw/1o/dont_believe_youll_selfdeceive/]"</p> <p style="text-align:right">Previous post: "<a href="0667.html">Belief in Self-Deception</a> [http://lesswrong.com/lw/s/belief_in_selfdeception/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq08.html">Sequence 08: Against Doublethink</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0667.html">Belief in Self-Deception</a></p></td><td><p><i>Next: </i><a href="0671.html">Don't Believe You'll Self-Deceive</a></p></td></tr></table><p><i>Referenced by: </i><a href="0667.html">Belief in Self-Deception</a> &#8226; <a href="0671.html">Don't Believe You'll Self-Deceive</a> &#8226; <a href="0698.html">Rationality is Systematized Winning</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/1f/moores_paradox/">Moore's Paradox</a></p></body></html>