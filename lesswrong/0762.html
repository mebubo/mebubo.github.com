<html><head><title>The Lifespan Dilemma</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>The Lifespan Dilemma</h1><p><i>Eliezer Yudkowsky, 10 September 2009 06:45PM</i></p><div><p>One of our most controversial posts ever was "<a href="0145.html">Torture vs. Dust Specks</a> [http://lesswrong.com/lw/kn/torture_vs_dust_specks/]".  Though I can't seem to find the reference, one of the more interesting uses of this dilemma was by a professor whose student said "I'm a utilitarian consequentialist", and the professor said "No you're not" and told them about SPECKS vs. TORTURE, and then the student - to the professor's surprise - chose TORTURE.  (Yay student!)</p> <p>In the spirit of always making these things worse, let me offer a dilemma that might have been more likely to unconvince the student - at least, as a consequentialist, I find the inevitable conclusion much harder to swallow.<a id="more"></a></p> <p>I'll start by briefly introducing Parfit's Repugnant Conclusion, sort of a little brother to the main dilemma.  Parfit starts with a world full of a million happy people - people with plenty of resources apiece.  Next, Parfit says, let's introduce one more person who leads a life barely worth living - but since their life <em>is </em>worth living, adding this person must be a good thing.  Now we redistribute the world's resources, making it fairer, which is also a good thing.  Then we introduce another person, and another, until finally we've gone to a billion people whose lives are barely at subsistence level.  And since (Parfit says) it's obviously better to have a million happy people than a billion people at subsistence level, we've gone in a circle and revealed inconsistent preferences.</p> <p>My own analysis of the Repugnant Conclusion is that its apparent force comes from equivocating between senses of <em>barely worth living.</em>  In order to <em>voluntarily create </em>a new person, what we need is a life that is <em>worth celebrating</em> or <em>worth birthing,</em> one that contains more good than ill and more happiness than sorrow - otherwise we should reject the step where we choose to birth that person.  Once someone is alive, on the other hand, we're obliged to take care of them in a way that <a href="0582.html">we wouldn't be obliged to create them in the first place</a> [http://lesswrong.com/lw/ws/for_the_people_who_are_still_alive/] - and they may choose not to commit suicide, even if their life contains more sorrow than happiness.  If we would be saddened to <em>hear the news </em>that such a person existed, we shouldn't <em>kill</em> them, but we should <em>not</em> voluntarily create such a person in an otherwise happy world.  So each time we <em>voluntarily</em> add another person to Parfit's world, we have a little celebration and say with honest joy "Whoopee!", not, "Damn, now it's <a href="0597.html">too late to uncreate them</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/]."</p> <p>And then the rest of the Repugnant Conclusion - that it's better to have a billion lives slightly worth celebrating, than a million lives very worth celebrating - is just "repugnant" because of standard <a href="0046.html">scope insensitivity</a> [http://lesswrong.com/lw/hw/scope_insensitivity/].  The brain fails to multiply a billion small birth celebrations to end up with a larger total celebration of life than a million big celebrations.  Alternatively, average utilitarians - <a href="0582.html">I suspect I am one</a> [http://lesswrong.com/lw/ws/for_the_people_who_are_still_alive/] - may just reject the very first step, in which the average quality of life goes down.</p> <p>But now we introduce the Repugnant Conclusion's big sister, the Lifespan Dilemma, which - at least in my own opinion - seems much worse.</p> <p>To start with, suppose you have a 20% chance of dying in an hour, and an 80% chance of living for 10<sup>10,000,000,000</sup> years -</p> <p>Now I know what you're thinking, of course.  You're thinking, "Well, 10^(10^10) years may <em>sound</em> like a long time, unimaginably vaster than the 10^15 years the universe has lasted so far, but it isn't much, really.  I mean, most finite numbers are very much larger than that.  The realms of math are infinite, the realms of novelty and knowledge are infinite, and <a href="http://wiki.lesswrong.com/wiki/Fun_theory">Fun Theory</a> [http://wiki.lesswrong.com/wiki/Fun_theory] argues that we'll never run out of fun.  If I live for 10<sup>10,000,000,000</sup> years and then die, then when I draw my last metaphorical breath - not that I'd still have anything like a human body after that amount of time, of course - I'll go out raging against the night, for a life so short compared to all the experiences I wish I could have had.  You can't compare that to real immortality.  As Greg Egan put it, immortality isn't living for a very long time and then dying.  Immortality is just not dying, ever."</p> <p>Well, I can't offer you <em>real</em> immortality - not in <em>this </em>dilemma, anyway.  However, on behalf of my patron, Omega, who I believe is sometimes also known as Nyarlathotep, I'd like to make you a little offer.</p> <p>If you pay me just one penny, I'll replace your 80% chance of living for 10^(10^10) years, with a 79.99992% chance of living 10^(10^(10^10)) years.  That's 99.9999% of 80%, so I'm just shaving a tiny fraction 10<sup>-6</sup> off your probability of survival, and in exchange, if you do survive, you'll survive - not ten times as long, my friend, but <em>ten to the power of </em>as long.  And it goes without saying that you won't run out of memory (RAM) or other physical resources during that time.  If you feel that the notion of "years" is ambiguous, let's just measure your lifespan in computing operations instead of years.  Really there's not much of a difference when you're dealing with numbers like 10^(10<sup>10,000,000,000</sup>).</p> <p>My friend - can I call you friend? - let me take a few moments to dwell on what a wonderful bargain I'm offering you.  Exponentiation is a rare thing in gambles.  Usually, you put $1,000 at risk for a chance at making $1,500, or some multiplicative factor like that.  But when you exponentiate, you pay linearly and buy whole factors of 10 - buy them in wholesale quantities, my friend!  We're talking here about 10<sup>10,000,000,000</sup> factors of 10!  If you could use $1,000 to buy a 99.9999% chance of making $10,000 - gaining a single factor of ten - why, that would be the greatest investment bargain in history, too good to be true, but the deal that Omega is offering you is far beyond that!  If you started with $1, it takes a mere <em>eight</em> factors of ten to increase your wealth to $100,000,000.  Three more factors of ten and you'd be the wealthiest person on Earth.  Five more factors of ten beyond that and you'd own the Earth outright.  How old is the universe?  Ten factors-of-ten years.  Just ten!  How many quarks in the whole visible universe?  Around eighty factors of ten, as far as anyone knows.  And we're offering you here - why, not even ten billion factors of ten.  Ten billion factors of ten is just what you started with!  No, this is <em>ten to the ten billionth power</em> factors of ten.</p> <p>Now, you may say that your utility isn't linear in lifespan, just like it isn't linear in money.  But even if your utility is <em>logarithmic </em>in lifespan - a pessimistic assumption, surely; doesn't money decrease in value faster than life? - why, just the <em>logarithm</em> goes from 10,000,000,000 to 10<sup>10,000,000,000</sup>.</p> <p>From a <a href="http://wiki.lesswrong.com/wiki/Fun_theory">fun-theoretic</a> [http://wiki.lesswrong.com/wiki/Fun_theory] standpoint, exponentiating seems like something that really should let you have Significantly More Fun.  If you can afford to simulate a mind a quadrillion bits large, then you merely need 2^(1,000,000,000,000,000) times as much computing power - a quadrillion factors of 2 - to simulate <em>all possible</em> minds with a quadrillion binary degrees of freedom so defined.  Exponentiation lets you <em>completely</em> explore the <em>whole space </em>of which you were previously a single point - and that's just if you use it for brute force.  So going from a lifespan of 10^(10^10) to 10^(10^(10^10)) seems like it ought to be a significant improvement, from a fun-theoretic standpoint.</p> <p>And Omega is offering you this special deal, not for a dollar, not for a dime, but one penny!  That's right!  Act now!  Pay a penny and go from a 20% probability of dying in an hour and an 80% probability of living 10<sup>10,000,000,000</sup> years, to a 20.00008% probability of dying in an hour and a 79.99992% probability of living 10^(10<sup>10,000,000,000</sup>) years!  That's far more <em>factors of ten</em> in your lifespan than the number of quarks in the visible universe raised to the millionth power!</p> <p>Is that a penny, friend?  - thank you, thank you.  But wait!  There's another special offer, and you won't even have to pay a penny for this one - this one is <em>free!</em>  That's right, I'm offering to exponentiate your lifespan <em>again,</em> to 10^(10^(10<sup>10,000,000,000</sup>)) years!  Now, I'll have to multiply your probability of survival by 99.9999% again, but really, what's that compared to the nigh-incomprehensible increase in your expected lifespan?</p> <p>Is that an avaricious light I see in your eyes?  Then go for it!  Take the deal!  It's free!</p> <p><em>(Some time later.)</em></p> <p>My friend, I really don't understand your grumbles.  At every step of the way, you seemed eager to take the deal.  It's hardly my fault that you've ended up with... let's see... a probability of 1/10<sup>1000</sup> of living 10^^(2,302,360,800) years, and otherwise dying in an hour.  Oh, the ^^?  That's just a compact way of expressing tetration, or repeated exponentiation - it's really supposed to be Knuth up-arrows, &#8593;&#8593;, but I prefer to just write ^^.  So 10^^(2,302,360,800) means 10^(10^(10^...^10)) where the exponential tower of tens is 2,302,360,800 layers high.</p> <p>But, tell you what - these deals <em>are</em> intended to be permanent, you know, but if you pay me another penny, I'll trade you your current gamble for an 80% probability of living 10<sup>10,000,000,000</sup> years.</p> <p>Why, thanks!  I'm glad you've given me your two cents on the subject.</p> <p>Hey, don't make that face!  You've learned something about your own preferences, and that's the most valuable sort of information there is!</p> <p>Anyway, I've just received telepathic word from Omega that I'm to offer you another bargain - hey!  Don't run away until you've at least heard me out!</p> <p>Okay, I know you're feeling sore.  How's this to make up for it?  Right now you've got an 80% probability of living 10<sup>10,000,000,000</sup> years.  But right now - for free - I'll replace that with an 80% probability (that's right, 80%) of living 10^^10 years, that's 10^10^10^10^10^10^10^10<sup>10,000,000,000</sup> years.</p> <p>See?  I thought that'd wipe the frown from your face.</p> <p>So right now you've got an 80% probability of living 10^^10 years.  But if you give me a penny, I'll <em>tetrate</em> that sucker!  That's right - your lifespan will go to 10^^(10^^10) years!  That's an exponential tower (10^^10) tens high!  You could write that as 10^^^3, by the way, if you're interested.  Oh, and I'm afraid I'll have to multiply your survival probability by 99.99999999%.</p> <p><em>What?</em>  What do you mean, <em>no?</em>  The benefit here is vastly larger than the mere 10^^(2,302,360,800) years you bought previously, and you merely have to send your probability to 79.999999992% instead of 10<sup>-1000</sup> to purchase it!  Well, that and the penny, of course.  If you turn down <em>this</em> offer, what does it say about that whole road you went down before?  Think of how silly you'd look in retrospect!  Come now, pettiness aside, this is the real world, wouldn't you rather have a 79.999999992% probability of living 10^^(10^^10) years than an 80% probability of living 10^^10 years?  Those arrows suppress a lot of detail, as the saying goes!  If you can't have Significantly More Fun with tetration, how can you possibly hope to have fun at all?</p> <p>Hm?  Why yes, that's right, I <em>am</em> going to offer to tetrate the lifespan and fraction the probability yet again... I was thinking of taking you down to a survival probability of 1/(10^^^20), or something like that... oh, don't make that face at me, if you want to refuse the whole garden path you've got to refuse some particular step along the way.</p> <p>Wait!  Come back!  I have even faster-growing functions to show you!  And I'll take even smaller slices off the probability each time!  Come back!</p> <p>...ahem.</p> <p>While I feel that the Repugnant Conclusion has an obvious answer, and that <a href="0145.html">SPECKS vs. TORTURE</a> [http://lesswrong.com/lw/kn/torture_vs_dust_specks/] has an obvious answer, the Lifespan Dilemma actually confuses me - the more I demand answers of my mind, the stranger my intuitive responses get.  How are yours?</p> <p>Based on <a href="0242.html">an argument by Wei Dai</a> [http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/1107].  Dai proposed a <em>reductio </em>of unbounded utility functions by (correctly) pointing out that an unbounded utility on lifespan implies willingness to trade an 80% probability of living some large number of years for a 1/(3^^^3) probability of living some <em>sufficiently longer </em>lifespan.  I looked at this and realized that there existed an obvious garden path, which meant that denying the conclusion would create a preference reversal.  Note also the relation to the <a href="http://en.wikipedia.org/wiki/St._Petersburg_paradox">St. Petersburg Paradox</a> [http://en.wikipedia.org/wiki/St._Petersburg_paradox], although the Lifespan Dilemma requires only a finite number of steps to get us in trouble.</p></div> <hr><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/17h/the_lifespan_dilemma/">The Lifespan Dilemma</a></p></body></html>