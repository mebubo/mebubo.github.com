<html><head><title>Rationalization</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Rationalization</h1><p><i>Eliezer Yudkowsky, 30 September 2007 07:29PM</i></p><div><p><strong>Followup to</strong>:  <a href="0114.html">The Bottom Line</a> [http://lesswrong.com/lw/js/the_bottom_line/], <a href="0115.html">What Evidence Filtered Evidence?</a> [http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/]</p> <p>In "The Bottom Line", I presented the dilemma of two boxes only one of which contains a diamond, with various signs and portents as evidence.  I dichotomized the curious inquirer and the clever arguer.  The curious inquirer writes down all the signs and portents, and processes them, and finally writes down "<em>Therefore,</em> I estimate an 85% probability that box B contains the diamond."  The clever arguer works for the highest bidder, and begins by writing, "<em>Therefore,</em> box B contains the diamond", and then selects favorable signs and portents to list on the lines above.</p> <p>The first procedure is rationality.  The second procedure is generally known as "rationalization".</p> <p>"Rationalization."  What a curious term.  I would call it a <em>wrong word.</em>  You cannot "rationalize" what is not already rational.  It is as if "lying" were called "truthization".</p> <p><a id="more"></a></p> <p>On a purely computational level, there is a rather large difference between:</p> <ol> <li>Starting from evidence, and then crunching probability flows, in order to output a probable conclusion.  (Writing down all the signs and portents, and then flowing forward to a probability on <a href="0114.html">the bottom line</a> [http://lesswrong.com/lw/js/the_bottom_line/] which depends on those signs and portents.) </li> <li>Starting from a conclusion, and then crunching probability flows, in order to output evidence apparently favoring that conclusion. (Writing down the bottom line, and then flowing backward to <a href="0115.html">select</a> [http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/] signs and portents for <a href="0115.html">presentation</a> [http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/] on the lines above.) </li> </ol> <p>What fool devised such confusingly similar words, "rationality" and "rationalization", to describe such extraordinarily different mental processes?  I would prefer terms that made the algorithmic difference obvious, like "rationality" versus "giant sucking cognitive black hole".</p> <p>Not every change is an improvement, but every improvement is necessarily a change.  You cannot obtain more truth for a fixed proposition by arguing it; you can make more people believe it, but you cannot make it more <em>true.</em> To improve our beliefs, we must necessarily change our beliefs. Rationality is the operation that we use to obtain more truth-value for our beliefs by changing them.  Rationalization operates to fix beliefs in place; it would be better named "anti-rationality", both for its pragmatic results and for its reversed algorithm.</p> <p>"Rationality" is the <em>forward</em> flow that gathers evidence, weighs it, and outputs a conclusion.  The curious inquirer used a forward-flow algorithm: <em>first</em> gathering the evidence, writing down a list of all visible signs and portents, which they then processed <em>forward</em> to obtain a previously unknown probability for the box containing the diamond.  During the entire time that the rationality-process was running forward, the curious inquirer did not yet know their destination, which was why they were <em>curious.</em>  In the Way of Bayes, the prior probability equals the <a href="0068.html">expected posterior probability</a> [http://lesswrong.com/lw/ii/conservation_of_expected_evidence/]:  If you know your destination, you are already there.</p> <p>"Rationalization" is a <em>backward</em> flow from conclusion to selected evidence.  First you write down the bottom line, which is known and fixed; the purpose of your processing is to find out which arguments you should write down on the lines above.  This, not the bottom line, is the variable unknown to the running process.</p> <p>I fear that Traditional Rationality does not properly sensitize its users to the difference between forward flow and backward flow.  In Traditional Rationality, there is nothing wrong with the scientist who arrives at a pet hypothesis and then sets out to find an experiment that proves it.  A Traditional Rationalist would look at this approvingly, and say, "This pride is the engine that drives Science forward."  Well, it <em>is</em> the engine that drives Science forward.  It is easier to find a prosecutor and defender biased in opposite directions, than to find a single unbiased human.</p> <p>But just because everyone does something, doesn't make it okay.  It would be better yet if the scientist, arriving at a pet hypothesis, set out to <em>test</em> that hypothesis for the sake of <em>curiosity</em>&#8212;creating experiments that would drive their own beliefs in an <a href="0068.html">unknown direction</a> [http://lesswrong.com/lw/ii/conservation_of_expected_evidence/].</p> <p>If you genuinely don't know where you are going, you will probably feel quite curious about it.  Curiosity is the <a href="http://yudkowsky.net/virtues/">first virtue</a> [http://yudkowsky.net/virtues/], without which your questioning will be purposeless and your skills without direction.</p> <p>Feel the flow of the Force, and make sure it isn't flowing backwards.</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization"><em>Against Rationalization</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0118.html">A Rational Argument</a> [http://lesswrong.com/lw/jw/a_rational_argument/]"</p> <p style="text-align:right">Previous post: "<a href="0115.html">What Evidence Filtered Evidence?</a> [http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq07.html">Sequence 07: Against Rationalization</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0115.html">What Evidence Filtered Evidence?</a></p></td><td><p><i>Next: </i><a href="0118.html">A Rational Argument</a></p></td></tr></table><p><i>Referenced by: </i><a href="0115.html">What Evidence Filtered Evidence?</a> &#8226; <a href="0118.html">A Rational Argument</a> &#8226; <a href="0120.html">Avoiding Your Belief's Real Weak Points</a> &#8226; <a href="0123.html">No One Can Exempt You From Rationality's Laws</a> &#8226; <a href="0125.html">Priming and Contamination</a> &#8226; <a href="0132.html">Hold Off On Proposing Solutions</a> &#8226; <a href="0135.html">Pascal's Mugging: Tiny Probabilities of Vast Utilities</a> &#8226; <a href="0149.html">An Alien God</a> &#8226; <a href="0154.html">The Tragedy of Group Selectionism</a> &#8226; <a href="0157.html">Fake Optimization Criteria</a> &#8226; <a href="0180.html">Affective Death Spirals</a> &#8226; <a href="0181.html">Resist the Happy Death Spiral</a> &#8226; <a href="0346.html">Many Worlds, One Best Guess</a> &#8226; <a href="0377.html">Against Devil's Advocacy</a> &#8226; <a href="0439.html">Anthropomorphic Optimism</a> &#8226; <a href="0440.html">Contaminated by Optimism</a> &#8226; <a href="0482.html">Raised in Technophilia</a> &#8226; <a href="0509.html">Crisis of Faith</a> &#8226; <a href="0708.html">That Crisis thing seems pretty useful</a> &#8226; <a href="0806.html">SotW: Avoid Motivated Cognition</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ju/rationalization/">Rationalization</a></p></body></html>