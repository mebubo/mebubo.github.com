<html><head><title>Beyond the Reach of God</title></head><body><h1>Beyond the Reach of God</h1><p><i>Eliezer Yudkowsky, 04 October 2008 03:42PM</i></p><div><p><strong>Followup to</strong>:  <a href="0496.html">The Magnitude of His Own Folly</a> [http://lesswrong.com/lw/ue/the_magnitude_of_his_own_folly/]</p> <p>Today's post is a tad gloomier than usual, as I measure such things.  It deals with a thought experiment I invented to smash my own optimism, after <a href="0496.html">I realized that optimism had misled me</a> [http://lesswrong.com/lw/ue/the_magnitude_of_his_own_folly/].  Those readers sympathetic to arguments like, "It's important to keep our biases because they help us stay happy," should consider not reading.  (Unless they have <a href="0241.html">something to protect</a> [http://lesswrong.com/lw/nb/something_to_protect/], including their own life.)</p> <p>So!  Looking back on the magnitude of my own folly, I realized that at the root of it had been a disbelief in the Future's vulnerability&#8212;a reluctance to accept that things could <em>really </em>turn out wrong.  Not as the result of any explicit propositional verbal belief.  More like something inside that persisted in believing, even in the face of adversity, that everything would be all right in the end.</p> <p>Some would account this a virtue (<em>zettai daijobu da yo</em>), and others would say that it's a thing necessary for mental health.</p> <p>But we don't live in that world.  We live in the world beyond the reach of God.</p> <p><a id="more"></a></p> <p>It's been a long, long time since I believed in God.  Growing up in an Orthodox Jewish family, I can recall the last remembered time I asked God for something, though I don't remember how old I was.  I was putting in some request on behalf of the next-door-neighboring boy, I forget what exactly&#8212;something along the lines of, "I hope things turn out all right for him," or maybe "I hope he becomes Jewish."</p> <p>I remember what it was like to have some higher authority to appeal to, to take care of things I couldn't handle myself.  I didn't think of it as "warm", because I had no alternative to compare it to.  I just took it for granted.</p> <p>Still I recall, though only from distant childhood, what it's like to live in the <a href="0477.html">conceptually impossible possible world</a> [http://lesswrong.com/lw/tv/excluding_the_supernatural/] where God exists.  <em>Really</em> exists, in the way that children and rationalists take all their <a href="0054.html">beliefs</a> [http://lesswrong.com/lw/i4/belief_in_belief/] at face value.</p> <p>In the world where God exists, does God intervene to optimize <em>everything?</em>  Regardless of what rabbis assert about the fundamental nature of reality, the take-it-seriously operational answer to this question is obviously "No".  You can't ask God to bring you a lemonade from the refrigerator instead of getting one yourself.  When I believed in God after the serious fashion of a child, so very long ago, I didn't believe that.</p> <p>Postulating that particular divine inaction doesn't provoke a full-blown theological crisis.  If you said to me, "I have constructed a benevolent superintelligent nanotech-user", and I said "Give me a banana," and no banana appeared, this would not <em>yet</em> disprove your statement.  Human parents don't always do everything their children ask.  There are some decent fun-theoretic arguments&#8212;I even believe them myself&#8212;against the idea that the <em>best</em> kind of help you can offer someone, is to always immediately give them everything they want.  I don't think that eudaimonia is formulating goals and having them instantly fulfilled; I don't <em>want</em> to become a simple wanting-thing that never has to plan or act or think.</p> <p>So it's not necessarily an attempt to avoid falsification, to say that God does not grant all prayers.  Even a Friendly AI might not respond to every request.</p> <p>But clearly, there exists <em>some</em> threshold of horror awful enough that God will intervene.  I remember that being true, when I believed after the fashion of a child.</p> <p>The God who does not intervene <em>at all</em>, no matter how bad things get&#8212;<em>that's</em> an obvious attempt to avoid falsification, to protect a <a href="0054.html">belief-in-belief</a> [http://lesswrong.com/lw/i4/belief_in_belief/].  Sufficiently young children don't have the deep-down knowledge that God doesn't really exist.  They really expect to see <a href="0054.html">a dragon in their garage</a> [http://lesswrong.com/lw/i4/belief_in_belief/].  They have no reason to imagine a loving God who never acts.  Where exactly is the boundary of sufficient awfulness?  Even a child can imagine arguing over the precise threshold.  But of course God will draw the line somewhere.  Few indeed are the loving parents who, desiring their child to grow up strong and self-reliant, would let their toddler be run over by a car.</p> <p>The obvious example of a horror so great that God cannot tolerate it, is death&#8212;true death, mind-annihilation.  I don't think that even Buddhism allows that.  So long as there is a God in the classic sense&#8212;full-blown, ontologically fundamental, <em>the</em> God&#8212;we can rest assured that no <em>sufficiently</em> awful event will ever, ever happen.  There is no soul anywhere that need fear true annihilation; God will prevent it.</p> <p>What if you build your own simulated universe?  The classic example of a simulated universe is Conway's Game of Life.  I do urge you to <a href="http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">investigate</a> [http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life] Life if you've never played it&#8212;it's important for comprehending the notion of "physical law".  Conway's Life has been proven Turing-complete, so it would be possible to build a sentient being in the Life universe, albeit it might be rather fragile and awkward.  Other cellular automata would make it simpler.</p> <p>Could you, by creating a simulated universe, escape the reach of God?  Could you simulate a Game of Life containing sentient entities, and torture the beings therein?  But if God is watching everywhere, then trying to build an unfair Life just results in <em>the</em> God stepping in to modify your computer's transistors.  If the physics you set up in your computer program calls for a sentient Life-entity to be endlessly tortured for no particular reason, <em>the</em> God will intervene.  God being omnipresent, there is no refuge <em>anywhere</em> for true horror:  Life is fair.</p> <p>But suppose that instead you ask the question:</p> <p><em>Given</em> such-and-such initial conditions, and <em>given</em> such-and-such cellular automaton rules, what <em>would be</em> the mathematical result?</p> <p>Not even God can modify the answer to this question, unless you believe that God can implement logical impossibilities.  Even as a very young child, I don't remember believing that.  (And why would you need to believe it, if God can modify anything that <em>actually</em> exists?)</p> <p>What does Life look like, in this imaginary world where every step follows <em>only</em> from its immediate predecessor?  Where things <em>only</em> ever happen, or don't happen, because of the cellular automaton rules?  Where the initial conditions and rules <em>don't</em> describe any God that checks over each state?  What does it look like, the world beyond the reach of God?</p> <p>That world wouldn't be fair.  If the initial state contained the seeds of something that could self-replicate, natural selection might or might not take place, and complex life might or might not evolve, and that life might or might not become sentient, with no God to guide the evolution.  That world might evolve the equivalent of conscious cows, or conscious dolphins, that lacked hands to improve their condition; maybe they would be eaten by conscious wolves who never thought that they were doing wrong, or cared.</p> <p>If in a vast plethora of worlds, something like humans evolved, then they would suffer from diseases&#8212;not to teach them any lessons, but only because viruses happened to evolve as well, under the cellular automaton rules.</p> <p>If the people of that world are happy, or unhappy, the causes of their happiness or unhappiness may have nothing to do with good or bad choices they made.  Nothing to do with free will or lessons learned.  In the what-if world where every step follows only from the cellular automaton rules, the equivalent of Genghis Khan can murder a million people, and laugh, and be rich, and never be punished, and live his life much happier than the average.  Who prevents it?  God would prevent it from ever <em>actually</em> happening, of course; He would at the very least visit some shade of gloom in the Khan's heart.  But in the mathematical answer to the question <em>What if?</em> there is no God in the axioms.  So if the cellular automaton rules say that the Khan is happy, that, simply, is the whole and only answer to the what-if question.  There is nothing, absolutely nothing, to prevent it.</p> <p>And if the Khan tortures people horribly to death over the course of days, for his own amusement perhaps?  They will call out for help, perhaps imagining a God.  And if you <em>really</em> wrote that cellular automaton, God would intervene in your program, of course.  But in the what-if question, what the cellular automaton <em>would</em> do under the mathematical rules, there isn't any God in the system.  Since the physical laws contain no specification of a utility function&#8212;in particular, no prohibition against torture&#8212;then the victims will be saved only if the right cells happen to be 0 or 1.  And it's not likely that anyone will defy the Khan; if they did, someone would strike them with a sword, and the sword would disrupt their organs and they would die, and that would be the end of that.  So the victims die, screaming, and no one helps them; that is the answer to the what-if question.</p> <p>Could the victims be completely innocent?  Why not, in the what-if world?  If you look at the rules for Conway's Game of Life (which is Turing-complete, so we can embed arbitrary computable physics in there), then the rules are really very simple.  Cells with three living neighbors stay alive; cells with two neighbors stay the same, all other cells die.  There isn't anything in there about only innocent people not being horribly tortured for indefinite periods.</p> <p>Is this world starting to sound familiar?</p> <p>Belief in a fair universe often manifests in more subtle ways than thinking that horrors should be outright prohibited:  Would the twentieth century have gone differently, if Klara P&#246;lzl and Alois Hitler had made love one hour earlier, and a different sperm fertilized the egg, on the night that Adolf Hitler was conceived?</p> <p>For so many lives and so much loss to turn on a single event, seems <em>disproportionate.</em>  The Divine Plan ought to make more <em>sense</em> than that.  You can believe in a Divine Plan without believing in God&#8212;Karl Marx surely did.  You shouldn't have millions of lives depending on a casual choice, an hour's timing, the speed of a microscopic flagellum.  It ought not to be allowed.  It's <em>too</em> disproportionate.  Therefore, if Adolf Hitler had been able to go to high school and become an architect, there would have been someone else to take his role, and World War II would have happened the same as before.</p> <p>But in the world beyond the reach of God, there isn't any clause in the physical axioms which says "things have to make sense" or "big effects need big causes" or "history runs on reasons too important to be so fragile".  There is no God to <em>impose</em> that order, which is so severely violated by having the lives and deaths of millions depend on one small molecular event.</p> <p>The point of the thought experiment is to lay out the God-universe and the Nature-universe side by side, so that we can recognize what kind of thinking belongs to the God-universe.  Many who are atheists, still think as if certain things are <em>not allowed</em>.  They would lay out arguments for why World War II was inevitable and would have happened in more or less the same way, even if Hitler had become an architect.  But in sober historical fact, this is an unreasonable belief; I chose the example of World War II because from my reading, it seems that events were mostly driven by Hitler's personality, often in defiance of his generals and advisors.  There is no particular empirical justification that I happen to have heard of, for doubting this.  The main reason to doubt would be <em>refusal to accept</em> that the universe could make so little sense&#8212;that horrible things could happen so <em>lightly,</em> for no more reason than a roll of the dice.</p> <p>But why not?  What prohibits it?</p> <p>In the God-universe, God prohibits it.  To recognize this is to recognize that we don't live in that universe.  We live in the what-if universe beyond the reach of God, driven by the mathematical laws and nothing else.  Whatever physics says will happen, will happen.  Absolutely <em>anything,</em> good or bad, will happen.  And there is nothing in the laws of physics to lift this rule even for the <em>really extreme</em> cases, where you might expect Nature to be a little more reasonable.</p> <p>Reading William Shirer's <em>The Rise and Fall of the Third Reich,</em> listening to him describe the disbelief that he and others felt upon discovering the full scope of Nazi atrocities, I thought of what a strange thing it was, to read all that, and know, already, that there wasn't a single protection against it.  To just read through the whole book and accept it; horrified, but not at all disbelieving, because I'd already understood what kind of world I lived in.</p> <p>Once upon a time, I believed that the extinction of humanity was not allowed.  And others who call themselves rationalists, may yet have things they trust.  They might be called "positive-sum games", or "democracy", or "technology", but they are sacred.  The mark of this sacredness is that the trustworthy thing can't lead to anything <em>really</em> bad; or they can't be <em>permanently</em> defaced, at least not without a compensatory silver lining.  In that sense they can be trusted, even if a few bad things happen here and there.</p> <p>The unfolding history of Earth can't ever turn from its positive-sum trend to a negative-sum trend; that is not allowed.  <a href="0097.html">Democracies</a> [http://lesswrong.com/lw/jb/applause_lights/]&#8212;<em>modern</em> <em>liberal</em> democracies, anyway&#8212;won't ever legalize torture.  <a href="0482.html">Technology</a> [http://lesswrong.com/lw/u0/raised_in_technophilia/] has done so much good up until now, that there can't possibly be a Black Swan technology that breaks the trend and does more harm than all the good up until this point.</p> <p>There are all sorts of clever arguments why such things can't possibly happen.  But the source of these arguments is a much deeper belief that such things are <em>not allowed</em>.  Yet who prohibits?  Who prevents it from happening?  If you can't visualize at least one lawful universe where physics say that such dreadful things happen&#8212;and so they <em>do</em> happen, there being nowhere to appeal the verdict&#8212;then you aren't yet ready to argue <em>probabilities</em>.</p> <p>Could it really be that sentient beings have died absolutely for thousands or millions of years, with no soul and no afterlife&#8212;and <em>not</em> as part of any grand plan of Nature&#8212;<em>not</em> to teach any great lesson about the meaningfulness or meaninglessness of life&#8212;not even to teach any profound lesson about what is impossible&#8212;so that a trick as simple and stupid-sounding as <a href="0371.html">vitrifying people in liquid nitrogen</a> [http://lesswrong.com/lw/qx/timeless_identity/] can save them from total annihilation&#8212;and a 10-second rejection of the silly idea can destroy someone's soul?  Can it be that a computer programmer who signs a few papers and buys a life-insurance policy continues into the far future, while Einstein rots in a grave?  We can be sure of one thing:  God wouldn't allow it.  Anything that ridiculous and disproportionate would be ruled out.  It would make a mockery of the Divine Plan&#8212;a mockery of the <em>strong reasons</em> why things must be the way they are.</p> <p>You can have secular rationalizations for things being <em>not allowed</em>.  So it helps to imagine that there <em>is</em> a God, benevolent as you understand goodness&#8212;a God who enforces throughout Reality a <em>minimum</em> of fairness and justice&#8212;whose plans make sense and depend proportionally on people's choices&#8212;who will never permit absolute horror&#8212;who does not always intervene, but who at least prohibits universes wrenched <em>completely</em> off their track... to imagine all this, but also imagine that <em>you,</em> yourself, live in a what-if world of pure mathematics&#8212;a world beyond the reach of God, an utterly unprotected world where anything at all can happen.</p> <p>If there's any reader still reading this, who thinks that being happy counts for more than anything in life, then maybe they <em>shouldn't</em> spend much time pondering the unprotectedness of their existence.  Maybe think of it <em>just</em> long enough to sign up themselves and their family for cryonics, and/or write a check to an existential-risk-mitigation agency now and then.  And wear a seatbelt and get health insurance and all those other dreary necessary things that can destroy your life if you miss that one step... but aside from that, if you want to be happy, meditating on the fragility of life isn't going to help.</p> <p>But this post was written for those who have <a href="0241.html">something to protect</a> [http://lesswrong.com/lw/nb/something_to_protect/].</p> <p>What can a twelfth-century peasant do to save themselves from annihilation?  Nothing.  Nature's little challenges aren't always fair.  When you run into a challenge that's too difficult, you suffer the penalty; when you run into a lethal penalty, you die.  That's how it is for people, and it isn't any different for planets.  Someone who wants to dance the deadly dance with Nature, does need to understand what they're up against:  Absolute, utter, exceptionless neutrality.</p> <p>Knowing this won't always save you.  It wouldn't save a twelfth-century peasant, even if they knew.  If you think that a rationalist who fully understands the mess they're in, must <em>surely</em> be able to find a way out&#8212;then you <a href="0353.html">trust rationality</a> [http://lesswrong.com/lw/qf/no_safe_defense_not_even_science/], enough said.</p> <p>Some commenter is bound to castigate me for putting too dark a tone on all this, and in response they will list out all the reasons why it's lovely to live in a neutral universe.  Life is allowed to be a <em>little</em> dark, after all; but not darker than a certain point, unless there's a silver lining.</p> <p>Still, because I don't want to create <em>needless</em> despair, I will say a few hopeful words at this point:</p> <p>If humanity's future unfolds in the right way, we might be able to make our future light cone fair(er).  We can't modify fundamental physics, but on a higher level of organization we could build some guardrails and put down some padding; organize the particles into a pattern that does some internal checks against catastrophe.  There's a lot of stuff out there that we can't touch&#8212;but it may help to consider everything that isn't in our future light cone, as being part of the "generalized past".  As if it had all already happened.  There's at least the <em>prospect</em> of defeating neutrality, in the only future we can touch&#8212;the only world that it accomplishes something to care about.</p> <p>Someday, maybe, immature minds will reliably be sheltered.  Even if children go through the equivalent of not getting a lollipop, or even burning a finger, they won't ever be run over by cars.</p> <p>And the adults wouldn't be in so much danger.  A superintelligence&#8212;a mind that could think a trillion thoughts without a misstep&#8212;would not be intimidated by a challenge where death is the price of a single failure.  The raw universe wouldn't seem so harsh, would be only another problem to be solved.</p> <p>The problem is that building an adult is itself an adult challenge.  That's what I finally realized, years ago.</p> <p>If there is a fair(er) universe, we have to get there starting from <em>this</em> world&#8212;the neutral world, the world of hard concrete with no padding, the world where challenges are not calibrated to your skills.</p> <p>Not every child needs to stare Nature in the eyes.  Buckling a seatbelt, or writing a check, is not that complicated or deadly.  I don't say that every rationalist should meditate on neutrality.  I don't say that every rationalist should think all these unpleasant thoughts.  But anyone who plans on confronting an uncalibrated<em> </em>challenge of instant death, must not avoid them.</p> <p>What does a child need to do&#8212;what rules should they follow, how should they behave&#8212;to solve an adult problem?</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age"><em>Yudkowsky's Coming of Age</em></a> [http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age]</p> <p style="text-align:right">Next post: "<a href="0503.html">My Bayesian Enlightenment</a> [http://lesswrong.com/lw/ul/my_bayesian_enlightenment/]"</p> <p style="text-align:right">Previous post: "<a href="0496.html">The Magnitude of His Own Folly</a> [http://lesswrong.com/lw/ue/the_magnitude_of_his_own_folly/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq22.html">Sequence 22: Yudkowsky's Coming of Age</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0496.html">The Magnitude of His Own Folly</a></p></td><td><p><i>Next: </i><a href="0503.html">My Bayesian Enlightenment</a></p></td></tr></table><p><i>Referenced by: </i><a href="0496.html">The Magnitude of His Own Folly</a> &#8226; <a href="0503.html">My Bayesian Enlightenment</a> &#8226; <a href="0505.html">On Doing the Impossible</a> &#8226; <a href="0506.html">Make an Extraordinary Effort</a> &#8226; <a href="0507.html">Shut up and do the impossible!</a> &#8226; <a href="0513.html">Ends Don't Justify Means (Among Humans)</a> &#8226; <a href="0514.html">Entangled Truths, Contagious Lies</a> &#8226; <a href="0517.html">Protected From Myself</a> &#8226; <a href="0583.html">Not Taking Over the World</a> &#8226; <a href="0602.html">The Uses of Fun (Theory)</a> &#8226; <a href="0603.html">Growing Up is Hard</a> &#8226; <a href="0608.html">Serious Stories</a> &#8226; <a href="0618.html">Sympathetic Minds</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/uk/beyond_the_reach_of_god/">Beyond the Reach of God</a></p></body></html>