<html><head><title>Growing Up is Hard</title></head><body><h1>Growing Up is Hard</h1><p><i>Eliezer Yudkowsky, 04 January 2009 03:55AM</i></p><div><p>Terrence Deacon's <em>The Symbolic Species</em> is the best book I've ever read on the evolution of intelligence.  Deacon somewhat overreaches when he tries to theorize about what our X-factor <em>is;</em> but his exposition of its <em>evolution</em> is first-class.</p> <p>Deacon makes an excellent case&#8212;he has quite persuaded me&#8212;that the increased <em>relative </em>size of our frontal cortex, compared to other hominids, is of overwhelming importance in understanding the evolutionary development of humanity.  It's not just a question of increased computing capacity, like adding extra processors onto a cluster; it's a question of what kind of signals dominate, in the brain.</p> <p>People with Williams Syndrome (caused by deletion of a certain region on chromosome 7) are hypersocial, ultra-gregarious; as children they fail to show a normal fear of adult strangers.  WSers are cognitively impaired on most dimensions, but their verbal abilities are spared or even exaggerated; they often speak early, with complex sentences and large vocabulary, and excellent verbal recall, even if they can never learn to do basic arithmetic.</p> <p>Deacon makes a case for some Williams Syndrome symptoms coming from a frontal cortex that is <em>relatively too large</em> for a human, with the result that prefrontal signals&#8212;including certain social emotions&#8212;dominate more than they should.</p> <p><a id="more"></a></p> <p>"Both postmortem analysis and MRI analysis have revealed brains with a reduction of the entire posterior cerebral cortex, but a sparing of the cerebellum and frontal lobes, and perhaps even an exaggeration of cerebellar size," says Deacon.</p> <p>Williams Syndrome's deficits can be explained by the shrunken posterior cortex&#8212;they can't solve simple problems involving shapes, because the parietal cortex, which handles shape-processing, is diminished.  But the frontal cortex is not actually <em>enlarged;</em> it is simply <em>spared.</em>  So where do WSers' <em>augmented </em>verbal abilities come from?</p> <p>Perhaps because the signals sent out by the frontal cortex, saying "pay attention to this verbal stuff!", <em>win out</em> over signals coming from the shrunken sections of the brain.  So the verbal abilities get lots of exercise&#8212;and other abilities don't.</p> <p>Similarly with the hyper-gregarious nature of WSers; the signal saying "Pay attention to this person!", originating in the frontal areas where social processing gets done, dominates the emotional landscape.</p> <p>And Williams Syndrome is not frontal <em>enlargement,</em> remember; it's just frontal <em>sparing</em> in an otherwise shrunken brain, which increases the <em>relative</em> force of frontal signals...</p> <p>...beyond the <em>narrow </em>parameters within which a human brain is adapted to work.</p> <p>I mention this because you might look at the history of human evolution, and think to yourself, "Hm... to get from a chimpanzee to a human... you enlarge the frontal cortex... so if we enlarge it <em>even further...</em>"</p> <p>The road to +Human is not that simple.</p> <p>Hominid brains have been tested billions of times over through thousands of generations.  But you shouldn't reason <a href="0464.html">qualitatively</a> [http://lesswrong.com/lw/ti/qualitative_strategies_of_friendliness/], "Testing creates 'robustness', so now the human brain must be 'extremely robust'."  Sure, we can expect the human brain to be robust against <em>some</em> insults, like the loss of a single neuron.  But testing in an evolutionary paradigm only creates robustness over the domain tested.  Yes, <em>sometimes </em>you get robustness beyond that, because sometimes evolution finds simple solutions that prove to generalize&#8212;</p> <p>But people do go crazy.  Not colloquial crazy, actual crazy.  Some ordinary young man in college suddenly decides that everyone around them is <em>staring </em>at them because they're part of the <em>conspiracy</em>.  (I saw that happen once, and made a classic non-Bayesian mistake; I knew that this was archetypal schizophrenic behavior, but I didn't realize that similar symptoms can arise from many other causes.  Psychosis, it turns out, is a general failure mode, "the fever of CNS illnesses"; it can also be caused by drugs, brain tumors, or just sleep deprivation.  I saw the perfect fit to what I'd read of schizophrenia, and didn't ask "<a href="0082.html">What if other things fit just as perfectly?</a> [http://lesswrong.com/lw/iw/positive_bias_look_into_the_dark/]"  So my snap diagnosis of schizophrenia turned out to be wrong; but as I wasn't foolish enough to try to handle the case myself, things turned out all right in the end.)</p> <p>Wikipedia says that the current main hypotheses being considered for psychosis are (a) too much dopamine in one place (b) not enough glutamate somewhere else.  (I thought I remembered hearing about serotonin imbalances, but maybe that was something else.)</p> <p>That's how <em>robust </em>the human brain is: a gentle little neurotransmitter imbalance&#8212;so subtle they're still having trouble tracking it down after who knows how many fMRI studies&#8212;can give you a full-blown case of stark raving mad.</p> <p>I don't know how often psychosis happens to hunter-gatherers, so maybe it has something to do with a modern diet?  We're not getting exactly the right ratio of Omega 6 to Omega 3 fats, or we're eating too much processed sugar, or something.  And among the many other things that go haywire with the metabolism as a result, the brain moves into a more fragile state that breaks down more easily...</p> <p>Or whatever.  That's just a random hypothesis.  By which I mean to say:  The brain really <em>is</em> adapted to a very narrow range of operating parameters.  It doesn't tolerate a little too much dopamine, just as your metabolism isn't very robust against non-ancestral ratios of Omega 6 to Omega 3.  Yes, <em>sometimes </em>you get bonus robustness in a new domain, when evolution solves W, X, and Y using a compact adaptation that also extends to novel Z.  Other times... quite often, really... Z just isn't covered.</p> <p>Often, you step outside the box of the ancestral parameter ranges, and things just plain break.</p> <p>Every part of your brain assumes that all the other surrounding parts work a certain way.  The present brain is the Environment of Evolutionary Adaptedness for every individual piece of the present brain.</p> <p>Start modifying the pieces in ways that seem like "good ideas"&#8212;making the frontal cortex larger, for example&#8212;and you start operating outside the ancestral box of parameter ranges.  And then everything goes to hell.  Why <em>shouldn't</em> it?  Why would the brain be designed for easy upgradability?</p> <p>Even if one change works&#8212;will the second?  Will the third?  Will all four changes work well together?  Will the fifth change have all that greater a probability of breaking something, because you're already operating that much further outside the ancestral box?  Will the sixth change prove that you exhausted all the brain's robustness in tolerating the changes you made already, and now there's no adaptivity left?</p> <p>Poetry aside, a human being <em>isn't</em> the seed of a god.  We don't have neat little dials that you can easily tweak to more "advanced" settings.  We are <em>not</em> designed for our parts to be upgraded.  Our parts are adapted to work exactly as they are, in their current context, every part tested in a regime of the other parts being the way they are.  <a href="0151.html">Idiot evolution</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/] does not look ahead, it does not design with the intent of different future uses.  We are <em>not</em> designed to unfold into something bigger<em>.<br></em></p> <p>Which is not to say that it could never, ever be done.</p> <p>You could build a modular, cleanly designed AI that could make a billion sequential upgrades to itself using deterministic guarantees of correctness.  A Friendly AI programmer could do even more arcane things to make sure the AI knew what you would-want if you understood the possibilities.  And then the AI could apply superior intelligence to untangle the pattern of all those neurons (<a href="0594.html">without simulating you in such fine detail as to create a new person</a> [http://lesswrong.com/lw/x4/nonperson_predicates/]), and to foresee the consequences of its acts, and to understand the meaning of those consequences under your values.  And the AI could upgrade one thing while simultaneously tweaking the five things that depend on it and the twenty things that depend on them.  Finding a gradual, incremental path to greater intelligence (so as not to effectively erase you and replace you with someone else) that didn't drive you psychotic or give you Williams Syndrome or a hundred other syndromes.</p> <p>Or you could walk the path of unassisted human enhancement, trying to make changes to yourself <em>without</em> understanding them fully.  Sometimes changing yourself the wrong way, and being murdered or suspended to disk, and replaced by an earlier backup.  Racing against the clock, trying to raise your intelligence without breaking your brain or mutating your will.  Hoping you became sufficiently super-smart that you could improve the skill with which you modified yourself.  Before your hacked brain moved so far outside ancestral parameters and tolerated so many insults that its fragility reached a limit, and you fell to pieces with every new attempted modification beyond that.  Death is far from the worst risk here.  Not every form of madness will appear immediately when you branch yourself for testing&#8212;some insanities might incubate for a while before they became visible.  And you might not notice if your goals shifted only a bit at a time, as your emotional balance altered with the strange new harmonies of your brain.</p> <p>Each path has its little upsides and downsides.  (E.g:  AI requires supreme precise knowledge; human upgrading has a nonzero probability of success through trial and error.  Malfunctioning AIs mostly kill you and tile the galaxy with smiley faces; human upgrading might produce insane gods to rule over you in Hell forever.  Or so my current understanding would predict, anyway; it's not like I've observed any of this as a fact.)</p> <p>And I'm sorry to dismiss such a gigantic dilemma with three paragraphs, but it wanders from the point of today's post:</p> <p>The point of today's post is that growing up&#8212;or even deciding what you want to be when you grow up&#8212;is as around <a href="0597.html">as hard as designing a new intelligent species</a> [http://lesswrong.com/lw/x7/cant_unbirth_a_child/].  Harder, since you're constrained to start from the base of an existing design.  There is no <em>natural</em> path laid out to godhood, no Level attribute that you can neatly increment and watch everything else fall into place.  It is an <a href="0502.html">adult problem</a> [http://lesswrong.com/lw/uk/beyond_the_reach_of_god/].</p> <p>Being a transhumanist means <em>wanting</em> certain things&#8212;judging them to be good.  It doesn't mean you think those goals are easy to achieve.</p> <p>Just as there's a wide range of understanding among people who talk about, say, quantum mechanics, there's also a certain range of competence among transhumanists.  There are transhumanists who fall into the trap of the <a href="0174.html">affect heuristic</a> [http://lesswrong.com/lw/lg/the_affect_heuristic/], who see the potential benefit of a technology, and therefore <em>feel really good</em> about that technology, so that it also seems that the technology (a) has readily managed downsides (b) is easy to implement well and (c) will arrive relatively soon.</p> <p>But only the <em>most </em>formidable adherents of an idea are any sign of its strength.  Ten thousand New Agers babbling nonsense, do not cast the least shadow on real quantum mechanics.  And among the more formidable transhumanists, it is not at all rare to find someone who wants something <em>and </em>thinks it will not be easy to get.</p> <p>One is much more likely to find, say, Nick Bostrom&#8212;that is, Dr. Nick Bostrom, Director of the Oxford Future of Humanity Institute and founding Chair of the World Transhumanist Assocation&#8212;arguing that <a href="http://www.nickbostrom.com/evolution.pdf">a possible test for whether a cognitive enhancement is</a> [http://www.nickbostrom.com/evolution.pdf] likely to have downsides, is the ease with which it <em>could</em> have occurred as a natural mutation&#8212;since if it had only upsides and could easily occur as a natural mutation, why hasn't the brain already adapted accordingly?  This is one reason to be wary of, say, cholinergic memory enhancers: if they have no downsides, why doesn't the brain produce more acetylcholine already?  Maybe you're using up a limited memory capacity, or forgetting something else...</p> <p>And that may or may not turn out to be a good heuristic.  But the point is that the serious, smart, technically minded transhumanists, do not always expect that the road to everything they want is easy.  (Where you want to be wary of people who say, "But I dutifully acknowledge that there are obstacles!" but stay in basically the same mindset of <a href="0061.html">never truly doubting</a> [http://lesswrong.com/lw/ib/the_proper_use_of_doubt/] the victory.)</p> <p>So you'll forgive me if I am somewhat annoyed with people who run around saying, "I'd like to be a hundred times as smart!" as if it were as simple as scaling up a hundred times instead of requiring a whole new cognitive architecture; and as if a change of that magnitude in one shot wouldn't amount to erasure and replacement.  Or asking, "Hey, <em>why not just</em> augment humans instead of building AI?" as if it wouldn't be a desperate race against madness.</p> <p>I'm not against being smarter.  I'm not against augmenting humans.  I am still a transhumanist; I still judge that these are good goals.</p> <p>But it's really not that <em>simple</em>, okay?</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0604.html">Changing Emotions</a> [http://lesswrong.com/lw/xe/changing_emotions/]"</p> <p style="text-align:right">Previous post: "<a href="0620.html">Failed Utopia #4-2</a> [http://lesswrong.com/lw/xu/failed_utopia_42/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0620.html">Failed Utopia #4-2</a></p></td><td><p><i>Next: </i><a href="0604.html">Changing Emotions</a></p></td></tr></table><p><i>Referenced by: </i><a href="0604.html">Changing Emotions</a> &#8226; <a href="0617.html">In Praise of Boredom</a> &#8226; <a href="0620.html">Failed Utopia #4-2</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0639.html">The Thing That I Protect</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/xd/growing_up_is_hard/">Growing Up is Hard</a></p></body></html>