<html><head><title>The Uses of Fun (Theory)</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>The Uses of Fun (Theory)</h1><p><i>Eliezer Yudkowsky, 02 January 2009 08:30PM</i></p><div><p style="margin-left: 40px;">"But is there anyone who actually wants to live in a Wellsian Utopia?  On the contrary, not to live in a world like that, not to wake up in a hygenic garden suburb infested by naked schoolmarms, has actually become a conscious political motive.  A book like Brave New World is an expression of the actual fear that modern man feels of the rationalised hedonistic society which it is within his power to create."<br>&#160;&#160;&#160; &#160;&#160;&#160; &#8212;George Orwell, <em>Why Socialists Don't Believe in Fun</em></p> <p>There are three reasons I'm talking about Fun Theory, some more important than others:</p> <ol> <li>If every picture ever drawn of the Future looks like a terrible place to actually live, it might tend to drain off the motivation to create the future.  <a href="0580.html">It takes hope to sign up for cryonics</a> [http://lesswrong.com/lw/wq/you_only_live_twice/].</li> <li>People who leave their religions, but don't familiarize themselves with the <a href="0180.html">deep</a> [http://lesswrong.com/lw/lm/affective_death_spirals/], <a href="0054.html">foundational</a> [http://lesswrong.com/lw/i4/belief_in_belief/], <a href="0477.html">fully general</a> [http://lesswrong.com/lw/tv/excluding_the_supernatural/] arguments against theism, are at risk of backsliding.  Fun Theory lets you look at our <a href="0502.html">present world</a> [http://lesswrong.com/lw/uk/beyond_the_reach_of_god/], and see that it is not optimized <em>even for considerations like personal responsibility or self-reliance.</em>  It is the fully general reply to theodicy.</li> <li>Going into the details of Fun Theory helps you see that eudaimonia is actually <em>complicated </em>&#8212;that there are a lot of properties necessary for a mind to lead a worthwhile existence.  Which helps you appreciate just how worthless a galaxy would end up looking (with extremely high probability) if it was optimized by something with a utility function rolled up at random.</li> </ol> <p><a id="more"></a></p> <p>To amplify on these points in order:</p> <p>(1)  You've got folks like Leon Kass and the other members of Bush's "President's Council on Bioethics" running around talking about what a terrible, terrible thing it would be if people lived longer than threescore and ten.  While some philosophers have pointed out the flaws in their arguments, it's one thing to point out a flaw and another to provide a counterexample.  "Millions long for immortality who do not know what to do with themselves on a rainy Sunday afternoon," said Susan Ertz, and that argument will sound plausible for as long as you can't imagine what to do on a rainy Sunday afternoon, and it seems unlikely that anyone could imagine it.</p> <p>It's not exactly the fault of Hans Moravec that his world in which humans are kept by superintelligences as pets, doesn't sound quite Utopian.  Utopias are just really hard to construct, for reasons I'll talk about in more detail later&#8212;but this observation has already been made by many, including George Orwell.</p> <p>Building the Future is part of the ethos of secular humanism, our common project.  If you have nothing to look forward to&#8212;if there's no image of the Future that can inspire real enthusiasm&#8212;then you won't be able to scrape up enthusiasm for that common project.  And if the project is, in fact, a worthwhile one, the expected utility of the future will suffer accordingly from that nonparticipation.  So that's one side of the coin, just as the other side is living so exclusively in a fantasy of the Future that you can't bring yourself to go on in the Present.</p> <p>I recommend thinking vaguely of the Future's hopes, thinking specifically of the Past's horrors, and spending <em>most </em>of your time in the Present.  This strategy has certain epistemic virtues beyond its use in cheering yourself up.</p> <p>But it helps to have <em>legitimate </em>reason to vaguely hope&#8212;to minimize the leaps of abstract optimism involved in thinking that, yes, you can live and obtain happiness in the Future.</p> <p>(2)  Rationality is our goal, and atheism is just a side effect&#8212;the judgment that happens to be produced.  But atheism is an <em>important</em> side effect.  John C. Wright, who wrote the heavily transhumanist <em>The Golden Age,</em> had some kind of temporal lobe epileptic fit and became a Christian.  There's a once-helpful soul, now lost to us.</p> <p>But it is possible to do better, even if your brain malfunctions on you.  I know a transhumanist who has strong religious visions, which she once attributed to future minds reaching back in time and talking to her... but then she reasoned it out, asking why future superminds would grant <em>only her</em> the solace of conversation, and why they could offer vaguely reassuring arguments but not tell her winning lottery numbers or the 900th digit of pi.  So now she still has strong religious experiences, but she is not religious.  That's the difference between weak rationality and strong rationality, and it has to do with the <em>depth </em> and <em>generality</em> of the epistemic rules that you know and apply.</p> <p>Fun Theory is part of the fully general reply to religion; in particular, it is the fully general reply to theodicy.  If you can't say how God could have <em>better</em> created the world without sliding into an antiseptic Wellsian Utopia, you can't carry <a href="http://i20.photobucket.com/albums/b217/luke1889/Epicurus.jpg">Epicurus's argument</a> [http://i20.photobucket.com/albums/b217/luke1889/Epicurus.jpg].  If, on the other hand, you have some idea of how you could build a world that was not only more pleasant but also a better medium for self-reliance, then you can see that permanently losing both your legs in a car accident when someone else crashes into you, doesn't seem very eudaimonic.</p> <p>If we can imagine what the world might look like if it had been designed by anything remotely like a benevolently inclined superagent, we can look at the world around us, and see that <em>this </em>isn't it.  This doesn't require that we correctly forecast the <em>full</em> optimization of a superagent&#8212;just that we can envision <em>strict improvements</em> on the present world, even if they prove not to be <em>maximal</em>.</p> <p>(3) There's a severe problem in which people, due to <a href="0439.html">anthropomorphic optimism</a> [http://lesswrong.com/lw/st/anthropomorphic_optimism/] and the lack of specific reflective knowledge about their <a href="0456.html">invisible background framework</a> [http://lesswrong.com/lw/ta/invisible_frameworks/] and many other biases which I have discussed, think of a "nonhuman future" and just subtract off a few aspects of humanity that are salient, like enjoying the taste of peanut butter or something.  While still envisioning a future filled with minds that have aesthetic sensibilities, experience happiness on fulfilling a task, get bored with doing the same thing repeatedly, etcetera.  These things seem <em>universal,</em> rather than <em>specifically human</em>&#8212;to a human, that is.  They don't involve having ten fingers or two eyes, so they must be universal, right?</p> <p>And if you're still in this frame of mind&#8212;where "real values" are the ones that <a href="0433.html">persuade</a> [http://lesswrong.com/lw/sn/interpersonal_morality/] <a href="0397.html">every possible mind</a> [http://lesswrong.com/lw/rn/no_universally_compelling_arguments/], and the rest is just some extra specifically human stuff&#8212;then Friendly AI will seem unnecessary to you, because, in its absence, you expect the universe to be <em>valuable</em> but not <em>human.</em></p> <p>It turns out, though, that once you start talking about what specifically is and isn't <em>valuable</em>, even if you try to keep yourself sounding as "non-human" as possible&#8212;then you still end up with a big complicated computation that is only instantiated physically in human brains and nowhere else in the universe.  Complex challenges?  Novelty?  Individualism?  Self-awareness?  Experienced happiness?  A paperclip maximizer cares not about these things.</p> <p>It is a long project to crack people's brains loose of thinking that things will turn out regardless&#8212;that they can subtract off a few specifically human-seeming things, and then end up with plenty of other things they care about that are universal and will appeal to arbitrarily constructed AIs.  And of this I have said a very great deal already.  But it does not seem to be enough.  So Fun Theory is one more step&#8212;taking the curtains off some of the invisible background of our values, and revealing some of the complex criteria that go into a life worth living.</p> <p> </p> <p style="text-align:right">Part of <a href="0624.html"><em>The Fun Theory Sequence</em></a> [http://lesswrong.com/lw/xy/the_fun_theory_sequence/]</p> <p style="text-align:right">Next post: "<a href="0622.html">Higher Purpose</a> [http://lesswrong.com/lw/xw/higher_purpose/]"</p> <p style="text-align:right">Previous post: "<a href="0615.html">Seduced by Imagination</a> [http://lesswrong.com/lw/xp/seduced_by_imagination/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq15.html">Sequence 15: Fun Theory</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0615.html">Seduced by Imagination</a></p></td><td><p><i>Next: </i><a href="0622.html">Higher Purpose</a></p></td></tr></table><p><i>Referenced by: </i><a href="0608.html">Serious Stories</a> &#8226; <a href="0615.html">Seduced by Imagination</a> &#8226; <a href="0622.html">Higher Purpose</a> &#8226; <a href="0624.html">The Fun Theory Sequence</a> &#8226; <a href="0674.html">Raising the Sanity Waterline</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/xc/the_uses_of_fun_theory/">The Uses of Fun (Theory)</a></p></body></html>