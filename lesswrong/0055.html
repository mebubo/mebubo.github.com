<html><head><title>Bayesian Judo</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>Bayesian Judo</h1><p><i>Eliezer Yudkowsky, 31 July 2007 05:53AM</i></p><div><p>You can have some fun with people whose <a href="0054.html">anticipations get out of sync with what they believe they believe</a> [http://lesswrong.com/lw/i4/belief_in_belief/].</p> <p>I was once at a dinner party, trying to explain to a man what I did for a living, when he said: "I don't believe Artificial Intelligence is possible because only God can make a soul."</p> <p>At this point I must have been divinely inspired, because I instantly responded: "You mean if I can make an Artificial Intelligence, it proves your religion is false?"</p> <p><a id="more"></a></p> <p>He said, "What?"</p> <p>I said, "Well, if your religion predicts that I can't possibly make an Artificial Intelligence, then, if I make an Artificial Intelligence, it means your religion is false. Either your religion allows that it might be possible for me to build an AI; or, if I build an AI, that disproves your religion."</p> <p>There was a pause, as the one realized he had just made his hypothesis vulnerable to falsification, and then he said, "Well, I didn't mean that you couldn't make an intelligence, just that it couldn't be emotional in the same way we are."</p> <p>I said, "So if I make an Artificial Intelligence that, without being deliberately preprogrammed with any sort of script, starts talking about an emotional life that sounds like ours, <em>that</em> means your religion is wrong."</p> <p>He said, "Well, um, I guess we may have to agree to disagree on this."</p> <p>I said: "No, we can't, actually. There's a theorem of rationality called Aumann's Agreement Theorem which shows that no two rationalists can agree to disagree. If two people disagree with each other, at least one of them must be doing something wrong."</p> <p>We went back and forth on this briefly. Finally, he said, "Well, I guess I was really trying to say that I don't think you can make something eternal."</p> <p>I said, "Well, I don't think so either! I'm glad we were able to reach agreement on this, as Aumann's Agreement Theorem requires."  I stretched out my hand, and he shook it, and then he wandered away.</p> <p>A woman who had stood nearby, listening to the conversation, said to me gravely, "That was beautiful."</p> <p>"Thank you very much," I said.</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions"><em>Mysterious Answers to Mysterious Questions</em></a> [http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions]</p> <p style="text-align:right">Next post: "<a href="0056.html">Professing and Cheering</a> [http://lesswrong.com/lw/i6/professing_and_cheering/]"</p> <p style="text-align:right">Previous post: "<a href="0054.html">Belief in Belief</a> [http://lesswrong.com/lw/i4/belief_in_belief/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq02.html">Sequence 02: Mysterious Answers to Mysterious Questions</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0054.html">Belief in Belief</a></p></td><td><p><i>Next: </i><a href="0056.html">Professing and Cheering</a></p></td></tr></table><p><i>Referenced by: </i><a href="0054.html">Belief in Belief</a> &#8226; <a href="0056.html">Professing and Cheering</a> &#8226; <a href="0075.html">Fake Explanations</a> &#8226; <a href="0142.html">Why Are Individual IQ Differences OK?</a> &#8226; <a href="0337.html">Decoherence as Projection</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/i5/bayesian_judo/">Bayesian Judo</a></p></body></html>