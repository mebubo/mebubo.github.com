<html><head><title>Fake Optimization Criteria</title></head><body><h1>Fake Optimization Criteria</h1><p><i>Eliezer Yudkowsky, 10 November 2007 12:10AM</i></p><div><p><strong>Followup to:</strong>  <a href="0148.html">Fake Justification</a> [http://lesswrong.com/lw/kq/fake_justification/], <a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]</p> <p><a href="0071.html">I've</a> [http://lesswrong.com/lw/il/hindsight_bias/] <a href="0072.html">previously</a> [http://lesswrong.com/lw/im/hindsight_devalues_science/] <a href="0065.html">dwelt</a> [http://lesswrong.com/lw/if/your_strength_as_a_rationalist/] <a href="0060.html">in</a> [http://lesswrong.com/lw/ia/focus_your_uncertainty/] <a href="0067.html">considerable</a> [http://lesswrong.com/lw/ih/absence_of_evidence_is_evidence_of_absence/] <a href="0068.html">length</a> [http://lesswrong.com/lw/ii/conservation_of_expected_evidence/] <a href="0053.html">upon</a> [http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/] <a href="0054.html">forms</a> [http://lesswrong.com/lw/i4/belief_in_belief/] <a href="0115.html">of</a> [http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/] <a href="0116.html">rationalization</a> [http://lesswrong.com/lw/ju/rationalization/] <a href="0056.html">whereby</a> [http://lesswrong.com/lw/i6/professing_and_cheering/] <a href="0057.html">our</a> [http://lesswrong.com/lw/i7/belief_as_attire/] <a href="0058.html">beliefs</a> [http://lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/] <a href="0075.html">appear </a> [http://lesswrong.com/lw/ip/fake_explanations/]<a href="0076.html">to</a> [http://lesswrong.com/lw/iq/guessing_the_teachers_password/] <a href="0078.html">match</a> [http://lesswrong.com/lw/is/fake_causality/] <a href="0079.html">the</a> [http://lesswrong.com/lw/it/semantic_stopsigns/] <a href="0107.html">evidence</a> [http://lesswrong.com/lw/jl/what_is_evidence/] <a href="0080.html">much</a> [http://lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/] <a href="0081.html">more</a> [http://lesswrong.com/lw/iv/the_futility_of_emergence/] <a href="0083.html">strongly</a> [http://lesswrong.com/lw/ix/say_not_complexity/] <a href="0082.html">than</a> [http://lesswrong.com/lw/iw/positive_bias_look_into_the_dark/] <a href="0028.html">they</a> [http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/] <a href="0114.html">actually</a> [http://lesswrong.com/lw/js/the_bottom_line/] <a href="0115.html">do</a> [http://lesswrong.com/lw/jt/what_evidence_filtered_evidence/].  And I'm not overemphasizing the point, either.  If we could beat this fundamental metabias and see what every hypothesis <em>really</em> predicted, we would be able to recover from almost any other error of fact.</p> <p>The mirror challenge for decision theory is seeing which option a choice criterion <em>really</em> endorses.  If your <a href="0148.html">stated moral principles</a> [http://lesswrong.com/lw/kq/fake_justification/] call for you to provide laptops to everyone, does that <em>really</em> endorse buying a $1 million gem-studded laptop for yourself, or spending the same money on shipping 5000 OLPCs?</p> <p>We seem to have evolved a knack for arguing that practically any goal implies practically any action.  A phlogiston theorist explaining why magnesium gains weight when burned has nothing on an Inquisitor explaining why God's infinite love for all His children requires burning some of them at the stake.</p> <p>There's no mystery about this.  <a href="0010.html">Politics</a> [http://lesswrong.com/lw/gw/politics_is_the_mindkiller/] was a feature of the ancestral environment.  We are descended from those who argued most persuasively that the good of the tribe meant executing their hated rival Uglak.  (We sure ain't descended from Uglak.) </p> <p><a id="more"></a></p> <p>And yet... is it possible to <em>prove</em> that if Robert Mugabe cared <em>only</em> for the good of Zimbabwe, he would resign from its presidency?  You can <em>argue</em> that the policy follows from the goal, but haven't we just seen that humans can match up any goal to any policy?  How do you know that you're right and Mugabe is wrong?  (There are a number of reasons this is a good guess, but bear with me here.)</p> <p>Human motives are manifold and obscure, our decision processes as vastly complicated as our brains.  And the world itself is vastly complicated, on every choice of real-world policy.  Can we even <em>prove</em> that human beings are rationalizing&#8212;that we're systematically distorting the link from principles to policy&#8212;when we lack a single firm place on which to stand?  When there's no way to find out <em>exactly</em> what even a single optimization criterion implies?  (Actually, you can just observe that people <em>disagree</em> about office politics in ways that strangely correlate to their own interests, while simultaneously denying that any such interests are at work.  But again, bear with me here.)</p> <p>Where is the standardized, open-source, generally intelligent, consequentialist optimization process into which we can feed a complete morality as an XML file, to find out what that morality <em>really</em> recommends when applied to our world?  Is there even a single real-world case where we can know <em>exactly</em> what a choice criterion recommends?  Where is the <em>pure</em> moral reasoner&#8212;of known utility function, purged of all other stray desires that might distort its optimization&#8212;whose trustworthy output we can contrast to human rationalizations of the same utility function?</p> <p>Why, it's our old friend the <a href="0149.html">alien god</a> [http://lesswrong.com/lw/kr/an_alien_god/], of course!  Natural selection is guaranteed free of all mercy, all love, all compassion, all aesthetic sensibilities, all political factionalism, all ideological allegiances, all academic ambitions, all libertarianism, all socialism, <a href="0007.html">all Blue and all Green</a> [http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/].  Natural selection doesn't <em>maximize</em> its criterion of inclusive genetic fitness&#8212;it's <a href="0151.html">not that smart</a> [http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/].  But when you look at the output of natural selection, you are guaranteed to be looking at an output that was optimized <em>only</em> for inclusive genetic fitness, and not the interests of the US agricultural industry.</p> <p>In the case histories of evolutionary science&#8212;in, for example, <a href="0154.html">The Tragedy of Group Selectionism</a> [http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/]&#8212;we can directly compare human rationalizations to the result of<em> pure</em> optimization for a known criterion.  What did Wynne-Edwards think would be the result of group selection for small subpopulation sizes?  Voluntary individual restraint in breeding, and enough food for everyone.  What was the actual laboratory result?  Cannibalism.</p> <p>Now you might ask:  Are these case histories of evolutionary science really relevant to human morality, which doesn't give two figs for inclusive genetic fitness when it gets in the way of love, compassion, aesthetics, healing, freedom, fairness, et cetera?  Human societies didn't even have a concept of "inclusive genetic fitness" until the 20th century.</p> <p>But I ask in return:  If we can't see clearly the result of a single monotone optimization criterion&#8212;if we can't even train ourselves to hear a single pure note&#8212;then how will we listen to an orchestra?  How will we see that "Always be selfish" or "Always obey the government" are poor guiding principles for human beings to adopt&#8212;if we think that even <em>optimizing genes for inclusive fitness</em> will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?</p> <p>To train ourselves to see clearly, we need simple practice cases.</p> <p> </p> <p style="text-align:right">(end of <a href="http://wiki.lesswrong.com/wiki/Evolution#Blog_posts_.28sequence.29"><em>The Simple Math of Evolution</em></a> [http://wiki.lesswrong.com/wiki/Evolution#Blog_posts_.28sequence.29])</p> <p> </p> <p style="text-align:right">Part of the <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization"><em>Against Rationalization</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization] subsequence of <a href="http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"><em>How To Actually Change Your Mind</em></a> [http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind]</p> <p style="text-align:right">Next post: "<a href="0573.html">Is That Your True Rejection?</a> [http://lesswrong.com/lw/wj/is_that_your_true_rejection/]"</p> <p style="text-align:right">Previous post: "<a href="0148.html">Fake Justification</a> [http://lesswrong.com/lw/kq/fake_justification/]"</p> <p> </p></div> <hr><table><tr><th colspan="2"><a href="seq07.html">Sequence 07: Against Rationalization</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0148.html">Fake Justification</a></p></td><td><p><i>Next: </i><a href="0573.html">Is That Your True Rejection?</a></p></td></tr></table><table><tr><th colspan="2"><a href="seq20.html">Sequence 20: Evolution</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0154.html">The Tragedy of Group Selectionism</a></p></td><td><p><i>Next: </i><a href="seq21.html">Sequence 21: Challenging the Difficult</a></p></td></tr></table><p><i>Referenced by: </i><a href="0148.html">Fake Justification</a> &#8226; <a href="0154.html">The Tragedy of Group Selectionism</a> &#8226; <a href="0161.html">Thou Art Godshatter</a> &#8226; <a href="0171.html">The Hidden Complexity of Wishes</a> &#8226; <a href="0172.html">Lost Purposes</a> &#8226; <a href="0183.html">Fake Fake Utility Functions</a> &#8226; <a href="0184.html">Fake Utility Functions</a> &#8226; <a href="0195.html">Guardians of Ayn Rand</a> &#8226; <a href="0312.html">GAZP vs. GLUT</a> &#8226; <a href="0391.html">Heading Toward Morality</a> &#8226; <a href="0439.html">Anthropomorphic Optimism</a> &#8226; <a href="0573.html">Is That Your True Rejection?</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/kz/fake_optimization_criteria/">Fake Optimization Criteria</a></p></body></html>