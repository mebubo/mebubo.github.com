<html><head><title>Do Scientists Already Know This Stuff?</title></head><body><h1>Do Scientists Already Know This Stuff?</h1><p><i>Eliezer Yudkowsky, 17 May 2008 02:25AM</i></p><div><p><strong>Followup to</strong>:  <a href="0351.html">Science Isn't Strict <em>Enough</em></a> [http://lesswrong.com/lw/qd/science_isnt_strict_enough/]</p> <p><a href="0351.html">poke</a> [http://lesswrong.com/lw/qd/science_isnt_strict_enough/] alleges:</p> <blockquote> <p>"Being able to create relevant hypotheses is an important skill and one a scientist spends a great deal of his or her time developing. It may not be part of the traditional <em>description</em> of science but that doesn't mean it's not included in the actual social institution of science that produces actual real science here in the real world; it's your description and not science that is faulty."</p> </blockquote> <p>I know I've been calling my younger self "stupid" but that is a figure of speech; "unskillfully wielding high intelligence" would be more precise.  Eliezer<sub>18</sub> was not in the habit of making obvious mistakes&#8212;it's just that his "obvious" wasn't my "obvious".</p> <p>No, I did not go through the traditional apprenticeship.  But when I look back, and see what Eliezer<sub>18</sub> did wrong, I see <em>plenty</em> of modern scientists making the same mistakes.  I cannot detect any sign that they were better warned than myself.</p> <p>Sir Roger Penrose&#8212;a world-class physicist&#8212;still<em> </em>thinks that consciousness is caused by quantum gravity.  I expect that no one ever warned him against <a href="0080.html">mysterious answers to mysterious questions</a> [http://lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/]&#8212;only told him his hypotheses needed to be falsifiable and have empirical consequences.  Just like Eliezer<sub>18</sub>.</p> <p><a id="more"></a></p> <p>"Consciousness is caused by quantum gravity" has testable implications:  It implies that you should be able to look at neurons and discover a coherent quantum superposition (whose collapse?) contributes to information-processing, and that you won't ever be able to reproduce a neuron's input-output behavior using a computable microanatomical simulation...</p> <p>...but even after you say "Consciousness is caused by quantum gravity", you don't anticipate anything about how your brain thinks "I think therefore I am!" or the mysterious redness of red, that you did not anticipate before, even though you feel like you know a cause of it.  This is a tremendous danger sign, <em>I now realize, </em>but it's not the danger sign that <em>I</em> was warned against, and I doubt that Penrose was ever told of it by his thesis advisor.  For that matter, I doubt that Niels Bohr was ever warned against it when it came time to formulate the Copenhagen Interpretation.</p> <p>As far as I can tell, the reason Eliezer<sub>18</sub> and Sir Roger Penrose and Niels Bohr were not warned, is that no standard warning exists.</p> <p>I did not <em>generalize</em> the concept of "mysterious answers to mysterious questions", in that many words, until I was writing a Bayesian analysis of what distinguishes <a href="http://yudkowsky.net/bayes/technical.html">technical, nontechnical and semitechnical</a> [http://yudkowsky.net/bayes/technical.html] scientific explanations.  Now, the final <em>output</em> of that analysis, can be phrased nontechnically in terms of four danger signs:</p> <ul> <li>First, the explanation acts as a <a href="0079.html">curiosity-stopper</a> [http://lesswrong.com/lw/it/semantic_stopsigns/] rather than an <a href="0053.html">anticipation-controller</a> [http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/].</li> <li>Second, the hypothesis has no moving parts&#8212;the secret sauce is not a specific complex mechanism, but a blankly solid substance or force.</li> <li>Third, those who proffer the explanation cherish their ignorance; they speak proudly of how the phenomenon defeats ordinary science or is unlike merely mundane phenomena.</li> <li>Fourth, <em>even after the answer is given, the phenomenon is still a mystery </em>and possesses the same quality of wonderful inexplicability that it had at the start. </li> </ul> <p>In principle, all this could have been said in the immediate aftermath of vitalism.  Just like elementary probability theory could have been invented by Archimedes, or the ancient Greeks could have theorized natural selection.  But <em>in fact</em> no one ever warned me against any of these four dangers, in those terms&#8212;the closest being the warning that hypotheses should have testable consequences.  And I didn't conceptualize the warning signs <em>explicitly</em> until I was trying to think of the whole affair in terms of probability distributions&#8212;some degree of overkill was required.</p> <p>I simply have no reason to believe that these warnings are passed down in scientific apprenticeships&#8212;certainly not to a majority of scientists.  Among other things, it is advice for handling <em>situations of confusion and despair</em>, scientific <em>chaos.</em>  When would the average scientist or average mentor have an opportunity to use that kind of technique?</p> <p>We just got through discussing the <a href="0346.html">single-world fiasco</a> [http://lesswrong.com/lw/q8/many_worlds_one_best_guess/] in physics. Clearly, no one told them about the formal definition of Occam's Razor, in whispered apprenticeship or otherwise.</p> <p>There is a known effect where great scientists have multiple great students.  This may well be due to the mentors passing on skills that they can't describe.  But I don't think that counts as part of <em>standard </em>science.  And if the great mentors haven't been able to put their guidance into words and publish it generally, that's not a good sign for how well these things are understood.</p> <p>Reasoning in the absence of definite evidence without going <em>instantaneously completely wrong</em> is <em>really really hard.</em>  When you're learning in school, you can miss one point, and then be taught fifty other points that happen to be correct.  When you're reasoning out new knowledge in the absence of crushingly overwhelming guidance, you can miss one point and wake up in Outer Mongolia fifty steps later.</p> <p>I am pretty sure that scientists who switch off their brains and relax with some comfortable nonsense as soon as they leave their own specialties, do not realize that <a href="0271.html">minds are engines</a> [http://lesswrong.com/lw/o5/the_second_law_of_thermodynamics_and_engines_of/] and that there is a causal story behind every trustworthy belief.  Nor, I suspect, were they ever told that there is an exact rational probability given a state of evidence, which <a href="0351.html">has no room for whims</a> [http://lesswrong.com/lw/qd/science_isnt_strict_enough/]; even if you can't calculate the answer, and even if you don't hear any authoritative command for what to believe.</p> <p>I doubt that scientists who are asked to pontificate on the future by the media, who sketch amazingly detailed pictures of Life in 2050, were ever taught about the <a href="0106.html">conjunction fallacy</a> [http://lesswrong.com/lw/jk/burdensome_details/].  Or how the representativeness heuristic can make more detailed stories seem more plausible, even as each extra detail drags down the probability.  The notion of every added detail needing its own support&#8212;of not being able to <em>make up</em> big detailed stories that sound just like the detailed stories you were <em>taught</em> in science or history class&#8212;is <em>absolutely vital</em> to precise thinking in the absence of definite evidence.  But how would a notion like that get into the <em>standard</em> scientific apprenticeship?  The cognitive bias was uncovered only a few decades ago, and not popularized until very recently.</p> <p>Then there's <a href="0180.html">affective death spirals</a> [http://lesswrong.com/lw/lm/affective_death_spirals/] around notions like "<a href="0081.html">emergence</a> [http://lesswrong.com/lw/iv/the_futility_of_emergence/]" or "<a href="0083.html">complexity</a> [http://lesswrong.com/lw/ix/say_not_complexity/]" which are sufficiently vaguely defined that you can say lots of nice things about them.  There's whole academic subfields built around the kind of mistakes that Eliezer<sub>18</sub> used to make!  (Though I never fell for the "emergence" thing.)</p> <p>I sometimes say that the goal of science is to amass such an enormous mountain of evidence that not even scientists can ignore it: and that this is the distinguishing feature of a scientist, a non-scientist will ignore it anyway.</p> <p>If there can exist some amount of evidence so crushing that you finally despair, stop making excuses and <em>just give up</em>&#8212;drop the old theory and never mention it again&#8212;then this is all it takes to let the ratchet of Science turn forward over time, and raise up a technological civilization.  Contrast to religion.</p> <p>Books by Carl Sagan and Martin Gardner and the other veins of Traditional Rationality are meant to accomplish this difference: to transform someone from a non-scientist into a potential scientist, and guard them from experimentally disproven madness.</p> <p>What further training does a professional scientist get?  Some frequentist stats classes on how to calculate statistical significance.  Training in standard techniques that will let them churn out papers within a solidly established paradigm.</p> <p>If Science demanded more than this from the average scientist, I don't think it would be possible for Science to get done.  We have problems enough from people who sneak in without the drop-dead-basic qualifications.</p> <p>Nick Tarleton <a href="0351.html">summarized</a> [http://lesswrong.com/lw/qd/science_isnt_strict_enough/k35] the resulting problem very well&#8212;better than I did, in fact:  If you come up with a bizarre-seeming hypothesis not yet ruled out by the evidence, and try to test it experimentally, Science doesn't call you a bad person.  Science doesn't trust its elders to decide which hypotheses "aren't worth testing". But this is a carefully lax <em>social </em>standard, and if you try to translate it into a standard of <em>individual</em> epistemic rationality, it lets you believe far too much.  Dropping back into the analogy with <a href="0349.html">pragmatic-distrust-based-libertarianism</a> [http://lesswrong.com/lw/qb/science_doesnt_trust_your_rationality/], it's the difference between "Cigarettes shouldn't be illegal" and "Go smoke a Marlboro".</p> <p>Do you remember ever being <em>warned against that mistake,</em> in so many words?  Then why <em>wouldn't</em> people make exactly that error?  How many people will <em>spontaneously </em>go an extra mile and be even stricter with themselves?  Some, but not many.</p> <p>Many scientists will believe all manner of ridiculous things <a href="0009.html">outside the laboratory</a> [http://lesswrong.com/lw/gv/outside_the_laboratory/], so long as they can convince themselves it hasn't been definitely disproven, or so long as they manage not to ask.  Is there some standard lecture that grad students get, of which people see this folly, and ask, "Were they absent from class that day?"  No, as far as I can tell.</p> <p>Maybe if you're super lucky and get a famous mentor, they'll tell you rare personal secrets like "Ask yourself which are the important problems in your field, and then work on one of those, instead of falling into something easy and trivial" or "Be more careful than the journal editors demand; look for new ways to guard your expectations from influencing the experiment, even if it's not standard."</p> <p>But I <em>really don't think</em> there's a huge secret standard scientific tradition of precision-grade rational reasoning on sparse evidence.  Half of all the scientists out there still <a href="0054.html">believe they believe</a> [http://lesswrong.com/lw/i4/belief_in_belief/] in God!  <em>The more difficult skills are not standard!</em></p> <p> </p> <p style="text-align:right">Part of <a href="0379.html"><em>The Quantum Physics Sequence</em></a> [http://lesswrong.com/lw/r5/the_quantum_physics_sequence/]</p> <p style="text-align:right">Next post: "<a href="0353.html">No Safe Defense, Not Even Science</a> [http://lesswrong.com/lw/qf/no_safe_defense_not_even_science/]"</p> <p style="text-align:right">Previous post: "<a href="0351.html">Science Isn't Strict <em>Enough</em></a> [http://lesswrong.com/lw/qd/science_isnt_strict_enough/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq13.html">Sequence 13: Quantum Physics</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0351.html">Science Isn't Strict Enough</a></p></td><td><p><i>Next: </i><a href="0353.html">No Safe Defense, Not Even Science</a></p></td></tr></table><p><i>Referenced by: </i><a href="0351.html">Science Isn't Strict Enough</a> &#8226; <a href="0353.html">No Safe Defense, Not Even Science</a> &#8226; <a href="0368.html">A Premature Word on AI</a> &#8226; <a href="0379.html">The Quantum Physics Sequence</a> &#8226; <a href="0675.html">A Sense That More Is Possible</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/qe/do_scientists_already_know_this_stuff/">Do Scientists Already Know This Stuff?</a></p></body></html>